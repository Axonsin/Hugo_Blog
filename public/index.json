
[{"content":"Welcome.！！！！！\n","date":"30 June 2025","externalUrl":null,"permalink":"/posts/","section":"","summary":"","title":"","type":"posts"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/tags/animation/","section":"Tags","summary":"","title":"Animation","type":"tags"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/tags/obs/","section":"Tags","summary":"","title":"OBS","type":"tags"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/tags/shader/","section":"Tags","summary":"","title":"Shader","type":"tags"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/tags/%E6%8F%92%E4%BB%B6/","section":"Tags","summary":"","title":"插件","type":"tags"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/categories/%E6%9D%82%E8%B0%88/","section":"Categories","summary":"","title":"杂谈","type":"categories"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/","section":"某人的小博客","summary":"","title":"某人的小博客","type":"page"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/tags/%E6%B8%B2%E6%9F%93/","section":"Tags","summary":"","title":"渲染","type":"tags"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/tags/%E8%89%B2%E5%BD%A9/","section":"Tags","summary":"","title":"色彩","type":"tags"},{"content":"OBS ShaderFilter是一个可以使用HLSL/GLSL对图层进行处理的插件，而且内部会有很多OBS原生并不会带有的滤镜，比如box模糊和一堆搞怪滤镜。https://obsproject.com/forum/resources/obs-shaderfilter.1736/ 直接下载exe文件，可以直接锁定到OBS目录并安装，还是很方便的。\n使用方法 # 安装后，对某个图层右键——滤镜——＋User Defined Filter。在这里你可以选择使用effect文件、还是shader文件、亦或是直接自行输入hlsl。 当然我自己的话比较熟悉HLSL，对于HLSL的一些函数比较眼熟，所以我偏向于直接用HLSL。当然用OpenGL也不是不行，要在快捷方式和高级设置中稍微改一下即可。\n如何让OBS启用OpenGL渲染？ # 因为程序会优先启动微软的DirectX渲染，也就是hlsl，但是如果偏要使用OpenGL也是可以的。\nStep1. 增加 \u0026ndash;allow-opengl 启动参数，选中OBS快捷方式，在目标后面增加**** \u0026ndash;allow-opengl\nStep2. 设置中进行切换\n进入obs设置界面，在 高级-视频-渲染器 中切换到OpenGL，切换应用设置之后需要重启！\n关于OBS和ShaderToy一些系统变量的变换 # ShaderToy会有这些基础变量：\niTime：时间 iResolution：屏幕分辨率 OBS（hlsl）则会有这些变量：\nelapsed_time：OBS的时间变量 uv_size ：uv大小 同时由于语言不同，一些运算函数也会有所不同。比如HLSL需要使用fract()和fract2()函数实现GLSL的mod()计算。\n最后奉上一个我自己转换的shader；\n// 立体线框立方体着色器 - 从Shadertoy (https://shadertoy.com/view/McS3DW) 转换为OBS格式 uniform float speed\u0026lt; string label = \u0026#34;动画速度\u0026#34;; string widget_type = \u0026#34;slider\u0026#34;; float minimum = 0.0; float maximum = 200.0; float step = 0.01; \u0026gt; = 100.0; uniform float thickness\u0026lt; string label = \u0026#34;线条粗细\u0026#34;; string widget_type = \u0026#34;slider\u0026#34;; float minimum = 0.1; float maximum = 50.0; float step = 0.1; \u0026gt; = 15.0; uniform float scale\u0026lt; string label = \u0026#34;图案缩放\u0026#34;; string widget_type = \u0026#34;slider\u0026#34;; float minimum = 1.0; float maximum = 50.0; float step = 0.1; \u0026gt; = 10.0; uniform float4 line_color\u0026lt; string label = \u0026#34;线条颜色\u0026#34;; string widget_type = \u0026#34;color\u0026#34;; \u0026gt; = {1.0, 1.0, 1.0, 1.0}; uniform float opacity\u0026lt; string label = \u0026#34;整体不透明度\u0026#34;; string widget_type = \u0026#34;slider\u0026#34;; float minimum = 0.0; float maximum = 1.0; float step = 0.01; \u0026gt; = 1.0; // 辅助函数 float fract(float v){ return v - floor(v); } float2 fract2(float2 v){ return float2(v.x - floor(v.x), v.y - floor(v.y)); } // 计算点到线段的距离 float segment(float2 p, float2 a, float2 b) { p -= a; b -= a; return length(p - b * clamp(dot(p, b) / dot(b, b), 0.0, 1.0)); } // 旋转函数 float2 rotate(float2 p, float a) { float c = cos(a); float s = sin(a); return float2(p.x * c - p.y * s, p.x * s + p.y * c); } // 使用旋转变换3D点（现在接受旋转值作为参数，而不是使用全局变量） float2 T(float3 p, float rotation) { p.xy = rotate(p.xy, -rotation); p.xz = rotate(p.xz, 0.785); // 约45度 p.yz = rotate(p.yz, -0.625); // 约-36度 return p.xy; } float4 mainImage(VertData v_in) : TARGET { // 采样背景图像 float4 originalColor = image.Sample(textureSampler, v_in.uv); float2 R = uv_size; float2 u = v_in.uv; u.y = 1.0 - u.y; // 翻转Y坐标以匹配Shadertoy u = u * R; // 从0-1范围转换为像素坐标 // 计算图案坐标 float2 X, U = scale * u / R.y; float2 M = float2(2.0, 2.3); // 平铺大小 float2 I = floor(U/M)*M; float2 J; // 计算平铺模数 U = fract2(U/M)*M; // 初始无线条 float lineIntensity = 0.0; // 绘制2x2网格中的四个瓦片 for (int k = 0; k \u0026lt; 4; k++) { X = float2(k % 2, k / 2) * M; J = I + X; // 偏移每隔一个瓦片以增加视觉效果 if (int((J.x / M.x) + (J.y / M.y)) % 2 \u0026gt; 0) X.y += 1.15; // 计算时间依赖的旋转（现在作为局部变量） float adjustedTime = elapsed_time * speed / 100.0; float t_rotation = tanh(-0.2 * (J.x + J.y) + fract(2.0 * adjustedTime / 10.0) * 10.0 - 1.6) * 0.785; // 绘制立方体的六个部分（共12条线） for (float a = 0.0; a \u0026lt; 6.0; a += 1.57) { // 每约90度 float3 A = float3(cos(a), sin(a), 0.7); float3 B = float3(-A.y, A.x, 0.7); // 绘制立方体边缘的线条，将旋转值作为参数传递 lineIntensity += smoothstep(thickness/R.y, 0.0, segment(U-X, T(A, t_rotation), T(B, t_rotation))); lineIntensity += smoothstep(thickness/R.y, 0.0, segment(U-X, T(A, t_rotation), T(A * float3(1.0, 1.0, -1.0), t_rotation))); // 镜像点用于立方体背面 A.z = -A.z; B.z = -B.z; lineIntensity += smoothstep(thickness/R.y, 0.0, segment(U-X, T(A, t_rotation), T(B, t_rotation))); } } // 限制值避免线条重叠区域过亮 lineIntensity = min(lineIntensity, 1.0); // 应用线条颜色并与原始图像混合 float4 finalColor = float4( lerp(originalColor.rgb, line_color.rgb, lineIntensity * opacity), originalColor.a ); return finalColor; } ","date":"30 June 2025","externalUrl":null,"permalink":"/posts/%E9%87%8D%E7%94%9F%E4%B9%8B%E6%88%91%E5%9C%A8obs%E5%86%99shaderobs-shaderfilter%E6%8F%92%E4%BB%B6/","section":"","summary":"OBS自定义Shader效果","title":"重生之我在OBS写Shader——OBS ShaderFilter插件","type":"posts"},{"content":"","date":"19 June 2025","externalUrl":null,"permalink":"/tags/unity/","section":"Tags","summary":"","title":"Unity","type":"tags"},{"content":"又水一篇（\n","date":"19 June 2025","externalUrl":null,"permalink":"/posts/unity%E7%9A%84scene%E7%95%8C%E9%9D%A2%E9%BC%A0%E6%A0%87%E6%BB%9A%E8%BD%AE%E6%85%A2%E5%88%B0%E6%8B%96%E4%B8%8D%E5%8A%A8%E7%9A%84%E9%97%AE%E9%A2%98/","section":"","summary":"问题解决方案","title":"Unity的Scene界面鼠标滚轮慢到拖不动的问题","type":"posts"},{"content":"","date":"19 June 2025","externalUrl":null,"permalink":"/tags/%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4/","section":"Tags","summary":"","title":"故障排除","type":"tags"},{"content":"在Unity中，移动（平移）、旋转、缩放的矩阵变换是通过4×4的齐次坐标矩阵实现的，这些矩阵共同构成了物体在3D空间中的变换。以下是每个部分的详细解释及其作用：\n1. 平移（Translation）矩阵 # 作用： # 移动物体：将物体在空间中沿X、Y、Z轴方向平移，改变物体的位置。 矩阵结构： # | 1 0 0 tx | | 0 1 0 ty | | 0 0 1 tz | | 0 0 0 1 | 参数：tx, ty, tz 是沿X、Y、Z轴的位移量。 位置：平移向量位于矩阵的第四列（前三行最后一列）。 关键点： # 仅对点有效：平移仅对具有位置的点（w=1）生效，对方向向量（w=0）无效。 变换顺序：平移通常在旋转和缩放之后应用（如复合变换M = T * R * S）。 2. 旋转（Rotation）矩阵 # 作用： # 改变物体方向：绕X、Y、Z轴旋转，保持物体形状不变。 矩阵结构： # 旋转矩阵是3×3的正交矩阵，位于齐次矩阵的左上角（前3行前3列）。具体形式取决于旋转轴：\n绕X轴旋转（θ角）： # | 1 0 0 0 | | 0 cosθ -sinθ 0 | | 0 sinθ cosθ 0 | | 0 0 0 1 | 绕Y轴旋转（θ角）： # | cosθ 0 sinθ 0 | | 0 1 0 0 | | -sinθ 0 cosθ 0 | | 0 0 0 1 | 绕Z轴旋转（θ角）： # | cosθ -sinθ 0 0 | | sinθ cosθ 0 0 | | 0 0 1 0 | | 0 0 0 1 | 关键点： # 正交性：旋转矩阵的行/列向量是单位向量且彼此正交（确保不扭曲形状）。 组合旋转：多个旋转可通过矩阵相乘组合（如欧拉角Rx * Ry * Rz）。 Unity中的实现：Unity使用四元数（Quaternion）简化旋转操作，但底层仍通过矩阵计算。 3. 缩放（Scaling）矩阵 # 作用： # 改变物体尺寸：沿X、Y、Z轴缩放，控制物体的大小。 矩阵结构： # | sx 0 0 0 | | 0 sy 0 0 | | 0 0 sz 0 | | 0 0 0 1 | 参数：sx, sy, sz 是沿X、Y、Z轴的缩放因子。 位置：缩放因子位于矩阵的主对角线（左上3×3子矩阵的对角线）。 关键点： # 均匀/非均匀缩放： 均匀缩放（sx = sy = sz）：保持物体形状比例。 非均匀缩放（如sx ≠ sy）：可能导致物体变形（如拉伸）。 对旋转的影响：非均匀缩放会改变旋转轴的方向，需谨慎使用。 4. 齐次坐标（Homogeneous Coordinates） # 作用： # 统一表示变换：通过添加w分量，将平移、旋转、缩放统一为矩阵乘法。 结构： # 齐次坐标将3D点(x, y, z)扩展为(x, y, z, 1)，向量扩展为(x, y, z, 0)：\n点（w=1）：参与平移和线性变换。 向量（w=0）：仅参与旋转和缩放（无平移）。 关键点： # 矩阵乘法兼容性：齐次坐标允许将平移、旋转、缩放组合成单个4×4矩阵。 Unity中的应用：所有变换矩阵（如Model、View、Projection）均基于齐次坐标。 5. 复合变换（Combined Transform） # 在Unity中，物体的变换通常通过缩放→旋转→平移的顺序组合：\nM_{\\text{复合}} = T \\times R \\times S 矩阵结构： # | Rx*x Ry*x Rz*x tx | | Rx*y Ry*y Rz*y ty | | Rx*z Ry*z Rz*z tz | | 0 0 0 1 | 左上3×3：旋转后的缩放矩阵（R × S）。 第四列：平移向量T。 最后一行：始终为(0, 0, 0, 1)。 Unity中的实现： # Transform组件：封装了位置（Position）、旋转（Rotation）、缩放（Scale），底层通过矩阵运算实现。 Matrix4x4类：直接操作矩阵（如Matrix4x4.TRS生成复合矩阵）。 6. 典型应用示例 # (1) 移动物体： # // 生成平移矩阵 Matrix4x4 translation = Matrix4x4.Translate(new Vector3(5, 0, 0)); Vector3 point = new Vector3(1, 2, 3); Vector3 translatedPoint = translation.MultiplyPoint(point); // 结果(6,2,3) (2) 旋转物体： # // 绕Y轴旋转90度 Matrix4x4 rotation = Matrix4x4.Rotate(Quaternion.Euler(0, 90, 0)); Vector3 rotatedPoint = rotation.MultiplyPoint(new Vector3(1, 0, 0)); // 结果(0,0,-1) (3) 缩放物体： # // X轴缩放2倍 Matrix4x4 scale = Matrix4x4.Scale(new Vector3(2, 1, 1)); Vector3 scaledPoint = scale.MultiplyPoint(new Vector3(1, 2, 3)); // 结果(2,2,3) (4) 复合变换： # // 缩放→旋转→平移 Matrix4x4 transformMatrix = Matrix4x4.TRS( position: new Vector3(5, 0, 0), // 平移 rotation: Quaternion.Euler(0, 90, 0), // 旋转 scale: new Vector3(2, 1, 1) // 缩放 ); 7. 注意事项 # 矩阵相乘顺序： 右乘优先：M = T × R × S 表示先缩放，再旋转，最后平移（矩阵相乘顺序与操作顺序相反）。 非均匀缩放的风险： 非均匀缩放会破坏旋转矩阵的正交性，可能导致法线方向错误（需重新计算法线）。 Unity的优化： Unity通过四元数（Quaternion）和欧拉角（EulerAngles）封装旋转，避免直接操作旋转矩阵的复杂性。 总结 # 平移矩阵：控制物体位置（第四列）。 旋转矩阵：控制物体方向（左上3×3的正交子矩阵）。 缩放矩阵：控制物体尺寸（左上3×3的对角线）。 齐次坐标：统一平移、旋转、缩放的计算。 通过这些矩阵的组合，Unity实现了物体在3D空间中的灵活变换，开发者可通过Transform组件或直接操作矩阵实现复杂效果。\n为什么是这个顺序？ # 在3D图形学和Unity引擎中，变换操作的顺序必须遵循“先缩放（Scale）→再旋转（Rotation）→最后平移（Translation）”，这是因为矩阵乘法的非交换性和变换的几何意义所决定的。以下是详细解释：\n1. 变换顺序的核心原因：矩阵乘法的非交换性 # 矩阵乘法不满足交换律（即 ( )），因此变换的顺序直接影响最终结果。复合变换的矩阵乘法顺序需要从右到左依次应用，即：\n但实际操作顺序是先缩放、再旋转、最后平移（即从右到左读取矩阵相乘的顺序）。\n2. 顺序分解：缩放 → 旋转 → 平移 # (1) 先缩放（Scale） # 作用：改变物体的尺寸（沿坐标轴方向放大或缩小）。 关键点： 缩放操作不改变坐标轴的方向和原点，因此后续的旋转和平移不会受到缩放的影响。 如果先缩放，后续的旋转将以缩放后的坐标轴为基准（例如，缩放后旋转会以缩放后的轴方向进行旋转）。 (2) 再旋转（Rotation） # 作用：改变物体的方向（绕坐标轴旋转）。 关键点： 旋转会改变坐标轴的方向，但不改变原点。 如果旋转在缩放之后，旋转后的坐标轴方向是基于缩放后的物体尺寸（例如，旋转后的轴方向不会因后续的平移而改变）。 如果旋转在平移之前，旋转是围绕原点进行的，而平移后的物体位置不会被旋转影响。 (3) 最后平移（Translation） # 作用：移动物体到目标位置。 关键点： 平移会改变物体的原点位置，但不会改变坐标轴的方向或缩放后的尺寸。 如果平移在最后，它将物体从原点移动到目标位置，而缩放和旋转已经确定了物体的尺寸和方向。 3. 顺序错误的后果 # (1) 先平移再缩放 # 问题：平移后的物体距离原点更远，缩放会放大或缩小平移的距离。 例如：先平移 (2, 0, 0)，再缩放 2，最终位置会是 (4, 0, 0)（缩放影响了平移的距离）。 预期：可能希望缩放仅改变尺寸，但平移后的缩放会扭曲位置。 (2) 先旋转再缩放 # 问题：旋转后的坐标轴方向可能与缩放方向不一致，导致物体沿旋转后的轴方向缩放，产生意外变形。 例如：先绕Y轴旋转90°，再沿X轴缩放2倍，实际会沿旋转后的X轴（原Z轴方向）缩放，导致方向错误。 (3) 先平移再旋转 # 问题：旋转会围绕原点进行，而平移后的物体远离原点，导致旋转中心不正确。 例如：先平移 (5, 0, 0)，再绕原点旋转90°，物体将绕原点旋转，而非自身的中心。 4. 为什么Unity和其他引擎遵循此顺序？ # (1) 符合数学定义 # 复合变换矩阵：\nUnity的Transform组件默认的变换顺序是缩放→旋转→平移，其矩阵表达式为：\n其中，(S)是缩放矩阵，(R)是旋转矩阵，(T)是平移矩阵。 这样，物体的变换过程是：先缩放自身，再旋转自身方向，最后平移到目标位置。 (2) 父子对象的层级关系 # 本地坐标系的继承：\n子对象的变换是基于父对象的变换后的坐标系。例如： 如果父对象先缩放，子对象的旋转和后续平移会基于父对象缩放后的坐标轴。 如果父对象先平移，子对象的缩放和旋转会基于父对象的位置，可能导致位置偏移。 (3) 避免几何错误 # 保持旋转中心：\n默认顺序确保旋转和缩放均以物体的本地原点（通常是几何中心）为基准，避免因平移导致旋转中心偏移。 5. 实际应用中的验证 # (1) 场景中的立方体 # 正确顺序（缩放→旋转→平移）：\n一个立方体先缩放为 2x2x2，绕Y轴旋转90°，最后平移到 (5, 0, 0)，结果符合预期。 错误顺序（平移→旋转→缩放）：\n先平移到 (5, 0, 0)，再旋转会导致物体绕原点旋转（远离中心），缩放会改变平移后的距离，导致位置混乱。 (2) Unity代码示例 # // 正确顺序：缩放→旋转→平移 transform.localScale = new Vector3(2, 2, 2); // 缩放 transform.Rotate(0, 90, 0); // 旋转（绕Y轴） transform.position = new Vector3(5, 0, 0); // 平移 // 错误顺序：平移→旋转→缩放 transform.position = new Vector3(5, 0, 0); // 平移 transform.Rotate(0, 90, 0); // 旋转（绕原点，导致位置偏移） transform.localScale = new Vector3(2, 2, 2); // 缩放会放大平移后的距离 6. 总结 # 操作顺序 数学意义 几何意义 缩放（S） 改变坐标轴的长度（保持原点和方向） 改变物体尺寸，但不移动或旋转 旋转（R） 改变坐标轴的方向（保持原点和缩放后的长度） 改变物体方向，但不改变尺寸或位置 平移（T） 改变原点位置（保持坐标轴的方向和缩放） 移动物体到目标位置，但不改变尺寸或方向 遵循此顺序的原因：\n缩放确保后续旋转和移动的基准是物体的原始尺寸。 旋转基于缩放后的坐标轴方向，避免方向错误。 平移最后执行，确保物体移动到正确的位置，不受缩放或旋转的影响。 通过这种方式，可以精确控制物体的变换，避免因顺序错误导致的逻辑或视觉错误。\n","date":"17 June 2025","externalUrl":null,"permalink":"/posts/unity%E7%9A%84%E6%97%8B%E8%BD%AC%E7%BC%A9%E6%94%BE%E7%A7%BB%E5%8A%A8%E7%9F%A9%E9%98%B5/","section":"","summary":"算法原理解析","title":"Unity的旋转、缩放、移动矩阵","type":"posts"},{"content":"","date":"17 June 2025","externalUrl":null,"permalink":"/tags/%E7%AE%97%E6%B3%95/","section":"Tags","summary":"","title":"算法","type":"tags"},{"content":"","date":"11 June 2025","externalUrl":null,"permalink":"/tags/blender/","section":"Tags","summary":"","title":"Blender","type":"tags"},{"content":"","date":"11 June 2025","externalUrl":null,"permalink":"/tags/mmd/","section":"Tags","summary":"","title":"MMD","type":"tags"},{"content":"","date":"11 June 2025","externalUrl":null,"permalink":"/tags/retargeting/","section":"Tags","summary":"","title":"Retargeting","type":"tags"},{"content":" 前置可选条件：package manager中加入了Animation Rigging（骨骼可视化Bone Renderer）和MMD4Macanim（用于把pmx转换为fbx）\n在Unity中常常会遇见使用不同角色的动画，也就是Bones Retargeting系统。可以在Project中点击fbx后的Rig分支选项看到目前的状态。\nAnimation Type：Generic、Humanoid、None、Legacy。\nGeneric：导入fbx的时候默认继承的选项，也就是不更改任何的骨骼名称，直接使用fbx内的所有骨骼命名；\nLegacy：一个比较老的标准，常见于很久以前的Builtin管线，如果不是从老项目挪过来的资源不会使用\nHumanoid：重点讲这个东西，这是Retarget的重点。\nHumanoid： # 从generic更改为Humanoid后，Unity会尝试根据内部的描述文件逐一进行描述模糊匹配（Postprocessors的描述文件）。\nDefination：\n1、 Create from this model： 直接使用当前FBX文件里的骨骼结构，自动生成一个新的Avatar（骨骼映射）。\n2、 Copy from other avatar： 用另一个FBX或Prefab中已经创建好的Avatar（骨骼映射）。 优势是可以直接沿用所有骨骼的名字，不需要进行映射， 动画资产更容易批量复用。\nSkinWeights：\n这是蒙皮权重的设置，用来控制每个顶点最多受多少根骨骼影响。\nStandard是表明每个顶点最多有4个骨骼可以参与并控制进行骨骼影响。 顶点权重大于4的，只保留最重要的4个，其余自动舍弃或归零。\nCustom：可以选择并允许你指定“每顶点允许的骨骼数量”，比如2、4、8等（需要在Graphics Settings里自定义）。 但是一般我们只需要选择4个即可。因为大多数GPU对每个顶点可参与变形的骨骼数量是有限制的，4是最常见的上限。\n这和DCC中的蒙皮有什么区别？ # Unity的Skin Weights设置，决定了建模软件里“每个顶点蒙皮权重”最多有几组能被保留和使用。\n在Blender、Maya等3D建模/动画软件中，你可以给每个顶点分配任意数量的权重，比如1、2、4、甚至10根骨骼影响一个顶点。 一般来说， 权重越多，顶点变形越平滑，但数据量越大，性能损耗也增加。 也可以自由涂抹、调整每个骨骼对每个顶点的影响比例。\nUnity在导入模型时，会根据你在Skin Weights（蒙皮权重）选项里选的最大权重数，对每个顶点做一次“筛选”。\n选“4 Bones”，就只保留每个顶点影响最大的4个骨骼的权重，其余全部舍弃（并重新归一化）。\n如果你在建模软件里有顶点被5、6、8根骨骼影响，导入到Unity后，只会留下影响最大的4个，其余全部丢弃。\n这一步只发生在导入时，跟你建模软件里的原始权重关系密切，但会被Unity“削减”到上限如果超过了Unity的设置则会自动只保留对该顶点权重最高的四个骨骼。\n选“Custom”可以设置更高（比如8），就会保留更多组权重。但是性能开销会十分美丽\n引用：\n导入带有人形动画的模型 - Unity 手册 # Strip bones\n勾选 Strip Bones，Unity会在导入模型时自动移除所有没有被蒙皮权重（Skin Weights）影响到的骨骼，即那些对模型变形没有实际作用的骨骼节点不会导入进来。如果原始骨骼里有一些辅助骨、挂点骨、导出时残留的无用骨骼，但这些骨骼没有任何顶点被它们影响（权重为0），勾选这个后这些骨骼会被Unity忽略掉，减少无用数据。\n但是要注意的是，如果我们有一些辅助骨骼（比如说使用辅助骨骼完成武器动画绑定）的时候，就不要勾选这个，这回去掉辅助骨骼。\noptimize game objects（ 优化游戏对象 ）\n勾选后，Unity会直接将骨骼链优化为底层的数据结构，只保留Mesh和根节点，把大部分骨骼节点从Hierarchy中“隐藏”掉，不再作为GameObject存在。这极大地减少了场景中的对象数量，显著提升运行效率。\n来自GPT对这个选项的解释： # 你的疑问很正常，Optimize Game Objects 这个选项确实容易让人迷糊。下面用最直白的语言和实例对比帮你彻底搞懂它的作用和意义！\n1. 问题本质：为什么要“优化骨骼对象”？ # 在Unity里，导入带骨骼动画的模型时，每一根骨骼通常会生成一个GameObject（带Transform），比如这样一棵骨骼树：\n角色 ├── Hips │ ├── Spine │ │ ├── Chest │ │ │ ├── Neck │ │ │ └── Head │ │ └── LeftShoulder │ │ └── LeftArm │ │ └── ... ... 每一个节点（骨骼）都变成场景里的一个“游戏对象”！（可以在inspector中看到）\n2. 这样会有什么问题？ # 如果一个角色有50根骨骼，就有50个GameObject。 一个场景有100个角色，就有5000个骨骼对象！ Unity在运行时要每帧同步这些Transform、管理层级、分配内存，有很大性能开销，特别是在移动端、VR等对性能敏感的场合。 3. 什么是“Optimize Game Objects”？ # 勾选了它，Unity就不再为每根骨骼生成独立GameObject。 # 只保留最重要的根节点和Mesh节点。 # 骨骼动画的数据结构变成底层C++层内存里的“骨骼矩阵数组”，不用Unity的GameObject系统去维护。 # Hierarchy视图里看不到骨骼树了，只有主对象和Mesh。 # 对比： # #### 不勾选Optimize #### 勾选Optimize #### 每个骨骼都是GameObject #### 只有根节点是GameObject #### 可以脚本随时访问骨骼Transform #### 大部分骨骼无法直接访问/脚本操作 #### 性能开销大 #### 性能极高，内存和CPU压力小 4. 实际例子 # 不优化： # 你可以写：transform.Find(\u0026quot;Spine/Chest/Neck/Head\u0026quot;) 直接找到头部骨骼，挂武器、加特效。 但每个角色都要消耗很多GameObject内存，场景多了就卡。 优化后： # 你没法直接脚本操作骨骼（因为它们不再是GameObject）。 但同样动画能正常播放，性能大大提升。 适合大批量NPC、只看动画不用动态操作骨骼的角色。 5. 如果你还想操作骨骼怎么办？ # Unity有“暴露骨骼”(Expose Transforms)功能：\n你可以指定“需要动态操作的那几个骨骼”在Hierarchy里保留，其余优化掉。\n6. 一句话总结 # Optimize Game Objects就是让Unity不为每根骨骼生成GameObject，只在底层“默默执行动画”，这样能极大提升性能和效率。如果你只是让角色自动播放动画，不需要挂武器、特效、换装，强烈建议开启它！\n# 准备妥当后\u0026hellip; # 便可以开始打开Config对骨骼进行Retarget。\nMapping是进行关键配置的地方，如果有些地方没有出现映射的话需要我们进行手动调整，在这个情况下，模型应该摆出Tpose以方便我们进行骨骼映射。\n最下方的两个选项：\nMapping：指定映射方法。分为Clear、Automap、Load/Save。Clear则是直接从头进行分配，清空所有已经映射过的骨骼，从头进行手动分配；Automap则是之前提到过的使用Unity内置的配置文件模糊匹配；Save/Load则是用于模型的批量化处理。\nPose： 主要用于调整当前骨骼的姿势，便于正确映射和校准人形骨骼。 分为Reset、Sample pose和Enforce T-pose。\nSample pose：把骨骼恢复到导出FBX时的原始绑定姿势，通常就是建模时的A-Pose或T-Pose，或者动画师在蒙皮时设置的初始姿势。\nEnforce T-pose： 把骨骼恢复到导出FBX时的原始绑定姿势，通常就是建模时的A-Pose或T-Pose，或者动画师在蒙皮时设置的初始姿势。 有时如果不是T-pose那么自动绑定会出现问题。可以选择这个选项尝试重新自动映射。\nReset：重置姿势。恢复到导入姿势。\n还想钳制姿势？ # 我们知道，即使骨骼映射一直，蒙皮权重相似，但是如果对于跨风格的动画（二次元角色动画重新映射到欧美角色上），很容易出现骨骼运动过于不协调的时候。这就需要config的第二个页面： Muscles \u0026amp; Settings 了。\nMuscle Group Preview：这里的“Muscle”指的是Unity Humanoid系统对人形骨架的各个自由度的抽象，比如手臂的上举、下放、前后摆动等。通过拖动这些滑块，你可以实时预览角色各个大类动作的变形效果，比如张合嘴巴、左右转头、臂展、收腿等。用来检测骨骼分配和蒙皮权重是否合理，比如看张嘴会不会带动到脸部错误部位、转头会不会扭曲等。发现异常可以回到Mapping面板修正骨骼分配，或调整权重。\nPer-Muscles Settings： 展开 Body、Head、Left Arm 等子项后，可以单独调节每一块肌肉的活动范围（比如手臂能抬多高、脖子能转多远）。 这些滑块可以指定人体骨骼的极限运动值， 让Avatar更适配不同体型的模型（比如胳膊较短/较长、脖子较粗等）\nAdditional Settings：\nUpper Arm Twist / Lower Arm Twist：\n控制手臂扭转时的影响范围，防止手臂旋转时出现“爆炸”或不自然。\nUpper Leg Twist / Lower Leg Twist：\n控制大腿/小腿扭转的范围与效果。\nArm Stretch / Leg Stretch：\n控制手臂、腿部在极限动作时的伸展弹性，防止动画重定向时出现“拉长”或变短的畸形。\nFeet Spacing：\n控制两脚之间的默认距离，方便站立动作的适配。\nTranslation DoF （Translation Degree of Freedom，平移自由度） ：\n决定骨骼是否允许平移自由度（一般默认关闭）。 平移自由度是允许某些骨骼节点除了旋转，还可以在空间中移动（平移）。 但是一般不会出现，如果连骨骼都出现了平移，那么关节会出现非常严重的脱节情况。\n","date":"11 June 2025","externalUrl":null,"permalink":"/posts/unity-bones-retargeting%E7%B3%BB%E7%BB%9F-1/","section":"","summary":"骨骼动画指北","title":"Unity Bones Retargeting系统","type":"posts"},{"content":"","date":"11 June 2025","externalUrl":null,"permalink":"/tags/%E9%85%8D%E7%BD%AE/","section":"Tags","summary":"","title":"配置","type":"tags"},{"content":"又水了一篇欧耶（\nblender里面选中物体后不显示黄色线框，这样怎么调回去？ # 就是在物体模式不是编辑模式里面选择模型，右边的场景集合显示选中了，但视图界面的模型没有线框显示选中那个物体。\n点选物体就是选不上，右边的场景集合也没有选中的提示 # # ","date":"10 June 2025","externalUrl":null,"permalink":"/posts/%E5%85%B3%E4%BA%8Eblender%E9%80%89%E4%B8%AD%E7%89%A9%E4%BD%93%E4%B8%8D%E6%98%BE%E7%A4%BA%E9%AB%98%E4%BA%AE%E6%8F%8F%E8%BE%B9%E7%9A%84%E9%97%AE%E9%A2%98/","section":"","summary":"问题解决方案","title":"关于blender选中物体不显示高亮描边的问题","type":"posts"},{"content":"走进 Stencil Buffer 系列 4：Stencil 后处理局部描边 | indienova 独立游戏 走进 Stencil Buffer 系列 1：模型轮廓描边 | indienova 独立游戏 利用graphics bllit函数将纹理复制到对应的texture中\nhttps://docs.unity.cn/cn/2019.4/ScriptReference/Graphics.Blit.html ","date":"4 June 2025","externalUrl":null,"permalink":"/posts/%E5%88%A9%E7%94%A8stencil-buffer%E8%BF%9B%E8%A1%8C%E6%8F%8F%E8%BE%B9/","section":"","summary":"技术分享","title":"利用Stencil Buffer进行描边","type":"posts"},{"content":" 写在前面 # 这两套渲染方案，每一个渲染方案对应的是硬件层面（IMR、TBDR）和软件层面（Foward、Deferred），对应的是目前主流的两套方案：移动端渲染方案和电脑端渲染方案\n为什么不同类型设备会采用不同方案？ # 这要从移动端的性能敏感说起。直接上图：\n相机也有一句话说的是“底大一级压死人”，这句话同样也可以应用在芯片设计中。（~~光是看着面积就知道很吓人了）~~同时，桌面端直接利用电源供电，仅算上GPU就可以获得高达375W的功率，不仅能在算力上“力大飞砖”，同时同样的功能，桌面端的指标明显要优异于移动端。更别说芯片面积受限，On-Chip Memory（L1/L2缓存）容量要比桌面端更小（的多）。好的，现在我们知道了手机的算力天生就要比电脑小得多，接下来就先从硬件结构上说明这两个硬件渲染方法：TBDR/IMR。\nTBDR/IMR # 隶属于硬件层面的渲染架构，不可以进行两者间的切换。\nTBDR： Tile-Based Deferred Rendering # TBDR大致可以分为三个模块：Binning Pass, Rendering Pass, Resolve Pass. 最上一层：Render Pipeline (渲染管线)\n中间一层：On-Chip Buffer（a.k.a. 片上内存，Tiled Frame Buffer \u0026amp; Tiled Depth Buffer）\n最下一层：系统内存，CPU和GPU共享 Binning Pass： # 目标：将几何数据（顶点、三角形）分配到对应的屏幕小块（Tile）中，为后续渲染做准备。\n流程步骤：\n顶点处理： 顶点着色器（Vertex Shader）会接收来自软件层面的顶点数据，处理顶点数据，计算顶点的屏幕坐标（投影到屏幕空间）。 简化处理：部分GPU（如Adreno）可能使用简化版顶点着色器，仅计算顶点位置，忽略纹理坐标、法线等细节（这些在分块阶段不需要）。 分块分配： 将屏幕划分为固定大小的 Tile（如16x16像素的小方块）。 遍历所有三角形，确定每个三角形覆盖了哪些 Tile。 将覆盖的 Tile 记录到 Primitive List（图元列表）中，记录每个 Tile 包含哪些三角形。 数据存储： 将每个 Tile 的 Primitive List 和顶点数据写入 系统内存(aka. LPDDR)。 关键点：分块阶段仅记录几何信息，不进行光栅化或像素着色。 参与组件：\nTiler 单元：负责将三角形分配到对应的 Tile。 顶点着色器：处理顶点坐标。 系统内存：存储分块后的几何数据（Primitive List）。 Rendering Pass: # 目标：逐 Tile 处理几何数据，完成光栅化和像素着色，结果暂存到片上内存（On-Chip Memory）。\n流程步骤：\nTile 遍历： 按顺序处理每个 Tile（如从左到右、从上到下）。 几何处理： 重新处理顶点：读取该 Tile 的 Primitive List，重新执行顶点着色器（这次会完整计算顶点属性，如纹理坐标、法线等）。 裁剪与剔除：进行背面剔除、视锥体剔除等，减少无效三角形。 光栅化（****Rasterization）： 将三角形转换为该 Tile 内的像素（片元）。 深度测试（Z-Test）：利用 HSR（Hidden Surface Removal，隐藏面消除） 或 LRZ（Low Resolution Z） 技术，快速剔除被遮挡的片元，减少不必要的像素着色计算。（需要注意的是，虽然我们认为IMR可以“不切实际的进行高强度运算”，但是它同样可以采用Early-Z/Pre-Z来进行剔除并减少overdraw） 像素着色： 开始执行像素着色器（Fragment Shader），计算每个片元的颜色、光照等。 结果暂存：将处理后的像素数据写入 片上内存（Tile Buffer）。 参与组件：\n光栅化引擎：将三角形转换为像素。 像素着色器：处理像素颜色和光照。 HSR/LRZ 单元：隐藏面消除，减少过绘制（Overdraw）。 片上内存：临时存储 Tile 的渲染结果（颜色、深度缓冲等）。 Resolve Pass： # 目标：将所有 Tile 的渲染结果合并到最终的帧缓冲区（Frame Buffer）。\n流程步骤：\n合并 Tile 数据： 将每个 Tile 的 Tile Buffer 内容（颜色、深度等）从片上内存写入系统内存的帧缓冲区Framebuffer。 多采样抗锯齿（MSAA）：如果启用了 MSAA，此时会直接将多个样本点合并为最终像素颜色。这里由于Tile的像素数据直接按块存在于片上内存，极大提高了MSAA的缓存命中率（直接采样点即可而不需要从片外内存读取Framebuffer）。这也是为什么MSAA在低性能移动端的性能损耗会小于TAA。但是，MSAA的每个像素存储多个样本点（如4x MSAA占用4倍显存），在移动端设备中，显存容量有限（如低端设备仅1-2GB），可能导致显存不足或CPU和GPU间的左右脑互搏，带宽竞争。 清理与提交： 清除片上内存中的临时数据（如深度缓冲）。 将完成的帧提交给显示屏。 参与组件：\nROP（光栅操作单元）：合并 Tile 数据并执行最终的写入操作。 系统内存DDR：存储最终的帧缓冲区。 IMR: Immediate Mode Rendering # IMR的渲染流程分为以下4个核心阶段：Vertex Process, Rasterizer, Pixel Shading, Merge.\nVertex Process # 目标：将顶点数据转换为屏幕空间坐标，并执行顶点着色器计算。\n参与组件：\n顶点缓冲区（Vertex Buffer）：存储顶点的原始数据（坐标、法线、纹理坐标等）。 顶点着色器（Vertex Shader, VS）：GPU的可编程单元，负责计算顶点的最终位置、颜色、纹理坐标等。 固定功能单元（Fixed-Function Units）： 顶点属性组装（Vertex Assembly）：将顶点数据从内存加载到GPU内部缓存。 投影变换（Projection Transform）：将顶点坐标从模型空间转换为屏幕空间。 缓存（L1/L2 Cache）：减少顶点数据从主存重复读取的延迟。 流程：\n顶点数据从系统内存加载到GPU的顶点缓冲区。 顶点着色器对每个顶点执行计算（如光照、动画变形）。 处理后的顶点数据通过固定功能单元完成投影变换，得到屏幕坐标。 Rasterization # 目标：将三角形转换为像素（片元），并进行深度测试（Early-Z）。\n参与组件：\n光栅化单元（Rasterizer）：负责将三角形分解为像素级别的片元。 深度缓冲区（Depth Buffer）：存储每个像素的深度值，用于隐藏面消除（Z-Test）。 裁剪单元（Clipping Unit）：剔除超出视口的三角形。 流程：\n光栅化单元将三角形的顶点坐标扩展为覆盖的像素区域。 每个像素生成一个片元（Fragment），并计算其深度值。 片元通过Early-Z测试：若深度值小于Depth Buffer中的值，则保留；否则丢弃。 未被丢弃的片元传递给像素着色器处理。 Pixel Shadeing # 目标：计算每个片元的最终颜色值。\n参与组件：\n像素着色器（Pixel Shader, PS aka. Fragment Shader, FS）：对通过Early-Z的片元执行光照、纹理采样等计算。 纹理单元（Texture Units）：从纹理内存加载纹理数据（如法线贴图、颜色贴图）。 固定功能插值单元：对顶点属性进行插值（如颜色、纹理坐标）。 流程：\n片元通过插值单元获取顶点属性的插值结果（如像素颜色、纹理坐标）。 像素着色器根据插值后的数据、纹理采样结果及光照模型计算最终颜色。 若启用了Alpha测试（Alpha Test），部分片元可能被丢弃。 Merger # 目标：将片元颜色写入Frame Buffer，并执行混合（Blending）和深度更新。\n参与组件：\n混合单元（Blending Unit）：根据混合方程（如透明度）合并新颜色与已有颜色。 深度Stencil单元：执行Late-Z测试和Stencil Buffer操作。 Frame Buffer（颜色缓冲区）：存储最终像素颜色。 Depth Buffer：存储深度值，用于后续帧的深度测试。 流程：\n通过像素着色的片元进入Late-Z测试（若未通过Early-Z）。 混合单元将新颜色与Frame Buffer中的颜色混合（如透明物体叠加）。 更新Frame Buffer和Depth Buffer，完成像素输出。 电脑端参与的组件： # 硬件组件： # GPU核心模块： 计算单元（Shader Cores）：执行顶点着色器、像素着色器。 光栅化引擎：处理光栅化和深度测试。 内存控制器：管理与系统内存的交互。 缓存结构（片上内存）： L1/L2 Cache：缓存顶点数据和中间结果。 Texture Cache：加速纹理采样。 显存（VRAM）： 存储Frame Buffer、Depth Buffer、纹理等数据。 软件组件： # 图形API（如DirectX、OpenGL）： 管理顶点缓冲区、渲染状态（如混合模式、深度测试）。 驱动程序： 将API命令转换为GPU可执行的指令流。 着色器代码： 顶点着色器、像素着色器的可编程逻辑。 Forward搭配TBDR，而Deferred则会搭配IMR？ # 这就要说到缺点了。虽然Forward在目标实现接近于IMR（两者的目的都是需要及时渲染），按理来说应该是Forward+IMR。我们来简要过一下两个渲染管线：\nForward管线流程 # 顶点处理：顶点着色器计算顶点坐标、法线等。 光栅化：生成片元并进行Early-Z测试。 片段处理： ForwardBase Pass：计算主光源（如Directional Light）。 ForwardAdd Pass：为每个附加光源（点光源、聚光灯）单独渲染，叠加光照。 需要注意的是，在URP14中，ForwardAdd Pass被取消，为了简洁化，可以直接调用GetAdditionalLightsCount()。但是底层实现逻辑仍然一致。 LightAdd in URP14： #ifdef _ADDITIONAL_LIGHTS int additionalLightsCount = GetAdditionalLightsCount(); for (int i = 0; i \u0026lt; additionalLightsCount; ++i) { Light light = GetAdditionalLight(i, input.positionWS); // 简单的Lambert漫反射 half3 additionalLambert = CalculateLambert(N, light.direction, light.color, light.distanceAttenuation * light.shadowAttenuation); additionalLightsColor += baseColor.rgb * additionalLambert * _AdditionalLightsIntensity; } #endif 混合与输出：将结果写入Frame Buffer。 Deferred管线流程 # 顶点处理：仅计算顶点坐标，不处理光照。 光栅化：开始采集并生成G-Buffer（存储几何数据：位置、法线、颜色、材质属性等）。 光照Pass：遍历屏幕像素，根据G-Buffer数据计算所有光源对像素的影响。在这一阶段， 会在屏幕上逐像素执行光照，不再访问场景几何 。 混合与输出：将结果写入Frame Buffer。 发现了没有？Deferred管线这里多了一个Gbuffer。这个Gbuffer是什么呢，这是一个会储存在**片外内存（对应桌面端的VRAM，移动端的LPDDR）**的Buffer。\n延迟渲染需要将几何属性（如位置、法线、材质参数、颜色等）存储到 Gbuffer的多张渲染目标（Render Targets, RT） 中，例如：\nPosition RT：存储像素的三维位置。 Normal RT：存储法线向量。 Albedo RT：存储基础颜色。 Depth RT：存储深度信息。 虽然deferred在多光源计算的性能消耗明显优于forward管线，那么代价是什么呢？代价是deferred虽然可以一次性解决屏幕的所有物体渲染，不需要逐物体渲染；但是这个条件的前提是你的Gbuffer可以迅速地从系统内存读取到计算单元中并作计算。更别说你还没有开启MSAA（4xMSAA=从Gbuffer采样四次，带宽要求直接x4），TAA（从Framebuffer的时域采样，直接*n帧带宽）。\n另外，由于Gbuffer不会存储透明物体数据，因为Deferred渲染的核心是几何处理阶段（Geometry Pass），将所有不透明物体的几何信息（如位置、法线、材质属性、深度）写入G-Buffer（多个渲染目标）。\n也就是说，如果还要加一个透明处理，就意味着：\n可以用加一个Forward管线单独处理，但是这会脱离Gbuffer，导致Deferred的屏幕空间效果不起作用 单独开一个Gbuffer接着渲染。再次从片外内存读写一遍。 对比复杂度 # 最后对比一下算法（由通义生成）：在分析延迟渲染（Gbuffer + IMR）与前向渲染（+ TBDR）的带宽压力差异时，需要从渲染流程、数据存储方式、架构特性 三个维度进行对比。\n1. 延迟渲染（Gbuffer + IMR）的带宽压力来源 # (1) Gbuffer的存储与传输 # Gbuffer的定义： 延迟渲染需要将几何属性（如位置、法线、材质参数、颜色等）存储到 多张渲染目标（Render Targets, RT） 中，例如： Position RT：存储像素的三维位置。 Normal RT：存储法线向量。 Albedo RT：存储基础颜色。 Depth RT：存储深度信息。 带宽消耗点： 初始化阶段：渲染所有几何体时，必须将所有可见像素的几何属性写入Gbuffer，这需要 多次系统内存写入。 光照阶段：每个光源的计算需要读取Gbuffer中的所有RT数据（如法线、位置、材质），导致 多次系统内存读取。 屏幕分辨率越高，Gbuffer的带宽需求呈线性增长（如1080p屏幕需要约200万像素 × 多个RT通道）。 (2) IMR架构的劣势 # 直接写入系统内存： 在IMR架构下，Gbuffer的写入和读取均直接通过 系统内存（VRAM） 进行，无法利用片上内存暂存中间结果。 频繁的读写操作：例如，每个光源的计算需读取所有RT数据，带宽压力随光源数量线性增长。 抗锯齿（MSAA）的额外开销： 若启用MSAA，Gbuffer的每个像素需存储多个样本点的数据（如4x MSAA需4倍存储空间），进一步增加带宽需求。 对于读写速度的刚需： Gbuffer的显存带宽需求在移动端是致命的（如1080p屏幕下，四张Gbuffer的带宽消耗可达 2GB/s，而移动端总带宽仅30GB/s）。 延迟渲染需要 多张RT，导致显存带宽成为瓶颈。 2. 前向渲染（+ TBDR）的带宽优化 # (1) TBDR架构的优势 # 分块处理与片上内存暂存： 分块（Tiling）：将屏幕分割为小Tile，每个Tile的渲染数据（颜色、深度、模板）暂存在 片上内存（On-Chip Memory）。 中间结果暂存：仅在 解析阶段（Resolve） 将Tile结果合并到系统内存的Framebuffer，减少频繁写入。 隐藏面消除（HSR/LRZ）： 提前剔除被遮挡的像素，仅对 可见像素 进行纹理采样和光照计算，减少无效带宽消耗。 纹理读取的局部性优化： 同一Tile内的像素共享纹理数据，片上缓存（如L2缓存）可高效复用，减少系统内存访问。 (2) 前向渲染的流程优化 # 逐Tile处理： 每个Tile的渲染在片上内存完成，仅需读取 当前Tile相关的纹理和几何数据。 无需存储Gbuffer：颜色和深度数据直接暂存于Tile Buffer，最终合并到Framebuffer。 过绘制（Overdraw）的减少： HSR/LRZ技术剔除被遮挡像素，减少纹理和着色计算的带宽需求。 (3) 知识库依据 # 知识库[2][5][7] 强调： TBDR通过 片上内存 存储中间数据，仅最终合并时写入系统内存，显著降低带宽。 HSR/LRZ技术可将Overdraw降至接近1，减少无效像素的纹理读取。 3. 带宽压力对比：延迟渲染 vs 前向渲染 # 维度 延迟渲染（Gbuffer + IMR） 前向渲染（+ TBDR） Gbuffer存储需求 需存储多张RT（如Position、Normal、Albedo等），带宽随RT数量线性增长。 无需Gbuffer，仅存储颜色和深度数据，带宽需求更低。 系统内存访问频率 每个Pass需读取所有RT数据（如每个光源需读取Gbuffer），导致 高频系统内存访问。 仅最终合并时写入系统内存，中间数据在片上内存处理， 系统内存访问频率低。 纹理读取优化 纹理数据需多次读取（如每个光源读取Gbuffer），无片上缓存优化。 纹理数据通过片上缓存复用，且仅处理可见像素， 带宽消耗显著降低。 抗锯齿开销 MSAA需存储多倍样本数据，Gbuffer带宽压力进一步增加。 TBDR的 片上MSAA 可暂存多样本数据，减少系统内存带宽需求。 移动设备适用性 移动端显存带宽有限，Gbuffer的高带宽需求导致性能瓶颈（如知识库[4]提到的2GB/s vs 30GB/s总带宽）。 TBDR的优化使其在移动端成为主流，前向渲染与TBDR协同降低带宽压力。 4. 具体场景示例 # 场景：1080p屏幕，4光源 # 延迟渲染（Gbuffer + IMR）： Gbuffer带宽：假设每像素存储4个通道（如Position、Normal、Albedo、Depth），8bit/通道： 单帧带宽 = 1920×1080 × 4通道 × 8bit × 60fps ≈ 2.5GB/s。 光照阶段带宽：每个光源需读取Gbuffer数据，4光源总带宽 ≈ 10GB/s。 前向渲染（+ TBDR）： 纹理读取：仅对可见像素进行光照计算，假设Overdraw减少为1，带宽 ≈ 1920×1080 × 4光源 × 0.2（缓存命中） ≈ 0.8GB/s。 系统内存写入：仅最终合并时写入Framebuffer，带宽 ≈ 0.5GB/s。 5. 总结：带宽压力差异的核心原因 # 延迟渲染（Gbuffer + IMR）： 带宽杀手：多张RT的存储与高频读写、无片上内存优化导致带宽需求激增。 移动端不友好：显存带宽有限，Gbuffer的高开销易触发带宽瓶颈。 前向渲染（+ TBDR）： 带宽优化：利用片上内存暂存数据、HSR剔除无效像素、纹理缓存复用，显著降低系统内存访问。 移动端首选：与TBDR架构协同，平衡性能与功耗，适合移动设备的典型场景（少光源、透明物体）。 类比理解 # 延迟渲染（Gbuffer + IMR）：像快递公司每次配送都要先建一个庞大的中央仓库（Gbuffer），再多次往返取货送货，导致交通拥堵（带宽不足）。 前向渲染（+ TBDR）：快递员分片区配送（分块），每个片区的货物暂存在本地仓库（片上内存），仅最终合并到中央仓库，减少重复运输。 ","date":"25 May 2025","externalUrl":null,"permalink":"/posts/tbdr+forward-vs.-imr+deferred_-%E4%B8%A4%E7%A7%8D%E8%AE%BE%E5%A4%87%E7%9A%84%E4%B8%8D%E5%90%8C%E6%9D%83%E8%A1%A1/","section":"","summary":"算法原理解析","title":"TBDR+Forward vs. IMR+Deferred_ 两种设备的不同权衡","type":"posts"},{"content":"","date":"25 May 2025","externalUrl":null,"permalink":"/tags/%E5%85%89%E7%85%A7/","section":"Tags","summary":"","title":"光照","type":"tags"},{"content":"","date":"25 May 2025","externalUrl":null,"permalink":"/tags/%E6%9D%90%E8%B4%A8/","section":"Tags","summary":"","title":"材质","type":"tags"},{"content":"在 Unity 渲染管线里，顶点着色器（vertex）要把模型顶点转换到裁剪空间（clip space）里，然后在屏幕映射步骤顶点才会从裁剪空间映射到电脑屏幕上。\n当然，可以注意到，对于摄像机控件，经常会看到“正交模式”和“透视模式”。这两者的视锥体是不同的，正交的视锥体是正方体，透视的视锥体是金字塔形状的锥体，两者在inspector面板中的参数是：\n这两个模式的裁剪空间都是由6个面组成的封闭立方体组成，此时还没有变换到NDC坐标。超过这个立方体坐标都不再会被渲染。透视模式的两条平行直线可以相交于一点，但是在传统的笛卡尔xyz坐标系中确实永远不可能实现的，这就是xyzw的第四个分量w的功劳！\n在透视摄像机的画面（透视空间）里使用的并不是笛卡尔坐标系，为了描述透视空间，科学家提出了 “ 齐次坐标 ” 的概念：\n即，用 N+1 个数来表示 N 维空间中的点或向量，对于三维空间中的点，通常是使用 (x, y, z, w) 来表示三维空间中的点在齐次坐标空间中的位置。三维空间（笛卡尔坐标系）和齐次坐标系之间可以通过齐次除法进行相互转换，科学家定义的规则是：\n齐次坐标系的一个功能便是能描述透视空间下的renti对于上面的平行线相交于一点，这个点的齐次坐标便是（x，y，z，0）。因为这个w值转换为一般的三维坐标便是无穷大，但是在齐次坐标系中它又是一个确定的值——这是在一般的笛卡尔坐标系中做不到的。此外，太阳光用的也是这个思路：w设置为0，xyz仅用于表达方向，这就等价于平行光矢量。\n第二个功能就是能够描述坐标的位移；对于标准坐标系中，平移、旋转、缩放都需要不同的计算方式，尤其是位移变换，需要向量的加法；但是加法在性能消耗上是大于矩阵乘法的。而齐次坐标系，所有的仿射变换（包括平移、旋转、缩放等）都可以用一个单一的矩阵乘法来表示。这意味着复杂的变换序列可以被组合成单个矩阵，简化了变换流程。在视图空间进行设置时（点未作任何矩阵投影变换），\nw的值可以区分点和向量：1为点，0为向量。因为向量不需要平移。\n裁剪空间（clipspace）中的顶点是用齐次坐标表示的，在屏幕映射阶段里要对裁剪空间的顶点进行一个统一的齐次除法操作，来把顶点从齐次坐标系转换到笛卡尔坐标系的归一化设备坐标（Normalized Device Coordinates, NDC）空间里，经过这一步之后，裁剪空间将会变换到一个立方体内。\nOpenGL 和 DirectX 对 NDC 空间的定义有所不同，前者定义 NDC 空间的 xyz 取值范围是 [-1, 1] ，而后者定义 NDC 空间的 xy 取值范围为 [-1, 1]，z 的取值范围为 [0, 1]。而 Unity 选择了 OpenGL 的规范：\n要进行齐次除法就必须要获得正确的 w 分量，投影矩阵之前的 w(0和1) 分量并没有齐次除法的用途。\n所以，投影矩阵的一个作用就是：正确计算出从视图空间到透视空间中的顶点的 w 分量，以便在之后的屏幕映射步骤中进行齐次除法操作，当一个顶点通过透视投影矩阵变换后，其位置会变为齐次坐标形式(x′,y′,z′,w′)。注意，在裁剪空间这里的w′值并不是视图空间固定的1/0，而是取决于原始顶点的深度（即它离相机的距离）。具体来说，对于一个位于视图空间中的点(x,y,z)，经过透视投影矩阵变换后得到的新w\u0026rsquo;通常是基于该裁剪空间点的z值（深度）计算出来的。\n在裁剪空间中，完成所有必要的裁剪操作之后，为了将这些坐标映射到标准化设备坐标(NDC, Normalized Device Coordinates)空间，需要将齐次坐标除以w′（即进行透视除法：x′′=x′/w′,y′′=y′/w′,z′′=z′/w′）。这一过程使得远处的物体看起来更小，近处的物体看起来更大，从而实现了透视的效果。经过投影矩阵处理之后的顶点，将会给他的 w 分量赋予重要的含义。\n投影矩阵的另一个作用是：对顶点的位置进行一定的缩放，使得顶点转换到 NDC 空间的范围中，\n因为用视锥体的六个面来判断顶点是否在视锥体里，非常麻烦，我们希望用一个矩阵直接转换顶点进我们定义的视锥体里，转换之后，如果 x, y, z 符合条件：\n那么就会参与最终的渲染，全片完。\n顺带一提，对于正交摄像机的话，他的裁剪空间是一个长方体，拍摄出来的画面是可以用三维坐标来描述的，不需要齐次坐标来描述，所以正交摄像机的投影矩阵对 w 分量没有进行操作（或者理解为正交摄像机的裁剪空间是 w 恒等于 1 的齐次坐标空间）。\n","date":"23 May 2025","externalUrl":null,"permalink":"/posts/xyzw%E4%B8%AD%E7%9A%84w%E9%BD%90%E6%AC%A1%E5%9D%90%E6%A0%87%E7%B3%BB%E7%9A%84%E4%BD%9C%E7%94%A8/","section":"","summary":"渲染技术解析","title":"xyzw中的w：齐次坐标系的作用","type":"posts"},{"content":"","date":"11 May 2025","externalUrl":null,"permalink":"/tags/git/","section":"Tags","summary":"","title":"Git","type":"tags"},{"content":"","date":"11 May 2025","externalUrl":null,"permalink":"/tags/houdini/","section":"Tags","summary":"","title":"Houdini","type":"tags"},{"content":" 引言 # 这个契机其实是来自于我的一场面试\n面试官：来讲一下披风的物理状态是如何实现的？\n答：VAT（顶点形变贴图）\n面试官：要动态的\n答：magica clothes\n面试官：我要的是物理算法是如何实现的。\n我：。。。？我怎么知道？答：PhysX？\n面试官：我要的是物理效果的实现 PhysX、clothes组件本质上也和magica clothes是一种东西\n我：。。。。\n所以这个文章就诞生了 这次我来讲一下目前常用的物理算法的实现\nPart. 1 我们常见的应用？ # 如果我们讲引擎的话，那就直接上最常见的引擎：\nUnity Clothes组件 （默认PhysX） Unreal Engine Chaos Clothes组件（默认PhysX） 3dsMax/Maya PhysX；Bullet Blender： Bullet, MantraFlow Houdini: 多种解算器实现 Cinema 4D : X-Particles、RealFlow、Bullet 从上面的一些名字我们也了解了个大概，什么，还是不是很具体？那就来看PhysX对于物理模拟的分类：\n简单而言，可以分为：\nPhysX：\n布料和头发： 以 基于位置的动力学 (PBD) 为主流，辅以引导线和约束系统。 刚体： 以 基于冲量的解算器 (如顺序冲量) 为主，结合高效的碰撞检测算法。 软体：常用 PBD和形状匹配，对于高精度需求则可能涉及FEM。 PhysX Blast：\nPCG（Procedural Generation）： 断裂/破坏模拟 、程序化建筑生成。以Voronoi碎裂 (Voronoi Fracturing / Shattering) 为主，使用FEM进行辅助应力分析和裂纹扩展的算法集成。可以选择在准备模型文件前完成**预切割/预碎裂 (Pre-fracturing / Pre-shattering)抑或是层级破坏 (Hierarchical Destruction / Support Graphs)**分配权重，实现更低的性能损耗和更好的破坏效果。 PhysX Flow：\n** Particle Instancing: ** 液体、火焰、烟雾 的大批量粒子物理解算。 整个实现基于**网格的流体动力学（欧拉方法）、平流算法 (Advection Algorithms，常用半拉格朗日法 (Semi-Lagrangian Method) 或 MacCormack法 模拟速度场的粒子运动 ) 、压力求解 (Pressure Projection / Poisson Equation，使用压力泊松方程求解) 、涡度限制 (Vorticity Confinement) 以及燃烧和浮力模型 (Buoyancy Models) 进行模拟。有时会使用稀疏网格/自适应网格 (Sparse Grids / Adaptive Grids) 进行性能优化 ** Part 2. PhysX # 这是什么东西啊 我怎么一点都不懂？没事我也不懂（\n没关系，下面我们由浅入深了解一下常见的物理算法！\n质点-弹簧系统 (Mass-Spring Systems): # 这其实是我们最常见的系统，也就是直接由骨骼驱动。经常玩MMD的朋友会知道，MMD模型对于头发、胸部、裙子这种需要物理的地方都会存在骨骼绑定对应物件。 MMD的物理骨骼（Rigid Body/Joint设置）本质上是刚体+约束的组合，但其实在约束算法上并没有什么使用，对于头发和裙子更像是“串联刚体+弹簧约束”，用“物理骨骼”+“刚体”+“约束”来近似实现质点-弹簧系统的效果，与质点-弹簧系统的思想高度一致（而且算法和这个高度类似）。\n但是对于强调次世代模型工作流（如Marvelous Designer、Houdini）这种软件，在默认的参数预设中，对于头发抑或是反而没有骨骼绑定。这就是Mass-Spring Systems和有限元方法 (Finite Element Method - FEM) 的区别。有限元是什么在下面会提到。\n质点弹簧系统是最经典和基础的布料和头发模拟方法之一。同时也是Unity magica clothes的核心算法。它将物体（如布料或发束）离散化为一系列质点，这些质点之间通过弹簧连接。弹簧的力（拉伸、弯曲、剪切）根据胡克定律计算，然后通过牛顿第二定律（F=ma）更新质点的位置和速度。\n核心思路是：\n每个“骨骼”用一个质点表示，拥有位置、速度、质量等。 相邻两质点用“弹簧”连接，弹簧有弹性系数、阻尼、原长。 每帧根据弹簧力、重力、阻尼等计算每个质点的受力与运动。 有需要时加入碰撞检测和约束。 实现思路： 定义数据结构：质点、弹簧。 初始化链条：生成一串质点（用 Transform 表示），并设定它们之间的弹簧。 物理更新：每帧计算弹簧力和阻尼，更新质点位置。 骨骼绑定：将计算结果同步到动画骨骼或 Transform。 代码实现：\nusing UnityEngine; using System.Collections.Generic; // 质点节点 public class MassPoint { public Transform bone; // 关联骨骼 public Vector3 position; // 当前世界位置 public Vector3 velocity; // 当前速度 public float mass = 1.0f; // 质量 public MassPoint(Transform bone, float mass = 1.0f) { this.bone = bone; this.position = bone.position; this.velocity = Vector3.zero; this.mass = mass; } } // 弹簧约束 public class Spring { public int pointA, pointB; // 两端索引 public float restLength; // 原长 public float stiffness; // 弹性系数 public float damping; // 阻尼系数 public Spring(int a, int b, float restLength, float stiffness, float damping) { pointA = a; pointB = b; this.restLength = restLength; this.stiffness = stiffness; this.damping = damping; } } // 主组件 public class MassSpringHair : MonoBehaviour { public List\u0026lt;Transform\u0026gt; bones; // 需要模拟的骨骼链 public float stiffness = 500f; // 弹性 public float damping = 2f; // 阻尼 public float mass = 0.02f; // 质量 public Vector3 gravity = new Vector3(0, -9.81f, 0); private List\u0026lt;MassPoint\u0026gt; points; private List\u0026lt;Spring\u0026gt; springs; void Start() { Init(); } void Init() { points = new List\u0026lt;MassPoint\u0026gt;(); springs = new List\u0026lt;Spring\u0026gt;(); // 创建质点 foreach (var bone in bones) points.Add(new MassPoint(bone, mass)); // 创建弹簧（相邻骨骼之间） for (int i = 0; i \u0026lt; bones.Count - 1; i++) { float restLen = Vector3.Distance(bones[i].position, bones[i+1].position); springs.Add(new Spring(i, i+1, restLen, stiffness, damping)); } } void FixedUpdate() { float dt = Time.fixedDeltaTime; // 1. 力学积分（仅对非根节点） for (int i = 1; i \u0026lt; points.Count; i++) { var p = points[i]; // 重力 Vector3 force = gravity * p.mass; // 弹簧力（与前一节点） var s = springs[i-1]; var pA = points[s.pointA]; var pB = points[s.pointB]; Vector3 dir = pB.position - pA.position; float dist = dir.magnitude; dir.Normalize(); // 弹簧胡克定律 Vector3 springForce = -s.stiffness * (dist - s.restLength) * dir; // 阻尼 Vector3 relativeVel = pB.velocity - pA.velocity; Vector3 dampingForce = -s.damping * Vector3.Dot(relativeVel, dir) * dir; force += springForce + dampingForce; // 更新速度和位置 p.velocity += force / p.mass * dt; p.position += p.velocity * dt; } // 2. 根节点跟随动画骨骼 points[0].position = bones[0].position; points[0].velocity = Vector3.zero; // 3. 同步所有骨骼 for (int i = 0; i \u0026lt; points.Count; i++) bones[i].position = points[i].position; } } 约束动力学 (Constraint Dynamics) # 约束动力学(Constraint Dynamics)是物理引擎和机器人控制系统中处理复杂约束关系的核心技术，它能够在存在各种几何和运动学约束条件下，准确高效地计算物体运动状态。约束动力学存在两种主要实现方法：基于位置的PBD算法和隐式约束力算法。\n（约束动力学分支）基于位置的动力学 Position Based Dynamics (RBD) # Position-Based Dynamics（PBD）是一种基于位置约束的物理模拟算法，核心是约束满足（Constraint Satisfaction），即通过调整物体的位置来满足特定的物理约束条件（如距离约束、碰撞约束、面积约束等）。广泛应用于实时动态系统模拟（如布料、软体、流体等）。与上个算法，使用基于力或冲量的动力学方法不同，PBD通过直接操作物体的位置来满足物理约束，从而避免了数值积分中的不稳定性问题。不仅支持大量的并行计算，而且可以支持多种约束类型（碰撞约束、布料约束、流体约束）\n注意：PBD属于下面的约束动力学的一个分支\n基本流程\n对每个点应用外力（如重力），计算临时新位置（预测位置）。 对所有约束（弹簧、体积、碰撞等）进行多次迭代，直接“拉回/推开”点的位置，使其满足约束。 用修正后的位置更新速度，再同步到渲染模型或骨骼。 Ref：\nhttps://matthias-research.github.io/pages/publications/posBasedDyn.pdf https://www.cs.cmu.edu/~baraff/sigcourse/ （约束动力学分支）隐式约束力算法 # 隐式约束力算法通过直接求解约束力来确保约束条件得到满足。这类算法通常基于拉格朗日乘子法或线性互补问题(LCP)求解框架，适合处理刚性约束和复杂接触问题。\n什么是隐式？\n显式（Explicit）：\n直接用当前的力和状态，计算下一帧的位置和速度。例如欧拉显式积分，适合小步长、低刚性的模拟。 隐式（Implicit）：\n用下一帧的状态（未知）来计算力和速度，通常需要解方程组。适合高刚性或大步长的系统，更加稳定。 约束条件整合：将不同类型的约束(如等值约束、接触约束、摩擦约束)整合到统一的求解框架中。\n在模拟带有约束（如距离、体积、弯曲、碰撞等）的系统时，隐式约束力算法指的是：\n在积分过程中，把约束力以隐式方式耦合进整体动力学方程，一起求解。 这样做的好处是：即使约束很强、弹簧很硬，系统依然不会“爆炸”，不会因为步长过大而数值发散。\n有限元方法 (Finite Element Method - FEM) # FEM 是一种更精确但也更复杂的模拟方法。它将物体划分为更小的单元（例如三角形或四面体），并在这些单元上定义形变能量。通过最小化总能量来求解物体的平衡状态或动态行为。\n优点： 物理上非常精确，能够模拟复杂的材料属性和形变。 缺点： 计算量巨大，通常不适用于实时游戏，更多用于离线渲染或需要高精度模拟的场合。 应用： 高精度布料模拟、肌肉模拟等 链式结构/骨骼动力学 (Chain/Skeletal Dynamics) # 对于头发或一些垂坠的饰品，可以将其简化为一系列通过关节连接的刚性链条或骨骼。通过计算关节的旋转和父子骨骼的传递运动来模拟动态效果。通常会结合IK（反向动力学）或物理方法（如PBD）来驱动这些链条。\n优点： 相对简单，计算效率较高，容易与角色骨骼动画系统集成。 缺点： 对于大面积的布料模拟效果有限，更适合模拟线状或带状结构。 应用： 马尾辫、长发、飘带、项链等。 Verlet积分 (Verlet Integration): # Verlet 积分是一种数值积分方法，常用于模拟粒子系统。它的特点是不直接存储速度，而是根据当前位置、前一时刻位置和加速度来推算下一时刻的位置。 优点： 计算简单，稳定性较好，能耗散较小，适合约束多的系统。 缺点： 引入阻尼等效果可能不如其他方法直观。 应用： 布料模拟、绳索、毛发、粒子系统等。 实现细节： Verlet 积分的核心是存储当前位置和先前位置，速度由这两个位置的差异隐含给出。 ","date":"11 May 2025","externalUrl":null,"permalink":"/posts/%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BC%95%E6%93%8E%E7%89%A9%E7%90%86%E7%AE%97%E6%B3%95/","section":"","summary":"Blender导入Unity指南","title":"常见的引擎物理算法","type":"posts"},{"content":"","date":"11 May 2025","externalUrl":null,"permalink":"/tags/%E7%89%A9%E7%90%86/","section":"Tags","summary":"","title":"物理","type":"tags"},{"content":"__在Houdini中，Copy to Points (复制到点) 等节点在进行几何体实例化时，有一个非常重要的选项叫做 \u0026ldquo;Enforce Unique Name Attribute per instance\u0026rdquo;（为每个实例强制生成唯一名称属性）。\n作用是什么？ # 当你在Houdini中将一个几何体（例如一棵树、一块石头）复制到多个点上以创建大量实例时，这些实例在内部通常被视为“打包原语”（Packed Primitives）。默认情况下，这些打包原语可能没有一个唯一的标识符来区分彼此。\n\u0026ldquo;Enforce Unique Name Attribute per instance\u0026rdquo; 的作用是：\n它会在每个生成的打包原语（即每个实例）上创建一个唯一的字符串属性，通常这个属性的名称是 name。这个 name 属性的值会是类似 geo_0、geo_1、geo_2 这样的唯一标识符。\n为什么这很重要？\n这个唯一的名称属性对于许多下游工作流至关重要，特别是当你需要对每个实例进行单独的控制、识别或在渲染器、游戏引擎中进行特殊处理时：\n材质覆盖 (Material Overrides)： 允许你在渲染时为特定的实例应用不同的材质，即使它们共享相同的原始几何体。 属性变化 (Attribute Variations)： 即使实例共享相同的源几何体，你也可以通过这个唯一的 name 属性来引用它们，并为每个实例设置独特的属性（如颜色、缩放、旋转偏移等），而无需打破实例化的效率优势。 选择和调试 (Selection \u0026amp; Debugging)： 在复杂的场景中，你可以通过这个唯一的名称来选择或识别特定的实例，这对于调试和精确控制非常有用。 导出到其他软件/游戏引擎： 许多外部渲染器（如Redshift, Arnold, V-Ray, Karma）和游戏引擎（如Unity, Unreal Engine）在处理实例时，会查找这种唯一的名称属性，以便进行更精细的控制或优化。 举例说明 # 假设你正在创建一个森林场景，需要复制大量的树木。\n步骤：\n创建源几何体： 放置一个 Box SOP，作为你的“树”的简化模型。 创建散布点： 放置一个 Grid SOP。 连接一个 Scatter SOP 到 Grid，生成一些点作为树木的放置位置。 复制到点： 放置一个 Copy to Points SOP。 将 Box 连接到 Copy to Points 的第一个输入（要复制的几何体）。 将 Scatter 的输出连接到 Copy to Points 的第二个输入（点）。 启用唯一名称属性： 选择 Copy to Points 节点。 在参数面板中，找到 \u0026ldquo;Packed Primitives\u0026rdquo; 标签页。 勾选 \u0026ldquo;Enforce Unique Name Attribute per instance\u0026rdquo; 选项。 观察结果：\n现在，如果你在 Copy to Points 节点之后连接一个 Null 节点，并打开几何体电子表格（Geometry Spreadsheet），将显示类型切换为 Primitives。 你会发现每个打包原语（packedfragment）上都多了一个名为 name 的字符串属性，其值是唯一的，例如 box_0、box_1、box_2 等等。 应用场景（材质覆盖）：\n现在，你有了这些带有唯一名称的树木实例，你可以利用它们来做一些事情。例如，在Karma渲染器中，你可以通过这个 name 属性来覆盖特定树的材质：\n在你的场景中，为 Box 创建一个默认材质（例如，绿色）。 假设你想让 box_5 这个实例变成红色。 在Karma的材质覆盖系统（或者其他渲染器的类似机制）中，你可以指定一个规则：当实例的 name 属性是 box_5 时，将其材质替换为红色材质。 如果没有这个选项会怎样？\n如果你不勾选 \u0026ldquo;Enforce Unique Name Attribute per instance\u0026rdquo;，那么所有的打包原语可能都没有这个唯一的 name 属性。这意味着渲染器或下游工具将无法轻易地识别和区分每个单独的实例，你就无法对它们进行精细的、基于实例的材质覆盖或属性修改，除非你打破实例（这将大大增加内存占用和文件大小）。\n总之，\u0026ldquo;Enforce Unique Name Attribute per instance\u0026rdquo; 提供了一种高效且灵活的方式，来为大量实例提供唯一的标识符，从而实现更高级的控制和定制化。\n它更常出现在创建实例的节点上，最典型的就是 Copy to Points。\n然而，Pyro模拟确实可以与实例化的概念结合起来，尤其是在以下两种常见场景中：\n实例化Pyro的发射源 (Instancing Pyro Emitters): 你可能创建了多个独立的几何体作为Pyro的发射源（比如，多个火把、多个爆炸点）。 这些几何体本身可能是通过 Copy to Points 节点实例化出来的，而在这个 Copy to Points 节点上，你就会勾选“Enforce Unique Name Attribute per instance”来为每个发射源提供唯一的ID。 然后，这些带有唯一ID的实例化的几何体被送入 Pyro Source 节点，作为烟雾或火焰的发射区域。 在这种情况下，这个选项是在Pyro模拟的上游使用的。 实例化烘焙好的Pyro模拟结果 (Instancing Baked Pyro Results): 这是更常见，也更容易让你联想到“Pyro中出现实例选项”的场景。 你可能模拟了一个小型的、通用的火焰或烟雾效果（例如，一个小型爆炸、一团烟雾）。 为了在场景中重复使用这个效果，你会将这个模拟结果烘焙成一个静态的体积（Volume），然后将这个体积转换为一个“打包原语”（Packed Primitive）。 接着，你会使用 Copy to Points 节点将这个打包好的Pyro效果复制到场景中的多个位置。 在这个 Copy to Points 节点上，你就会找到并使用“Enforce Unique Name Attribute per instance”选项。 Pyro中“Enforce Unique Name Attribute per instance”的例子 # 我们以第二种场景为例：复制多个烘焙好的小型爆炸效果。\n目标： 在一个场景中放置多个相似但独立的爆炸效果，并能够对每个爆炸进行单独的材质或属性调整。\n工作流示例：\n创建单个Pyro爆炸模拟： 放置一个 Sphere SOP (作为爆炸源)。 连接一个 Pyro Source SOP 到 Sphere，将其转换为烟雾和火焰的体积属性。 连接一个 Pyro Solver SOP 到 Pyro Source，进行爆炸模拟。调整参数以获得一个满意的单次爆炸效果。 为了性能，通常会连接一个 Pyro Bake Volume SOP 到 Pyro Solver，将模拟结果烘焙成静态体积。 将Pyro结果打包成实例： 在 Pyro Bake Volume 之后，连接一个 Convert VDB SOP。 在 Convert VDB 节点上，将 \u0026ldquo;Convert To\u0026rdquo; 设置为 Polygons (或者其他你希望打包的几何体类型，但对于体积，通常是转换为VDB，然后直接打包VDB)。 关键步骤： 连接一个 Assemble SOP 到 Convert VDB。 在 Assemble 节点上，勾选 \u0026ldquo;Create Name Attribute\u0026rdquo; (通常会默认勾选)。 最重要的是，勾选 \u0026ldquo;Create Packed Geometry\u0026rdquo;。这将把你的烘焙好的Pyro体积转换为一个可以被实例化的打包原语。 创建散布点： 放置一个 Grid SOP。 连接一个 Scatter SOP 到 Grid，生成你希望放置爆炸效果的点。 复制Pyro实例到点： 放置一个 Copy to Points SOP。 将 Assemble 节点的输出（你的打包Pyro爆炸）连接到 Copy to Points 的第一个输入。 将 Scatter 节点的输出（散布点）连接到 Copy to Points 的第二个输入。 启用“Enforce Unique Name Attribute per instance”： 选择 Copy to Points 节点。 在参数面板中，导航到 \u0026ldquo;Packed Primitives\u0026rdquo; 标签页。 勾选 \u0026ldquo;Enforce Unique Name Attribute per instance\u0026rdquo;。 结果和用途：\n现在，你的场景中会有多个独立的爆炸效果实例。每个实例都带有一个唯一的 name 属性（例如 explosion_0, explosion_1 等）。\n渲染器中的材质变化： 你可以利用这个唯一的 name 属性，在渲染器（如Karma, Redshift, Arnold）中为特定的爆炸实例应用不同的颜色、亮度或任何其他材质属性覆盖。例如，让 explosion_3 看起来更暗，或者 explosion_7 带有蓝色的火焰。 游戏引擎中的控制： 如果你将这些带有唯一名称的实例导出到游戏引擎，引擎可以识别这些唯一的ID，从而允许游戏逻辑单独控制每个爆炸实例的生命周期、播放速度或特效参数。 Houdini内部操作： 你也可以在Houdini内部，使用 Group Expression 或 Blast 节点，通过 name 属性来选择或删除特定的爆炸实例。 所以，虽然“Enforce Unique Name Attribute per instance”选项不在 Pyro Solver 或 Pyro Source 节点本身，但它在处理和实例化Pyro模拟结果时，是实现高级控制的关键步骤。你可能是在这种“Pyro结果实例化”的工作流中看到过它。\n","date":"10 May 2025","externalUrl":null,"permalink":"/posts/houdini%E4%B8%AD%E7%9A%84enforce-unique-name-attribute-per-instance/","section":"","summary":"问题解决方案","title":"Houdini中的Enforce Unique Name Attribute per instance","type":"posts"},{"content":"虽然Rigify并没有像Unity那样的骨骼重定向功能，但是还有两个工具可以快速帮助我们将MMD 的骨骼模式调整为Rigify的模式。（因为带了控制器果然还是爽啊 直接薄纱传统k帧）\n另：其实Blender中是有重定向骨骼并转换为新骨骼组的功能，名叫 autorig pro（付费） 但是骨骼重定向的时候只会允许出现英文字符 也就是说你还要过一遍mmd tools的翻译\n5.14更新：我好像看到了一个新的叫做Bone Animation Copy Tool？\nMiku Miku Rig： # 国人开发的转换插件，因此对于中文的适配也会更好。https://github.com/LaoBro/Miku_Miku_Rig 经过转换后的骨骼将会复制一份作为存在（也就是说两个都可以控制人物），并且不再接受来自VRM的动作文件转换。适合想要快速手k生草动画的时候（\nUuunya Tools： # https://github.com/MMD-Blender/blender_mmd_uuunyaa_tools 这是MMD Tools的更加复杂的版本，作者的原句是：mmd_uuunyaa_tools is a blender addon for adjust scenes, models and materials in concert with UuuNyaa/blender_mmd_tools .\n需要安装MMD Tools作为前置安装环境。\n教程【Rigify: 导入VMD动作并修改，导出VMD，使用FBX动作】 https://www.bilibili.com/video/BV16V4y1q7dh 需要注意的是，虽然和MMR相比，Uuunya允许使用VRM的动作文件，但是由于某些不可抗力（其实这个插件的VRM动作文件最后也是嵌套到Rigify中去了，而Rigify的权重，IK，蒙皮等配置一般都会是有默认的配置的），会导致从VRM导入到控制器后会出现一些比例失调的问题。\n看了好久还是感叹maya确实不可撼动（暂时的） 毕竟Blender可以操作的地方太多了 对于对齐工作流绝对是一大弊端\n","date":"2 May 2025","externalUrl":null,"permalink":"/posts/mikumiku-rig--uuunya-tools/","section":"","summary":"Blender导入Unity指南","title":"MikuMiku Rig \u0026\u0026 Uuunya Tools","type":"posts"},{"content":"","date":"2 May 2025","externalUrl":null,"permalink":"/tags/%E6%95%99%E7%A8%8B/","section":"Tags","summary":"","title":"教程","type":"tags"},{"content":"这个博客的起源是我在参考其他人的思路的时候 ，日出日落的天空盒变化需要通过参考灯光向量的y轴进行lerp操作。但是Unity6中似乎无法复现，在日出日落（0.25时间，0.75时间）会出现明显的跳变现象。经过反复折磨调试发现似乎是Unity采用了投机取巧的方式。\n由于我的灯光向量（Rotation）从-90开始计数，根据时间的变化逐渐加满，结果灯光向量在加到360的时候居然会直接归0？方向确实也没有什么变化，灯光的方向也是对的，但是会导致天空盒出现十分明显的跳变（跳个几帧又恢复为原来的情况）。故想到了使用时间控制系统中的Time来控制天空盒的Lerp。我现在使用的方式是 Shader.SetGlobal的方法进行变量共享 。大致原理是：\nC# 脚本侧\n使用 Unity 提供的 API，例如 Shader.SetGlobalFloat、Shader.SetGlobalVector 等，将值传递给 Shader Shader 侧\n在 Shader 中定义与 C# 脚本中对应的全局变量（如 _TimeOfDay），然后通过变量的值来动态控制材质的外观或渲染效果 共享机制\n通过全局变量的方式，Shader 的多个实例（材质）可以共享同一个变量值，避免逐一设置变量值的繁琐 C#脚本侧 # public class DayNightCycle : MonoBehaviour { [Header(\u0026#34;Time Settings\u0026#34;)] public float dayDuration = 120f; public float maxSunIntensity = 1f; public float minSunIntensity = 0f; [Header(\u0026#34;Light Settings\u0026#34;)] public Light sunLight; public Gradient lightColor; [Range(0, 1)] public float timeOfDay = 0f; // 首先在class中声明一个timeOfDay private float timeSpeed; [Header(\u0026#34;Bloom Settings\u0026#34;)] public Volume postProcessingVolume; private Bloom bloom; public float maxBloomIntensity = 10f; public float minBloomIntensity = 0f; } 我们先进行声明，然后进入Start中将这个变量共享：\nvoid Start() { // 在这里设置全局 Shader 属性 Shader.SetGlobalFloat(\u0026#34;_TimeOfDay\u0026#34;, timeOfDay); timeSpeed = 1f / dayDuration; //Debug if (postProcessingVolume != null) { if (!postProcessingVolume.profile.TryGet(out bloom)) { Debug.LogError(\u0026#34;无法获取Bloom组件！请确保Volume中添加了Bloom效果。\u0026#34;); } } else { Debug.LogError(\u0026#34;未指定Post Processing Volume！\u0026#34;); } } 由于时间是变化的，因此要在Update中持续进行同步：\nvoid Update() { // 广播时间属性，让天空盒可以根据时间变化。前面是Shader中的变量名，后面是C#的变量名 Shader.SetGlobalFloat(\u0026#34;_TimeOfDay\u0026#34;, timeOfDay); timeOfDay += timeSpeed * Time.deltaTime; if (timeOfDay \u0026gt; 1f) { timeOfDay = 0f; } UpdateSunRotation(); UpdateSunColor(); UpdateSunIntensity(); UpdateBloomIntensity(); //Debug if (bloom != null) { Debug.Log($\u0026#34;Bloom intensity: {bloom.intensity.value}, Time: {timeOfDay}\u0026#34;); } else { Debug.LogWarning(\u0026#34;Bloom is null!\u0026#34;); } } Shader侧 # 在struct完appdata和v2f之后的变量声明中进行：\nfloat _Test, addSunandMoon, _addHorizon, _addGradient, _addCloud, _addStar, _MirrorMode; float _SunRadius, _MoonRadius, _MoonOffset, _MoonTexScale, _MoonTexBrightness, _MoonRotation; float _TimeOfDay;//在这里进行声明，注意要符合类型 float _MaxCloudHeight; float4 _DayTopColor, _DayBottomColor, _NightBottomColor, _NightTopColor, _StarsSkyColor; float4 _HorizonLightNight, _HorizonLightDay, _HorizonColorDay, _HorizonColorNight, _SunSet, _SunColor, _MoonColor; float4 _CloudColorDayMain, _CloudColorDaySec, _CloudColorNightMain, _CloudColorNightSec; float _HorizonBrightness, _MidLightIntensity, _CloudBrightnessDay, _CloudBrightnessNight, _Fuzziness, _FuzzinessSec, _DistortionSpeed, _CloudNoiseSpeed, _CloudNoiseScale, _DistortScale, _StarsCutoff, _StarsSpeed, _CloudCutoff, _CloudSpeed, _HorizonHeight, _HorizonIntensity, _CloudScale, _StarScale; sampler2D _Stars, _CloudNoise, _Cloud, _DistortTex, _MoonTex; 然后再fragment部分就可以计算了：\n//...other code float dayNightTransition; if (_TimeOfDay \u0026lt; 0.25f || _TimeOfDay \u0026gt; 0.75f) { // 夜晚时段 dayNightTransition = 0; } else { // 白天时段 dayNightTransition = 1; } // 在日出日落时添加平滑过渡 if (_TimeOfDay \u0026gt;= 0.2f \u0026amp;\u0026amp; _TimeOfDay \u0026lt;= 0.3f) { // 日出过渡 dayNightTransition = smoothstep(0.2f, 0.3f, _TimeOfDay); } else if (_TimeOfDay \u0026gt;= 0.7f \u0026amp;\u0026amp; _TimeOfDay \u0026lt;= 0.8f) { // 日落过渡 dayNightTransition = smoothstep(0.8f, 0.7f, _TimeOfDay); } float3 skyGradients = lerp(gradientNight, gradientDay, dayNightTransition); //...other code 自此变量便可以被共享并同步，根据时间的 变化进行lerp（夜晚、日出日落、白天）。\n还有要注意，这个Shader的属性，必须要这个脚本挂在在场景中。\n还有没有其他的方式进行共享？ # 有的兄弟有的！\nMaterial.Set*适合单个材质，简单易用，适合独立材质的变量传递，但注意会导致材质实例化 MaterialPropertyBlock可以作为单个渲染器动态传递变量，避免材质实例化，适合动态属性设置 ComputeBuffer传递数组或结构体；适合传递大规模数据，如粒子系统、动态网格等 cbuffer / uniform适用于单个材质，高效传递简单变量，适合基本场景 全局关键字/全局范围：用于控制 Shader 的功能开关，适合启用/禁用特效或功能模块 全局纹理或 RenderTexture：作用于单个材质或全局，传递动态纹理或图像数据，适合后处理或实时渲染场景 Shader.SetGlobal*：全局范围，作为API可以高效传递全局变量，适合全局控制（如时间、环境光） GraphicsBuffer（GBUFFER）是更加高级 GPU 数据传输，具有高性能，适合高级计算场景（如粒子模拟、大规模数据处理） ","date":"10 March 2025","externalUrl":null,"permalink":"/posts/c%23%E8%84%9A%E6%9C%AC%E5%92%8Cshader%E4%B8%AD%E5%85%B1%E4%BA%AB%E5%90%8C%E6%AD%A5%E5%8F%98%E9%87%8F/","section":"","summary":"问题解决方案","title":"C#脚本和Shader中共享同步变量","type":"posts"},{"content":"Post Processing Stack (PPS) 是Unity引擎中的一个模块，用于管理和应用各种后处理效果到渲染的图像上。它允许开发者和艺术家在最终图像输出之前，对场景的渲染结果进行一系列的图像处理操作，从而增强视觉效果或实现特定的艺术风格。对这个专门的后处理模块，我们成为Unity Post Processing Stack（后处理堆栈），因为在这个组件中多个后处理可以按照顺序依次实现，像一个栈的存入弹出，故得名。目前根据实现方法分为V1、V2和V3.\nhttps://github.com/Unity-Technologies/PostProcessing?tab=readme-ov-file (来自Unity Graphic项目的branch。主项目在这里https://github.com/Unity-Technologies/Graphics ）\nPPS V1 # 类似于启动时的bat/Tags文件，没有明确的「配置文件」概念，所有效果都集中在脚本中控制；基本上通过直接访问 Camera 的组件来应用效果。优点是可以快速实现简单的效果，不用考虑override/sealed等覆写和HLSL交互实现。缺点是难以实现高级的后处理效果。\nvar postProcessing = Camera.main.GetComponent\u0026lt;PostProcessingBehaviour\u0026gt;(); postProcessing.profile.bloom.enabled = true; // 启用 Bloom 效果 PPS V2 # 是目前使用较多的后处理标准。因为Unity在PPS V2采用模块化设计，效果分离为独立模块，易于扩展和控制；对应的代码/模块被储存在存储在 PostProcessingProfile 中，便于共享和编辑 。 （如 PostProcessEffectSettings和PostProcessEffectRenderer 和Shader之间互相隔离）\n核心功能是利用PostProcessEffectSettings 和 PostProcessEffectRenderer实现自定义的扩展效果。\n在 V2 中，自定义效果需要继承以下两个核心类：\n**PostProcessEffectSettings**\n用于定义自定义效果的参数和设置。 **PostProcessEffectRenderer**\n用于实现效果的渲染逻辑。 using UnityEngine; using UnityEngine.Rendering.PostProcessing; // 定义自定义效果的参数 [System.Serializable] [PostProcess(typeof(CustomEffectRenderer), PostProcessEvent.AfterStack, \u0026#34;Custom/CustomEffect\u0026#34;)] public class CustomEffect : PostProcessEffectSettings { public FloatParameter intensity = new FloatParameter { value = 1.0f }; // 参数1 public ColorParameter tint = new ColorParameter { value = Color.white }; // 参数2 } using UnityEngine; using UnityEngine.Rendering.PostProcessing; public class CustomEffectRenderer : PostProcessEffectRenderer\u0026lt;CustomEffect\u0026gt; { public override void Render(PostProcessRenderContext context) { var sheet = context.propertySheets.Get(Shader.Find(\u0026#34;Hidden/CustomEffectShader\u0026#34;)); sheet.properties.SetFloat(\u0026#34;_Intensity\u0026#34;, settings.intensity); sheet.properties.SetColor(\u0026#34;_Tint\u0026#34;, settings.tint); // 应用 Shader 效果 context.command.BlitFullscreenTriangle(context.source, context.destination, sheet, 0); } } 工作原理 # PostProcessEffectSettings 类描述了效果的配置，包括所有参数（如强度、颜色等）。 PostProcessEffectRenderer 类负责渲染这些效果，通常需要使用 Shader 实现具体的视觉效果。 https://www.youtube.com/watch?v=ehyMwVnnnTg 讲述了如何使用V2标准实现角色描边。\nPPS V3 # PPS V3将特性完全集成到 Unity 的 URP（Universal Render Pipeline） 和 HDRP（High Definition Render Pipeline），不再作为独立插件，而是作为渲染管线的一部分。后处理效果将完全通过 Volume Profile 管理\n效果参数不再通过 PostProcessVolume 调整，而是直接访问 Volume 组件。\nusing UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class URPPostProcessingExample : MonoBehaviour { public Volume volume; void Start() { // 获取 Volume Profile 中的 Bloom 设置 if (volume.profile.TryGet\u0026lt;Bloom\u0026gt;(out var bloom)) { bloom.intensity.value = 3f; // 修改强度 } } } ","date":"6 March 2025","externalUrl":null,"permalink":"/posts/unity%E7%9A%84post-processing%E6%A0%87%E5%87%86v1v2v3/","section":"","summary":"Git配置指南","title":"Unity的Post-Processing标准（V1、V2、V3）","type":"posts"},{"content":"这个脚本的核心任务是：\n遍历你指定的 Markdown 文件。 查找文件中所有符合语雀图片格式的 URL。 下载这些 URL 指向的图片。 将图片保存到本地（最好是与 Markdown 文件放在一起，利用 Hugo 的页面捆绑功能）。 替换 Markdown 文件中原来的网络 URL 为新的本地路径。 下面，我将为你提供一个开箱即用的 Python 脚本，并详细解释如何使用它来完美解决你的问题。\n方案：使用 Python 脚本自动化处理 # 这个方案是目前最灵活、最一劳永逸的。我们将编写一个 Python 脚本，它会自动将你的 Markdown 文件转换为 Hugo 推荐的 页面捆绑（Page Bundle） 结构，并将所有图片下载到文章对应的文件夹内。\n准备工作 # 安装 Python: 如果你的电脑还没有安装 Python，请先从 python.org 下载并安装。 安装 requests 库: 这是一个用来发送网络请求（如下载图片）的库。在你的终端或命令行中运行： pip install requests Python 脚本 (yuque_to_hugo.py) # 请将以下代码保存为一个名为 yuque_to_hugo.py 的文件，并将它放在你 Hugo 项目的根目录（与 config.toml 和 content 文件夹同级）。\nimport os import re import requests import hashlib from urllib.parse import urlparse import shutil # --- 配置区 --- # 你从语雀导出的 Markdown 文件存放的目录 # 假设你把它们都放在了 content/posts/yuque-import 目录下 MARKDOWN_DIR = \u0026#34;content/posts/yuque-import\u0026#34; # 语雀图片链接的特征，用于正则匹配 # 修正点: 使用了非捕获分组 (?:...) 来避免产生额外的捕获结果 YUQUE_CDN_PATTERN = r\u0026#34;https?://(?:cdn\\.nlark\\.com|aliyuncs\\.com)/yuque\u0026#34; # --- 配置区结束 --- # 正则表达式，用于匹配 Markdown 中的图片链接 # 现在它会正确地只捕获两个部分：1.alt文本, 2.图片URL IMG_REGEX = re.compile(r\u0026#34;!\\[(.*?)\\]\\((%s.*?)\\)\u0026#34; % YUQUE_CDN_PATTERN) def process_markdown_file(md_file_path): \u0026#34;\u0026#34;\u0026#34;处理单个 Markdown 文件\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;--- 开始处理文件: {md_file_path} ---\u0026#34;) # 1. 将普通 .md 文件转换为 Page Bundle 结构 dir_path, file_name = os.path.split(md_file_path) base_name, ext = os.path.splitext(file_name) # 如果已经是 index.md 或 _index.md，则直接使用其所在目录 if file_name.lower() in [\u0026#34;index.md\u0026#34;, \u0026#34;_index.md\u0026#34;]: bundle_dir = dir_path # 如果是这种情况，md_file_path 已经是正确的路径，不需要移动 else: bundle_dir = os.path.join(dir_path, base_name) new_md_path = os.path.join(bundle_dir, \u0026#34;index.md\u0026#34;) if not os.path.exists(bundle_dir): os.makedirs(bundle_dir) print(f\u0026#34;创建页面捆绑目录: {bundle_dir}\u0026#34;) # 移动并重命名 md 文件 shutil.move(md_file_path, new_md_path) md_file_path = new_md_path print(f\u0026#34;已将 {file_name} 移动到 {new_md_path}\u0026#34;) # 2. 读取新的 md 文件内容 with open(md_file_path, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: content = f.read() # 3. 查找所有匹配的图片链接 # 修正后的 findall 将只返回 (alt_text, url) 这样的二元组 images = IMG_REGEX.findall(content) if not images: print(\u0026#34;未找到需要处理的语雀图片。\\n\u0026#34;) return print(f\u0026#34;找到 {len(images)} 张语雀图片，开始下载和替换...\u0026#34;) # 4. 遍历所有找到的图片链接 for alt_text, url in images: try: # 清理 URL，移除 # 后面的参数 clean_url = url.split(\u0026#39;#\u0026#39;)[0] # 生成一个简短且唯一的文件名，避免中文或特殊字符问题 # 使用 URL 的 MD5 哈希值前8位作为文件名 file_ext = os.path.splitext(urlparse(clean_url).path)[1] or \u0026#39;.png\u0026#39; # 如果没有后缀，默认为.png file_hash = hashlib.md5(clean_url.encode()).hexdigest()[:8] new_filename = f\u0026#34;{file_hash}{file_ext}\u0026#34; # 图片要保存的本地路径 local_image_path = os.path.join(bundle_dir, new_filename) # 下载图片 if not os.path.exists(local_image_path): print(f\u0026#34; -\u0026gt; 正在下载: {clean_url}\u0026#34;) headers = {\u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0\u0026#39;} # 模拟浏览器，防止被禁 response = requests.get(clean_url, headers=headers, stream=True) response.raise_for_status() # 如果下载失败则抛出异常 with open(local_image_path, \u0026#34;wb\u0026#34;) as f: for chunk in response.iter_content(chunk_size=8192): f.write(chunk) print(f\u0026#34; 保存成功: {local_image_path}\u0026#34;) else: print(f\u0026#34; -\u0026gt; 图片已存在，跳过下载: {new_filename}\u0026#34;) # 替换 Markdown 内容中的旧 URL 为新本地路径 # 注意：这里我们只替换文件名，因为图片和md文件在同一目录 original_markdown_link = f\u0026#34;![{alt_text}]({url})\u0026#34; new_markdown_link = f\u0026#34;![{alt_text}]({new_filename})\u0026#34; content = content.replace(original_markdown_link, new_markdown_link) except requests.exceptions.RequestException as e: print(f\u0026#34; 下载失败: {url}, 错误: {e}\u0026#34;) except Exception as e: print(f\u0026#34; 处理失败: {url}, 错误: {e}\u0026#34;) # 5. 将修改后的内容写回文件 with open(md_file_path, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: f.write(content) print(f\u0026#34;文件 {md_file_path} 处理完成。\\n\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: if not os.path.isdir(MARKDOWN_DIR): print(f\u0026#34;错误: 目录 \u0026#39;{MARKDOWN_DIR}\u0026#39; 不存在。请检查配置。\u0026#34;) else: # 遍历目录下的所有 .md 文件 for root, _, files in os.walk(MARKDOWN_DIR): # 创建一个文件列表的副本进行迭代，因为我们可能会在循环中重命名文件 for file in list(files): if file.endswith(\u0026#34;.md\u0026#34;): process_markdown_file(os.path.join(root, file)) print(\u0026#34;所有 Markdown 文件处理完毕！\u0026#34;) 如何使用这个脚本？ # 导出并放置文件: 从语雀批量导出你的文章为 Markdown 格式。 在你的 Hugo 项目的 content/posts/ 目录下，创建一个新文件夹，比如 yuque-import。 将所有导出的 .md 文件复制到 content/posts/yuque-import/ 目录中。 配置脚本: 打开 yuque_to_hugo.py 文件。 确认 MARKDOWN_DIR 变量的值与你上一步创建的文件夹路径一致。默认是 content/posts/yuque-import，如果你的路径不同，请修改它。 运行脚本: 打开你的终端（命令行工具）。 确保你当前位于 Hugo 项目的根目录。 运行脚本： python yuque_to_hugo.py 查看结果: 脚本运行后，你会看到一系列处理日志。 完成后，检查你的 content/posts/yuque-import/ 文件夹。你会发现原来的 文章名.md 文件都变成了 文章名/index.md 的结构，并且每个文件夹里都包含了该文章下载好的图片。 打开任意一个 index.md 文件，你会看到图片链接 [...](https://cdn.nlark.com/...) 已经被替换成了 [...](abcdef12.png) 这样的本地链接。 脚本工作流程详解 # 转换为页面捆绑: 脚本首先将 文章A.md 这样的文件，转换成 文章A/index.md 的目录结构。这是 Hugo 的最佳实践，便于管理与文章相关的资源。 正则匹配: 使用正则表达式精准地找出所有语雀的图片链接。 生成唯一文件名: 直接使用 URL 中的文件名可能很长或包含特殊字符。脚本通过计算 URL 的 MD5 哈希值来生成一个简短、唯一且安全的新文件名，如 abcdef12.png。 下载与保存: 模拟浏览器下载图片，并将其保存在与 index.md 同一个文件夹下。 路径替换: 将 Markdown 中的长 URL 替换为新的、简单的本地文件名。因为图片和 index.md 在同一目录，所以可以直接引用文件名，非常简洁。 现在，你只需要 hugo server 启动本地预览，或者直接 git add ., git commit, git push 部署，你的博客就会完美地显示所有图片，并且这些图片已经成为了你博客项目的一部分，不再依赖语雀的服务器。这个脚本可以反复运行，以后每次从语雀导出新文章，只需重复上述步骤即可。\n","date":"3 March 2025","externalUrl":null,"permalink":"/posts/yuque-exporter%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/","section":"","summary":"Git配置指南","title":"Yuque Exporter使用说明","type":"posts"},{"content":"具体的情况是：在安装了20并且patch了激活证之后，不论是E盘还是C盘，在打开Davinci都会出现在加载插件部分闪退，但是任务管理器中的后台任务仍然存活\n原理：直接删掉C:\\Program Files\\Common Files\\OFX\\Plugins里面的所有插件，有可能是FilmConvert, RedGiant, iZotope之类的其中某个插件跟达芬奇有冲突，可以一个个排查，我就是整个直接删掉TopazAI插件后达芬奇就能启动了/好吧其实是直接把插件放到了另一个地方\n","date":"24 February 2025","externalUrl":null,"permalink":"/posts/%E5%85%B3%E4%BA%8Eda-vinci%E5%AE%89%E8%A3%85%E5%90%8E%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E7%9A%84%E9%97%AE%E9%A2%98/","section":"","summary":"问题解决方案","title":"关于Da Vinci安装后无法启动的问题","type":"posts"},{"content":"","date":"16 February 2025","externalUrl":null,"permalink":"/categories/animation/","section":"Categories","summary":"","title":"Animation","type":"categories"},{"content":"","date":"16 February 2025","externalUrl":null,"permalink":"/categories/retargeting/","section":"Categories","summary":"","title":"Retargeting","type":"categories"},{"content":"","date":"16 February 2025","externalUrl":null,"permalink":"/categories/unity/","section":"Categories","summary":"","title":"Unity","type":"categories"},{"content":" 前置可选条件：package manager中加入了Animation Rigging（骨骼可视化Bone Renderer）和MMD4Macanim（用于把pmx转换为fbx）\n在Unity中常常会遇见使用不同角色的动画，也就是Bones Retargeting系统。可以在Project中点击fbx后的Rig分支选项看到目前的状态。\nAnimation Type：Generic、Humanoid、None、Legacy。\nGeneric：导入fbx的时候默认继承的选项，也就是不更改任何的骨骼名称，直接使用fbx内的所有骨骼命名；\nLegacy：一个比较老的标准，常见于很久以前的Builtin管线，如果不是从老项目挪过来的资源不会使用\nHumanoid：重点讲这个东西，这是Retarget的重点。\nHumanoid： # 从generic更改为Humanoid后，Unity会尝试根据内部的描述文件逐一进行描述模糊匹配（Postprocessors的描述文件）。\nDefination：\n1、 Create from this model： 直接使用当前FBX文件里的骨骼结构，自动生成一个新的Avatar（骨骼映射）。\n2、 Copy from other avatar： 用另一个FBX或Prefab中已经创建好的Avatar（骨骼映射）。 优势是可以直接沿用所有骨骼的名字，不需要进行映射， 动画资产更容易批量复用。\nSkinWeights：\n这是蒙皮权重的设置，用来控制每个顶点最多受多少根骨骼影响。\nStandard是表明每个顶点最多有4个骨骼可以参与并控制进行骨骼影响。 顶点权重大于4的，只保留最重要的4个，其余自动舍弃或归零。\nCustom：可以选择并允许你指定“每顶点允许的骨骼数量”，比如2、4、8等（需要在Graphics Settings里自定义）。 但是一般我们只需要选择4个即可。因为大多数GPU对每个顶点可参与变形的骨骼数量是有限制的，4是最常见的上限。\n这和DCC中的蒙皮有什么区别？ # Unity的Skin Weights设置，决定了建模软件里“每个顶点蒙皮权重”最多有几组能被保留和使用。\n在Blender、Maya等3D建模/动画软件中，你可以给每个顶点分配任意数量的权重，比如1、2、4、甚至10根骨骼影响一个顶点。 一般来说， 权重越多，顶点变形越平滑，但数据量越大，性能损耗也增加。 也可以自由涂抹、调整每个骨骼对每个顶点的影响比例。\nUnity在导入模型时，会根据你在Skin Weights（蒙皮权重）选项里选的最大权重数，对每个顶点做一次“筛选”。\n选“4 Bones”，就只保留每个顶点影响最大的4个骨骼的权重，其余全部舍弃（并重新归一化）。\n如果你在建模软件里有顶点被5、6、8根骨骼影响，导入到Unity后，只会留下影响最大的4个，其余全部丢弃。\n这一步只发生在导入时，跟你建模软件里的原始权重关系密切，但会被Unity“削减”到上限如果超过了Unity的设置则会自动只保留对该顶点权重最高的四个骨骼。\n选“Custom”可以设置更高（比如8），就会保留更多组权重。但是性能开销会十分美丽\n引用：\n导入带有人形动画的模型 - Unity 手册 # Strip bones\n勾选 Strip Bones，Unity会在导入模型时自动移除所有没有被蒙皮权重（Skin Weights）影响到的骨骼，即那些对模型变形没有实际作用的骨骼节点不会导入进来。如果原始骨骼里有一些辅助骨、挂点骨、导出时残留的无用骨骼，但这些骨骼没有任何顶点被它们影响（权重为0），勾选这个后这些骨骼会被Unity忽略掉，减少无用数据。\n但是要注意的是，如果我们有一些辅助骨骼（比如说使用辅助骨骼完成武器动画绑定）的时候，就不要勾选这个，这回去掉辅助骨骼。\noptimize game objects（ 优化游戏对象 ）\n勾选后，Unity会直接将骨骼链优化为底层的数据结构，只保留Mesh和根节点，把大部分骨骼节点从Hierarchy中“隐藏”掉，不再作为GameObject存在。这极大地减少了场景中的对象数量，显著提升运行效率。\n来自GPT对这个选项的解释： # 你的疑问很正常，Optimize Game Objects 这个选项确实容易让人迷糊。下面用最直白的语言和实例对比帮你彻底搞懂它的作用和意义！\n1. 问题本质：为什么要“优化骨骼对象”？ # 在Unity里，导入带骨骼动画的模型时，每一根骨骼通常会生成一个GameObject（带Transform），比如这样一棵骨骼树：\n角色 ├── Hips │ ├── Spine │ │ ├── Chest │ │ │ ├── Neck │ │ │ └── Head │ │ └── LeftShoulder │ │ └── LeftArm │ │ └── ... ... 每一个节点（骨骼）都变成场景里的一个“游戏对象”！（可以在inspector中看到）\n2. 这样会有什么问题？ # 如果一个角色有50根骨骼，就有50个GameObject。 一个场景有100个角色，就有5000个骨骼对象！ Unity在运行时要每帧同步这些Transform、管理层级、分配内存，有很大性能开销，特别是在移动端、VR等对性能敏感的场合。 3. 什么是“Optimize Game Objects”？ # 勾选了它，Unity就不再为每根骨骼生成独立GameObject。 # 只保留最重要的根节点和Mesh节点。 # 骨骼动画的数据结构变成底层C++层内存里的“骨骼矩阵数组”，不用Unity的GameObject系统去维护。 # Hierarchy视图里看不到骨骼树了，只有主对象和Mesh。 # 对比： # #### 不勾选Optimize #### 勾选Optimize #### 每个骨骼都是GameObject #### 只有根节点是GameObject #### 可以脚本随时访问骨骼Transform #### 大部分骨骼无法直接访问/脚本操作 #### 性能开销大 #### 性能极高，内存和CPU压力小 4. 实际例子 # 不优化： # 你可以写：transform.Find(\u0026quot;Spine/Chest/Neck/Head\u0026quot;) 直接找到头部骨骼，挂武器、加特效。 但每个角色都要消耗很多GameObject内存，场景多了就卡。 优化后： # 你没法直接脚本操作骨骼（因为它们不再是GameObject）。 但同样动画能正常播放，性能大大提升。 适合大批量NPC、只看动画不用动态操作骨骼的角色。 5. 如果你还想操作骨骼怎么办？ # Unity有“暴露骨骼”(Expose Transforms)功能：\n你可以指定“需要动态操作的那几个骨骼”在Hierarchy里保留，其余优化掉。\n6. 一句话总结 # Optimize Game Objects就是让Unity不为每根骨骼生成GameObject，只在底层“默默执行动画”，这样能极大提升性能和效率。如果你只是让角色自动播放动画，不需要挂武器、特效、换装，强烈建议开启它！\n# 准备妥当后\u0026hellip; # 便可以开始打开Config对骨骼进行Retarget。\nMapping是进行关键配置的地方，如果有些地方没有出现映射的话需要我们进行手动调整，在这个情况下，模型应该摆出Tpose以方便我们进行骨骼映射。\n最下方的两个选项：\nMapping：指定映射方法。分为Clear、Automap、Load/Save。Clear则是直接从头进行分配，清空所有已经映射过的骨骼，从头进行手动分配；Automap则是之前提到过的使用Unity内置的配置文件模糊匹配；Save/Load则是用于模型的批量化处理。\nPose： 主要用于调整当前骨骼的姿势，便于正确映射和校准人形骨骼。 分为Reset、Sample pose和Enforce T-pose。\nSample pose：把骨骼恢复到导出FBX时的原始绑定姿势，通常就是建模时的A-Pose或T-Pose，或者动画师在蒙皮时设置的初始姿势。\nEnforce T-pose： 把骨骼恢复到导出FBX时的原始绑定姿势，通常就是建模时的A-Pose或T-Pose，或者动画师在蒙皮时设置的初始姿势。 有时如果不是T-pose那么自动绑定会出现问题。可以选择这个选项尝试重新自动映射。\nReset：重置姿势。恢复到导入姿势。\n还想钳制姿势？ # 我们知道，即使骨骼映射一直，蒙皮权重相似，但是如果对于跨风格的动画（二次元角色动画重新映射到欧美角色上），很容易出现骨骼运动过于不协调的时候。这就需要config的第二个页面： Muscles \u0026amp; Settings 了。\nMuscle Group Preview：这里的“Muscle”指的是Unity Humanoid系统对人形骨架的各个自由度的抽象，比如手臂的上举、下放、前后摆动等。通过拖动这些滑块，你可以实时预览角色各个大类动作的变形效果，比如张合嘴巴、左右转头、臂展、收腿等。用来检测骨骼分配和蒙皮权重是否合理，比如看张嘴会不会带动到脸部错误部位、转头会不会扭曲等。发现异常可以回到Mapping面板修正骨骼分配，或调整权重。\nPer-Muscles Settings： 展开 Body、Head、Left Arm 等子项后，可以单独调节每一块肌肉的活动范围（比如手臂能抬多高、脖子能转多远）。 这些滑块可以指定人体骨骼的极限运动值， 让Avatar更适配不同体型的模型（比如胳膊较短/较长、脖子较粗等）\nAdditional Settings：\nUpper Arm Twist / Lower Arm Twist：\n控制手臂扭转时的影响范围，防止手臂旋转时出现“爆炸”或不自然。\nUpper Leg Twist / Lower Leg Twist：\n控制大腿/小腿扭转的范围与效果。\nArm Stretch / Leg Stretch：\n控制手臂、腿部在极限动作时的伸展弹性，防止动画重定向时出现“拉长”或变短的畸形。\nFeet Spacing：\n控制两脚之间的默认距离，方便站立动作的适配。\nTranslation DoF （Translation Degree of Freedom，平移自由度） ：\n决定骨骼是否允许平移自由度（一般默认关闭）。 平移自由度是允许某些骨骼节点除了旋转，还可以在空间中移动（平移）。 但是一般不会出现，如果连骨骼都出现了平移，那么关节会出现非常严重的脱节情况。\n","date":"16 February 2025","externalUrl":null,"permalink":"/posts/vex%E5%8F%98%E9%87%8F/","section":"","summary":"骨骼动画指北","title":"Unity骨骼动画Retargeting","type":"posts"},{"content":"","date":"16 February 2025","externalUrl":null,"permalink":"/tags/%E5%8D%8A%E5%AF%BC%E4%BD%93/","section":"Tags","summary":"","title":"半导体","type":"tags"},{"content":"","date":"16 February 2025","externalUrl":null,"permalink":"/tags/%E7%94%B5%E5%AD%90/","section":"Tags","summary":"","title":"电子","type":"tags"},{"content":" 第二章 # 计算在T = 300 K时，比费米能级高3kT的能级被电子占据的概率。 # 根据费米-狄拉克分布函数，我来计算在T = 300 K时比费米能级高3kT的能级被电子占据的概率。 $D = \\frac{\\phi \\cdot \\lambda}{4\\pi}$ 当信号光场 $f_s(t) = A_s\\cos(\\omega_s t + \\phi_s)$ 和本振光场 $f_L(t) = A_L\\cos(\\omega_L t + \\phi_L)$ 在检测器上混频时：\n费米-狄拉克分布函数为：\n$$f(E) = \\frac{1}{1 + \\exp\\left(\\frac{E - E_F}{kT}\\right)}$$其中：\nE 是能级 E_F 是费米能级 k 是玻尔兹曼常数 T 是温度 给定条件：\nT = 300 K 能级比费米能级高3kT，即 E - E_F = 3kT 计算过程：\n将条件代入费米分布函数：\n$$f(E) = \\frac{1}{1 + \\exp\\left(\\frac{3kT}{kT}\\right)}$$$$f(E) = \\frac{1}{1 + \\exp(3)}$$$$f(E) = \\frac{1}{1 + e^3}$$计算 $e^3 \\approx 20.09$\n因此： $$f(E) = \\frac{1}{1 + 20.09} = \\frac{1}{21.09} \\approx 0.0474$$答案： 比费米能级高3kT的能级被电子占据的概率约为 4.74% 或 0.0474。\n这个结果说明，当能级远高于费米能级时（高出3kT），该能级被电子占据的概率很小，符合费米分布在高能级处迅速下降的特征。\n硅在300K时施主杂质浓度为ND=91015 cm-3, 受主杂质浓度为含有NA=1.11014 cm-3，计算电子和空穴的浓度 # 给定条件：\nT = 300 K 施主杂质浓度：ND = 9×10¹⁵ cm⁻³ 受主杂质浓度：NA = 1.1×10¹⁴ cm⁻³ 硅在300K时的本征载流子浓度：ni = 1.08×10¹⁰ cm⁻³（课件中给出） 分析步骤：\n步骤1：判断半导体类型 由于 ND \u0026gt; NA，这是一个n型半导体。\n步骤2：应用电中性条件 根据课件中的电中性条件：\np + ND = n + NA 即：\nn = p + ND - NA 步骤3：计算净掺杂浓度 净施主浓度 = ND - NA = 9×10¹⁵ - 1.1×10¹⁴ = 8.89×10¹⁵ cm⁻³\n步骤4：计算载流子浓度\n对于n型半导体，当掺杂浓度远大于本征载流子浓度时：\n电子浓度（多子）：\nn ≈ ND - NA = 8.89×10¹⁵ cm⁻³ 空穴浓度（少子）： 利用质量作用定律 np = ni²：\np = ni²/n = (1.08×10¹⁰)²/(8.89×10¹⁵)\rp = 1.166×10²⁰/(8.89×10¹⁵)\rp ≈ 1.31×10⁴ cm⁻³ 最终答案：\n电子浓度：n ≈ 8.89×10¹⁵ cm⁻³ 空穴浓度：p ≈ 1.31×10⁴ cm⁻³ 验证：\nn \u0026raquo; p，符合n型半导体特征 np = 8.89×10¹⁵ × 1.31×10⁴ ≈ 1.16×10²⁰ ≈ ni²，满足质量作用定律 求导带中E = Ec+ kT/2时被电子占据的概率，并计算T = 300 K时硅中的热平衡电子浓度。设费米能级位于导带下方0.25 eV处。T = 300 K时硅中的N= 2.8 × 1019 cm-3。 # 给定条件：\nT = 300 K 费米能级位于导带下方0.25 eV处，即 EF = Ec - 0.25 eV Nc = 2.8 × 10¹⁹ cm⁻³ 第一部分：求导带中E = Ec + kT/2时被电子占据的概率\n步骤1：计算kT值\nk = 8.617 × 10⁻⁵ eV/K\rkT = 8.617 × 10⁻⁵ × 300 = 0.0259 eV ≈ 0.026 eV 步骤2：计算能级差\nE - EF = (Ec + kT/2) - (Ec - 0.25)\rE - EF = kT/2 + 0.25 = 0.013 + 0.25 = 0.263 eV 步骤3：应用费米-狄拉克分布函数 根据课件中的费米分布函数：\nf(E) = 1/[1 + exp((E - EF)/kT)] (E - EF)/kT = 0.263/0.026 ≈ 10.12 f(E) = 1/[1 + exp(10.12)] = 1/[1 + 24766] ≈ 4.04 × 10⁻⁵ 第二部分：计算T = 300K时硅中的热平衡电子浓度\n根据课件中导带电子浓度公式：\nn = Nc × exp((EF - Ec)/kT) 步骤1：计算指数项\n(EF - Ec)/kT = (-0.25)/0.026 = -9.62 步骤2：计算电子浓度\nn = 2.8 × 10¹⁹ × exp(-9.62)\rn = 2.8 × 10¹⁹ × 6.69 × 10⁻⁵\rn ≈ 1.87 × 10¹⁵ cm⁻³ 最终答案：\nE = Ec + kT/2时被电子占据的概率：≈ 4.04 × 10⁻⁵ 热平衡电子浓度：n ≈ 1.87 × 10¹⁵ cm⁻³ 这个结果表明费米能级距离导带较远时，导带中的电子浓度相对较低，符合半导体的特征。\n已知载流子迁移率，求扩散系数。设T=300K时某载流子的迁移率为1000cm2/V·s。 # 给定条件：\nT = 300 K 载流子迁移率 μ = 1000 cm²/V·s 理论基础： 根据课件中的爱因斯坦关系：\nD = (kT/q)μ 其中：\nD：扩散系数 (cm²/s) k：玻尔兹曼常数 = 1.38 × 10⁻²³ J/K T：温度 (K) q：电子电荷量 = 1.6 × 10⁻¹⁹ C μ：迁移率 (cm²/V·s) 计算步骤：\n步骤1：计算kT/q\nkT/q = (1.38 × 10⁻²³ × 300)/(1.6 × 10⁻¹⁹)\rkT/q = 4.14 × 10⁻²¹/1.6 × 10⁻¹⁹\rkT/q = 0.0259 V = 25.9 mV 步骤2：计算扩散系数\nD = (kT/q) × μ\rD = 0.0259 × 1000\rD = 25.9 cm²/s 最终答案： 扩散系数 D = 25.9 cm²/s\n物理意义：\n爱因斯坦关系揭示了载流子的漂移运动和扩散运动之间的内在联系 在室温下，kT/q ≈ 26 mV，这是半导体器件分析中的一个重要参数 迁移率越高，扩散系数也越大，说明载流子的输运能力越强。 题目4 # 给定条件：\nT = 300 K 砷化镓(GaAs)的掺杂浓度：Na = 0，Nd = 10¹⁶ cm⁻³ 设杂质全部电离 电子迁移率：μn = 8500 cm²/Vs 空穴迁移率：μp = 400 cm²/Vs 外加电场强度：E = 10 V/cm 分析步骤：\n步骤1：确定半导体类型和载流子浓度 由于 Na = 0，Nd = 10¹⁶ cm⁻³，这是一个n型半导体。\n对于强掺杂的n型半导体：\n电子浓度（多子）：n ≈ Nd = 10¹⁶ cm⁻³ 空穴浓度（少子）：p \u0026laquo; n（可忽略） 步骤2：应用漂移电流密度公式 根据课件中的公式：\nJdrift = q(nμn + pμp)E 由于 p \u0026laquo; n，主要考虑电子的贡献：\nJdrift ≈ qnμnE 步骤3：代入数值计算\nq = 1.6 × 10⁻¹⁹ C\rn = 10¹⁶ cm⁻³\rμn = 8500 cm²/Vs\rE = 10 V/cm Jdrift = 1.6 × 10⁻¹⁹ × 10¹⁶ × 8500 × 10\rJdrift = 1.6 × 10⁻³ × 8500 × 10\rJdrift = 1.6 × 85000 × 10⁻³\rJdrift = 136 A/cm² 最终答案： 漂移电流密度 Jdrift = 136 A/cm²\n说明：\n由于这是强掺杂的n型半导体，电子是主要的载流子 空穴浓度相对很小，对总电流的贡献可以忽略 GaAs具有很高的电子迁移率，因此在相同电场下产生较大的漂移电流 题目5 # 给定条件：\nn型GaAs半导体 T = 300 K 电子浓度在0.10 cm距离内从1×10¹⁸ cm⁻³ 到7×10¹⁷ cm⁻³ 线性变化 电子扩散系数：Dn = 225 cm²/s 理论基础： 根据课件中的扩散电流密度公式：\nJn,diff = qDn(dn/dx) 其中：\nq = 1.6 × 10⁻¹⁹ C（电子电荷量） Dn = 225 cm²/s（电子扩散系数） dn/dx：电子浓度梯度 计算步骤：\n步骤1：计算浓度梯度 dn/dx\n初始浓度：n₁ = 1×10¹⁸ cm⁻³\r终末浓度：n₂ = 7×10¹⁷ cm⁻³\r距离：Δx = 0.10 cm Δn = n₂ - n₁ = 7×10¹⁷ - 1×10¹⁸ = -3×10¹⁷ cm⁻³ dn/dx = Δn/Δx = (-3×10¹⁷)/0.10 = -3×10¹⁸ cm⁻⁴ 步骤2：计算扩散电流密度\nJn,diff = qDn(dn/dx)\rJn,diff = 1.6×10⁻¹⁹ × 225 × (-3×10¹⁸)\rJn,diff = -108 A/cm² 最终答案： 扩散电流密度 Jn,diff = -108 A/cm²\n物理意义：\n负号表示电流方向与浓度梯度方向相反 电子从高浓度区域扩散到低浓度区域 由于电子带负电，所以电流方向与电子运动方向相反 扩散电流密度的大小为 108 A/cm² 这个结果符合课件中提到的扩散机制：载流子从浓度高的区域移动到低浓度的区域。\n第六章 # 试述门阵列和标准单元设计方法的概念和它们之间的异同点。 # 门阵列设计方法（GA） # 门阵列是一种半定制设计方法，其特点是：\n形状和尺寸完全相同的基本单元排列成阵列，内部含有若干器件 单元之间留有布线通道，通道宽度和位置固定 预先完成接触孔和连线以外的芯片加工步骤，形成母片 根据不同应用，设计出不同的接触孔版和金属连线版，实现所需电路功能 基本单元只是晶体管的集合，不具有电学属性 标准单元设计方法（SC） # 标准单元方法是一种库单元设计方法，其特点是：\n从标准单元库中调用经过设计的逻辑单元，并排列成行 行间留有可调整的布线通道 按功能要求将内部单元以及输入/输出单元连接起来 限制高度，不限制宽度 标准单元库中的单元是人工优化设计，力求达到最小面积和最好性能 异同点分析 # 相同点 # 定制性质：两者都属于ASIC（专用集成电路）设计方法 目标导向：都是为了降低设计成本，缩短设计周期 基于单元：都采用预设计的单元进行电路构建 需要全套掩膜版：都属于定制设计方法，需要完整的掩膜版制作 不同点 # 比较项目 门阵列设计方法 标准单元设计方法 定制情况 半定制 定制 基片状况 有母片 无基片 单元几何形状 完全相同的矩形 等高不等宽的矩形 单元电路属性 无电路属性 有单元电路功能 布线状况 等宽的布线通道 宽度可变的布线通道 设计灵活性 较低 较高 芯片利用率 门利用率低，芯片面积浪费 较高的芯片利用率和连线布通率 设计周期 短 相对较长 设计成本 低 相对较高 性能优化 有限 可在版图和性能上得到较好优化 适用批量 10³块 10⁴块 掩膜版数目 1-2层 全套 优缺点对比 # 门阵列方法：\n优点：设计周期短，设计成本低，适合中等性能、要求设计时间短、数量相对较少的电路 缺点：设计灵活性较低，门利用率低，芯片面积浪费 标准单元方法：\n优点：较高的芯片利用率和连线布通率，可变的单元数、压焊块数、通道间距，布局布线自由度大 缺点：依赖于标准单元库，库建立需较长周期和较高成本，特别是工艺更新时 应用场景 # 门阵列：适用于设计适当规模、中等性能、要求设计时间短、数量相对较少的电路 标准单元：适用于中批量或小批量但性能要求较高的芯片设计 总的来说，门阵列方法侧重于快速、低成本的设计实现，而标准单元方法更注重性能优化和设计灵活性，两者在集成电路设计中各有其适用的场景和优势。\n标准单元库中的单元的主要描述形式有哪些？分别在IC设计的什么阶段应用？ # 标准单元库中单元的主要描述形式 # 1. 逻辑符号（L） # 内容包括：\n单元名称与符号 I/O端口信息 特点：\n用于逻辑图表示 提供单元的逻辑功能标识 2. 功能描述 # 内容包括：\n单元的逻辑功能定义 行为特性描述 3. 电路结构和电学指标 # 内容包括：\n电路的具体实现结构 电学性能参数 时序特性等 4. 版图信息 # 分为两个层次：\n拓扑版图（O）：拓扑单元名、单元宽度高度、I/O位置及名称 掩膜版图（A）：具体的几何版图信息 在IC设计各阶段的应用 # 功能设计阶段 # 应用的描述形式：\n逻辑符号（L）：用于逻辑图绘制 功能描述：用于RTL级描述和行为仿真 具体应用：\n功能块划分时选择合适的标准单元 RTL级VHDL、Verilog描述中调用单元功能 行为仿真验证总体功能和时序正确性 逻辑和电路设计阶段 # 应用的描述形式：\n电路结构和电学指标：用于电路实现和性能分析 功能描述：用于逻辑综合 具体应用：\n逻辑综合过程中将RTL级描述转化为门级网表 调用单元库完成电路实现 电路模拟与分析，验证电路的直流、交流、瞬态特性 时序分析和优化 版图设计阶段 # 应用的描述形式：\n拓扑版图（O）：用于布局规划 掩膜版图（A）：用于最终版图生成 具体应用：\n布图规划：确定单元区位置、功能块的面积形状和相对位置 布局：根据拓扑信息确定各单元的物理位置 布线：利用单元的I/O位置信息完成连线 版图验证：DRC、ERC、LVS检查 标准单元库的层次化应用 # 课件提到标准单元库支持\u0026quot;层次描述\u0026quot;，从小到大包括：\n元件：基本器件级别 门：基本逻辑门级别 元胞：小规模功能单元 宏单元（功能块）：较大规模功能模块 不同层次的单元在设计的不同阶段发挥作用，体现了分层分级设计思想。\n设计流程中的迭代应用 # 在实际设计中，这些描述形式不是孤立使用的，而是在设计流程中相互配合：\n自顶向下设计：从功能描述开始，逐步细化到版图 验证反馈：各阶段仿真结果可能要求返回修改上层描述 优化迭代：根据性能要求在不同描述层次间进行优化 集成电路的可测性设计是指什么？ # 基于课件内容，我来详细回答集成电路的可测性设计相关问题。\n集成电路可测性设计的定义 # 根据课件内容，集成电路的可测性设计是指：\n在尽可能少地增加附加引线脚和附加电路，并使芯片性能损失最小的情况下，满足电路可控制性和可观察性的要求。\n可测性设计的核心要素 # 1. 可控制性（Controllability） # 定义： 从输入端将芯片内部逻辑电路置于指定状态的能力\n意义： 能够通过外部输入控制内部电路的状态，使测试向量能够有效地激励被测电路\n2. 可观察性（Observability） # 定义： 直接或间接地从外部观察内部电路状态的能力\n意义： 能够从外部引脚观察到内部电路的响应，以便判断电路是否正常工作\n集成电路测试的特殊性和背景 # 测试的目的 # 课件指出，集成电路测试是：\n对制造出的电路进行功能和性能检测，检测并定位出电路的故障，用尽可能短的时间挑选出合格芯片。\n测试面临的挑战 # 内部节点不可直接访问：集成电路内部大量节点无法直接从外部观测 时序电路测试复杂：时序逻辑的状态依赖使测试变得困难 测试时间和成本压力：需要在短时间内完成大量芯片的筛选 主要的可测性设计技术 # 课件介绍了两种主要的可测性设计技术：\n1. 扫描路径测试技术 # 基本原理：\n将时序元件和组合电路隔离开，解决时序电路测试困难的问题 在正常工作模式和测试模式之间切换 实现方式：\n将时序元件（如触发器）串联成移位寄存器链（扫描路径） 通过模式选择信号控制工作模式 测试流程：\n测试模式验证：检测扫描路径是否正确 组合逻辑测试： 将测试序列移入移位寄存器 稳定后与反馈输入一起通过组合逻辑 观察组合逻辑输出，与期望值比较 正常工作模式：组合电路反馈输出送入时序元件 结果观察：将时序元件内容移出，与期望值比较 优点：\n有效解决时序电路测试难题 测试序列可用确定性算法自动生成 缺点：\n需要增加控制电路数量和外部引脚 需要将分散的时序元件连接，导致芯片面积增加和速度降低 串行输出结果，测试时间较长 2. 特征量分析测试技术 # 基本概念：\n把对应输入信号的各节点响应序列压缩，提取出相应的特征量，保存在寄存器中，只需比较实测响应序列和正常序列的特征量。\n技术特点：\n内建测试技术：在芯片内部设计\u0026quot;测试设备\u0026quot;来检测芯片功能 数据压缩：避免数据需要串行传输到外部设备的问题 减少存储需求：可以减少计算机内存，提高测试速度 优缺点：\n优点：增加的芯片面积不多，测试速度快 缺点：故障检测和诊断的有效率不高 可测性设计的设计原则 # 从课件内容可以看出，可测性设计需要在以下方面取得平衡：\n附加硬件最小化：尽可能少地增加附加引线脚和附加电路 性能影响最小化：使芯片性能损失最小 测试效果最大化：满足电路可控制性和可观察性要求 测试时间优化：用尽可能短的时间完成测试 总结 # 集成电路的可测性设计是在电路设计阶段就考虑测试需求，通过增加必要的测试辅助电路和控制机制，使得复杂的集成电路能够被有效测试。这是现代集成电路设计中不可缺少的重要环节，直接影响到芯片的制造成品率和质量保证。\n第九章 太阳能电池 # 什么是直接带隙/间接带隙半导体？为什么直接带隙半导体的光吸收系数比间接带隙半导体大？ # 直接带隙与间接带隙半导体 # 直接带隙半导体：导带底部和价带顶部在k空间（动量空间）的同一点，通常在布里渊区的中心点（Γ点）。电子可以直接从价带跃迁到导带，不需要改变动量。\n间接带隙半导体：导带底部和价带顶部在k空间的不同位置。电子从价带跃迁到导带时需要同时改变能量和动量。\n光吸收系数的差异原因 # 直接带隙半导体的光吸收系数比间接带隙半导体大的主要原因在于跃迁机制的不同：\n直接带隙半导体 # 允许跃迁：电子可以直接吸收光子从价带跃迁到导带 只需要光子参与：满足能量守恒即可（hν ≥ Eg） 跃迁概率高：因为不需要额外的动量守恒条件 吸收系数大：通常在10⁴-10⁶ cm⁻¹数量级 间接带隙半导体 # 禁戒跃迁：需要同时满足能量和动量守恒 需要声子参与：必须有声子参与来提供或带走多余的动量 三体过程：涉及光子、电子和声子的三体相互作用 跃迁概率低：三体过程的概率远低于二体过程 吸收系数小：通常比直接带隙半导体小2-3个数量级 实际例子 # 从文档中可以看到：\n硅（Si）：间接带隙半导体，需要较厚的吸收层 砷化镓（GaAs）：直接带隙半导体，吸收系数大 CIGS：对1.5 eV以上光子具有极高吸收系数（\u0026gt; 10⁵ cm⁻¹） 非晶硅：虽然基于硅，但由于结构无序性，\u0026ldquo;不再是间接带隙半导体\u0026rdquo;，光吸收增强 这就是为什么在太阳能电池应用中，直接带隙材料（如GaAs、CIGS、CdTe）可以做得很薄就能有效吸收光，而硅太阳能电池需要相对较厚的吸收层的原因。 根据文档第15页的内容，我来详细解答这个关于PN结太阳能电池的问题。\n太阳能电池的电流表达式 # 当光线照射在PN结太阳能电池上时，总电流为光电流与正向电流的差值：\n$$I = I_L - I_F = I_L - I_S \\left[\\exp\\left(\\frac{eV}{kT}\\right) - 1\\right]$$其中：\n$I_L$：光电流（光照产生的电流） $I_F$：正向电流（二极管正向电流） $I_S$：PN结反向饱和电流密度 $V$：外加电压 $e$：电子电荷 $k$：玻尔兹曼常数 $T$：绝对温度 开路电压表达式 # 开路条件下，外电路断开，总电流 $I = 0$：\n$$0 = I_L - I_S \\left[\\exp\\left(\\frac{eV_{OC}}{kT}\\right) - 1\\right]$$解得开路电压：\n$$V_{OC} = \\frac{kT}{e} \\ln\\left(\\frac{I_L}{I_S} + 1\\right)$$由于通常 $I_L \u0026raquo; I_S$，可简化为：\n$$V_{OC} = \\frac{kT}{e} \\ln\\left(\\frac{I_L}{I_S}\\right)$$ 短路电流表达式 # 短路条件下，外电路短接，电压 $V = 0$：\n$$I_{SC} = I(V=0) = I_L - I_S \\left[\\exp\\left(\\frac{e \\cdot 0}{kT}\\right) - 1\\right]$$$$I_{SC} = I_L - I_S[1 - 1] = I_L$$因此： $$I_{SC} = I_L$$ 物理意义 # 开路电压：反映了PN结在光照下建立的内建电场强度，与光电流和反向饱和电流的比值对数成正比 短路电流：等于光电流，直接反映了太阳能电池在给定光照条件下产生载流子的能力 这些参数是评价太阳能电池性能的重要指标，与填充因子FF一起决定了电池的光电转换效率。\n","date":"16 February 2025","externalUrl":null,"permalink":"/posts/whatdoyouwant/","section":"","summary":"","title":"电子和空穴浓度计算","type":"posts"},{"content":"","date":"16 February 2025","externalUrl":null,"permalink":"/tags/%E7%A9%BA%E7%A9%B4/","section":"Tags","summary":"","title":"空穴","type":"tags"},{"content":"","date":"15 February 2025","externalUrl":null,"permalink":"/tags/vex/","section":"Tags","summary":"","title":"VEX","type":"tags"},{"content":"偶然在用的时候看到了一行代码\nfloat rotation = radians(@rotation*360); vector axis = v@axis; 这里在引用属性的时候出现了两种情况：一种是@rotation;一种是v@axis。这之间其实是指定类型和自动查找类型之间的差异：\n在VEX中，v@和单独的@是属性访问的不同语法，区别在于数据类型的明确指定：\n@ 语法 # @axis // 自动推断数据类型 VEX会根据上下文自动推断属性的数据类型 如果无法明确推断，可能导致错误或意外结果 v@ 语法 # v@axis // 明确指定为vector类型 明确指定属性为vector类型 即使原始属性是其他类型，也会尝试转换为vector 类型前缀列表 # i@ - integer (整数) f@ - float (浮点数) v@ - vector (向量) p@ - vector4 (四维向量) s@ - string (字符串) u@ - vector2 (二维向量) 实际区别示例 # // 如果axis属性实际是float类型 vector axis1 = @axis; // 可能报错或产生意外结果 vector axis2 = v@axis; // 强制转换为vector，如{axis_value, 0, 0} // 如果axis属性确实是vector类型 vector axis3 = @axis; // 正常工作 vector axis4 = v@axis; // 正常工作，等价 最佳实践 # 使用类型前缀（如v@）更安全、更明确，特别是在复杂场景中，能避免类型推断错误。\n","date":"15 February 2025","externalUrl":null,"permalink":"/posts/vex%E5%A6%82%E4%BD%95%E6%B2%BF%E7%94%A8%E4%B8%8A%E7%BB%84%E5%8F%98%E9%87%8F-1/","section":"","summary":"VEX编程技巧","title":"VEX：如何沿用上组变量","type":"posts"},{"content":" 二维卷积模糊 # // GaussianBlur.hlsl float4 GaussianBlur(sampler2D tex, float2 uv, float2 resolution, float radius) { float4 color = float4(0, 0, 0, 0); float totalWeight = 0.0; int samples = 5; // Number of samples for blur for (int x = -samples; x \u0026lt;= samples; x++) { for (int y = -samples; y \u0026lt;= samples; y++) { float2 offset = float2(x, y) * radius / resolution; float weight = exp(-dot(offset, offset) / (2.0 * radius * radius)); color += tex2D(tex, uv + offset) * weight; totalWeight += weight; } } return color / totalWeight; } Frosted Glass Blur（毛玻璃模糊，随机采样模糊） # ","date":"11 February 2025","externalUrl":null,"permalink":"/posts/%E6%A8%A1%E7%B3%8A%E7%AE%97%E6%B3%95/","section":"","summary":"算法原理解析","title":"模糊算法","type":"posts"},{"content":"确认被照射的物体是否勾选了\u0026lt;font style=\u0026quot;color:rgb(44, 44, 54);\u0026quot;\u0026gt;static\u0026lt;/font\u0026gt;属性。\n如果还是没有反应，可以检查\u0026lt;font style=\u0026quot;color:rgb(44, 44, 54);\u0026quot;\u0026gt;LightSettings\u0026lt;/font\u0026gt;面板，确保已经勾选了\u0026lt;font style=\u0026quot;color:rgb(44, 44, 54);\u0026quot;\u0026gt;Auto Generate\u0026lt;/font\u0026gt;选项。\n勾选 Static 的物体会告诉 Unity 这个物体可以用于特定的优化或功能。Static 选项下包含多个具体的静态属性，你可以单独启用它们，或者通过勾选 Static 启用所有静态属性。\nStatic 的具体优化作用：\nLightmapping（光照贴图）\n如果物体被标记为静态，Unity 会将它包括在光照贴图（Lightmap）的计算中。\n光照贴图是一种预计算的光照技术，用于烘焙全局光照、阴影等效果，使运行时无需实时计算。\n作用：\n提高性能：光照贴图的渲染开销低，适合静态场景。\n提供全局光照和间接光照：静态物体可以参与全局光的反射计算。\nOcclusion Culling（遮挡剔除）\n静态物体会被包括在 Unity 的遮挡剔除计算中。\n遮挡剔除会自动剔除被其他物体遮挡的物体，从而减少渲染工作。\n作用：\n提高性能：避免渲染玩家不可见的物体。\n静态物体更适合参与遮挡剔除，因为它们的位置不会改变。\n但是请注意一下不要背面一剔除那么就会有些不合适的观感（\nBatching（批处理优化）\n勾选 Static 后，静态物体可以参与以下两种批处理优化：\nStatic Batching（静态批处理）：\n静态物体会被合并为一个大网格（Mesh），减少绘制调用（Draw Calls）。\n要求：静态物体必须使用相同的材质。\n作用：\n显著减少 Draw Calls，提高性能。\n适用于数量多、变化少的静态物体。\n动态物体无法使用静态批处理，即使它们共享材质。\nNavigation（导航网格计算）\n静态物体会被包括在导航网格（NavMesh）计算中。\n作用：\n如果物体是静态的，则在生成导航网格时，Unity 会将其视为障碍物，供导航 AI 使用。\n适用于地形、墙壁等不会移动的物体。\n5. Reflection Probes（反射探针）\n静态物体会被包括在反射探针（Reflection Probe）数据中。\n作用：\n提供更准确的反射效果。\n静态物体在反射探针中的数据是预计算的，提高性能。\n6. Light Probes（光照探针）\n静态物体会影响光照探针（Light Probe）的预计算。\n作用：\n静态物体可用于影响动态物体的间接光照和反射。\n提供更精确的光照环境。\n7. Off-Mesh Links（导航链接）\n如果启用 Navigation Static，静态物体可以作为导航网格的起点或终点，用于创建导航链接。\n8.** Contribute GI（参与全局光照）**\n勾选 Static 后，物体可以参与全局光照（Global Illumination）的预计算。\n作用：\n静态物体会反射全局光，为其他物体提供间接光照。\n提供更逼真的光照效果。\n","date":"9 February 2025","externalUrl":null,"permalink":"/posts/unity%E5%8C%BA%E5%9F%9F%E5%85%89area-light-%E6%B2%A1%E6%9C%89%E5%8F%8D%E5%BA%94/","section":"","summary":"渲染技术解析","title":"Unity区域光（Area Light） 没有反应","type":"posts"},{"content":" Investigate how Unity renders shadows.\n用大白话说阴影是如何产生的\n阴影的产生来自于两个核心要素：来自摄像机的深度测试、以光源为中心进行shadowmap的产生，最后和摄像机的深度图进行测试比较的过程。\n有一些技术可以支持实时阴影。每个都有其优点和缺点。 除了Shadowmap技术，还有体积阴影、SSS屏幕空间阴影、Ray Tracing光线追踪阴影、区域光源阴影Area Light Shadow、阴影体Shadow Volumne。Unity使用了当今最常见的技术，那就是阴影贴图。这意味着 Unity 以某种方式将阴影信息存储在纹理中。\n在这个例子中，我用的是forward渲染模式，并且用两个pass进行阴影的处理：Foraward Lit Pass;Shadow Caster Pass.Shadow catser pass仅负责将物体渲染到阴影贴图中，输出不会被显示；Forward Lit Pass\n则不仅处理一般的光照，也会处理并生成阴影。\nShader \u0026#34;KelvinvanHoorn/ShadowOutlines\u0026#34; { Properties { } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;RenderPipeline\u0026#34; = \u0026#34;UniversalRenderPipeline\u0026#34;} Cull Back Pass { Name \u0026#34;ForwardLit\u0026#34; Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;UniversalForward\u0026#34; } HLSLPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; struct Attributes { float4 vertex : POSITION; float3 normal : NORMAL; }; struct Varyings { float4 posCS : SV_POSITION; float3 posWS : TEXCOORD0; float3 normalWS : TEXCOORD1; }; Varyings vert(Attributes IN) { Varyings OUT; VertexPositionInputs vertexInput = GetVertexPositionInputs(IN.vertex.xyz); OUT.posCS = vertexInput.positionCS; OUT.posWS = vertexInput.positionWS; VertexNormalInputs normalInput = GetVertexNormalInputs(IN.normal); OUT.normalWS = normalInput.normalWS; return OUT; } float4 frag (Varyings IN) : SV_Target { float3 col = float3(1, 1, 1); return float4(col, 1); } ENDHLSL } pass { Name \u0026#34;ShadowCaster\u0026#34; Tags{\u0026#34;LightMode\u0026#34; = \u0026#34;ShadowCaster\u0026#34;} ZWrite On ZTest LEqual ColorMask 0 HLSLPROGRAM #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; #pragma vertex vert #pragma fragment frag struct Attributes { float4 vertex : POSITION; float3 normal : NORMAL; }; struct Varyings { float4 posCS : SV_POSITION; }; float3 _LightDirection; Varyings vert(Attributes IN) { Varyings OUT = (Varyings)0; VertexPositionInputs vertexInput = GetVertexPositionInputs(IN.vertex.xyz); float3 posWS = vertexInput.positionWS; VertexNormalInputs normalInput = GetVertexNormalInputs(IN.normal); float3 normalWS = normalInput.normalWS; // Shadow biased ClipSpace position float4 positionCS = TransformWorldToHClip(ApplyShadowBias(posWS, normalWS, _LightDirection)); #if UNITY_REVERSED_Z positionCS.z = min(positionCS.z, positionCS.w * UNITY_NEAR_CLIP_VALUE); #else positionCS.z = max(positionCS.z, positionCS.w * UNITY_NEAR_CLIP_VALUE); #endif OUT.posCS = positionCS; return OUT; } float4 frag (Varyings IN) : SV_Target { return 0; } ENDHLSL } } } 分别拿到Clip Space和World Space下的各个坐标。\n光源投影阶段：生成 Shadow Map # “当启用定向阴影时，Unity 会根据设置的阴影分辨率生成一个深度贴图（Shadow Map），其分辨率由开发者在质量设置或渲染管线中定义。这个功能由Unity的Light文件支持：\n#include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; 但是在渲染ShadowMap之前，我们需要先获取ShadowCoord。需要调用TransformWorldToShadowCoord 函数（fragment），并向其传递我们的世界位置。\nfloat4 frag (Varyings IN) : SV_Target { float4 shadowCoord = TransformWorldToShadowCoord(IN.posWS); float3 col = float3(1, 1, 1); return float4(col, 1); } ShadowMap和ShadowCoord之间有什么区别？\n**ShadowCoord类比于书的页码，是用来查询的坐标（UV），而ShadowMap类似于书的内容，**是被查询的数据源（纹理）。\n注：这里的ShadowCoord = TransformWorldToShadowCoord可以在vertex部分进行计算，因为frag的量远大于vertex的量，并且vertex之间的计算数据可以共享，在顶点着色器中计算的结果可以通过插值（interpolation）传递给片段着色器，这样可以在片段着色器中使用这些已经计算好的值，减少了重复计算。然而插值会造成精度误差，尤其是纹理和阴影这种需要高精度计算的地方。\n计算完Shadowcoord，就可以为ShadowMap做准备了！ShadowMap渲染整个场景，但仅记录每个片段的深度信息。 GPU 使用该深度信息来判断光线是否能够直接到达场景中的每个片段，从而确定哪些片段位于阴影中 。该数据对应于光源视角下的深度值，它表示光线从光源出发到达片段表面的距离，并存储在光源的投影空间中。深度信息最终存储为 0-1 范围内的值。查看纹理时，附近的纹素显得很暗。纹理像素越远，它就越亮。\nfloat4 frag (Varyings IN) : SV_Target { float4 shadowCoord = TransformWorldToShadowCoord(IN.posWS); float shadowMap = MainLightRealtimeShadow(shadowCoord); float3 col = float3(1, 1, 1) * shadowMap; return float4(col, 1); } 摄像机渲染阶段：从摄像机视角检测阴影 # 在摄像机渲染阶段，Unity 利用之前从光源视角生成的深度贴图，结合摄像机视角下的像素位置，判断哪些像素位于阴影中。实际上，灯充当了相机的作用。这意味着深度值告诉我们光线在撞击某物之前传播了多远。根据产生的shadowmap，我们可以确定哪些物体被遮挡，哪些物体没有被遮挡。\n因为我们使用的是定向光（不是点光源），所以他们的相机是正交的。因此，不存在透视投影，并且灯光相机的确切位置并不重要。 Unity 将定位相机，以便它可以看到普通相机视野中的所有对象。\n在#include上方定义三个关键字，第一个关键字是获取主光源阴影所必需的，另外 2 个关键字用于处理 1.2 节中 URP 资源的设置。在 includes 上方添加以下 3 个突出显示的行。每个multi_compile基本上告诉 Unity 创建 2 个着色器变体，一个是打开的，另一个是关闭的。\n#pragma multi_compile _ _MAIN_LIGHT_SHADOWS #pragma multi_compile _ _MAIN_LIGHT_SHADOWS_CASCADE #pragma multi_compile _ _SHADOWS_SOFT #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; 第一行是打开Shadows，可以通过修改Lighting模块下ShadowMap的分辨率获取更高的精度；第二个是CSM阴影；第三个是开启PCF进行阴影的多次采样最后进行加权以获得软阴影。\n**对于定向光，Unity 使用级联阴影贴图（Cascaded Shadow Maps, CSM，也就是第二行的compile定义_MAIN_LIGHT_SHADOWS_CASCADE）技术，这个技术可以在Lighting中或者是Settings中的渲染管线中可以修改;CSM会将摄像机视锥体划分为多个区域（级联），每个区域生成独立的阴影贴图。这使得近处物体的阴影更精确，而远处物体的阴影可以使用较低分辨率贴图 **。如果要切换到两个级联，则每个灯光的场景将渲染两次。如果没有级联，每个灯光仅渲染一次。当我们观察阴影的质量时，我们就会明白为什么 Unity 会这样做（喜欢我全是锯齿的阴影吗）。\n阴影级联（Cascade Shadow Maps, CSM）是一种优化技术，主要用于方向光（如太阳光）的阴影渲染中。它通过将视锥体划分为多个层级或“级联”，每个级分配有不同的分辨率和覆盖范围，从而提高阴影的质量和性能。这种技术解决了传统阴影贴图在大场景中容易出现的问题，比如远处物体的阴影质量差、近处物体的阴影精度不足等。\n最后还有Shadow Bias和Normal Bias，这两个选项用于抵消Shadow Acne问题，这两个选项最后会应用于ShadowCaster的vertex处理。详见\nhttps://zhuanlan.zhihu.com/p/366555785 从相机的角度来看，我们已经拥有场景的深度信息了，并且还从每个灯光的角度获得了这些信息。当然，这些数据存储在不同的剪辑空间中，但我们知道这些空间的相对位置和方向。所以我们可以从一个空间转换到另一个空间。这使我们能够从两个角度比较深度测量结果。从概念上讲，我们有两个向量应该在同一点结束。如果他们这样做了，相机和灯光都可以看到该点，因此它会被点亮。如果光线的矢量在到达该点之前结束，则光线被阻挡，这意味着该点被遮挡。\nUnity 通过渲染覆盖整个视图的单个四边形来创建这些纹理。它使用 Hidden/Internal-ScreenSpaceShadows 此通道的着色器。每个片段从场景和灯光的深度纹理中进行采样，进行比较，并将最终阴影值渲染到屏幕空间阴影贴图。光照纹理元素设置为 1，阴影纹理元素设置为 0。此时，Unity 还可以执行过滤，以创建软阴影。（软阴影在此时可以进行渲染、虽然unity里是以阴影过渡的方式间接实现的）\n使用NdotL阴影提高阴影精度 # 我们还可以通过根据光线方向和物体的法线方向计算阴影并取两个阴影中的最小值来规避阴影精度过低的问题。我们通过在主光源方向和对象法线方向之间取点积来计算表面是否被照亮。此结果通常缩写为 NdotL。然后，我们的最终阴影值就是阴影贴图和 NdotL 的最小值。添加高亮显示的行并更改 col 的值以反映最终的阴影。\nfloat4 shadowCoord = TransformWorldToShadowCoord(IN.posWS); float shadowMap = MainLightRealtimeShadow(shadowCoord); float NdotL = saturate(dot(_MainLightPosition.xyz, IN.normalWS)); float combinedShadow = min(NdotL, shadowMap); float3 col = float3(1, 1, 1) * combinedShadow; 恭喜你，一个Lambert光照模型就完成了（绷）\n最后黑的地方太黑了，可以加一个ShadowMin对0的地方加权一下最后在col加起来：\nProperties { _ShadowStep (\u0026#34;Shadow step value\u0026#34;, Range(0, 1)) = 0.1 _ShadowMin (\u0026#34;Minimum shadow value\u0026#34;, Range(0, 1)) = 0.2 } ... Varyings vert(Attributes IN) {} float _ShadowStep, _ShadowMin; float4 frag (Varyings IN) : SV_Target { float4 shadowCoord = TransformWorldToShadowCoord(IN.posWS); float shadowMap = MainLightRealtimeShadow(shadowCoord); float NdotL = saturate(dot(_MainLightPosition.xyz, IN.normalWS)); float combinedShadow = min(NdotL, shadowMap); float shadowValue = saturate( combinedShadow + _ShadowMin); //float shadowValue = saturate(step(_ShadowStep, combinedShadow) + _ShadowMin); //对于卡通着色器可以用step钳制后加上min，造成明显的二分效果 float3 col = float3(1, 1, 1) * shadowValue; return float4(col, 1); } 渲染之后：对阴影贴图进行采样、Sobel算子 # 渲染的每个片段都会对阴影贴图进行采样。还有最终隐藏在稍后绘制的其他对象后面的片段。因此，这些碎片最终可能会接收到最终隐藏它们的物体的阴影。Framedebugger也可以看到。还可以看到阴影出现在实际投射阴影的对象之前。当然，这些错误仅在渲染帧时才会显现出来。完成后，图像是正确的。\n说人话就是：在处理阴影贴图（Shadow Maps）时。它指出了一个潜在的问题：即使是在最终图像中看不到的片段（fragments），也会进行阴影贴图的采样。这可能导致阴影先出现，物体之后才被渲染，尽管这些异常只会在帧渲染的过程中短暂出现，最终输出的图像仍然是正确的。\n渲染引擎通常按照一定的顺序绘制对象，以深度测试为基底的绘制顺序会先绘制背景或远处的对象，然后逐步绘制前景或近处的对象。然而，阴影计算通常是基于光源视角下的场景深度信息，而不是摄像机视角下的深度信息。因此有可能造成阴影已经被渲染出来，而被遮挡出来的物体没有被渲染。不过只会在非常短的时间出现，总体的场景仍然是正确的。\n最后的Sobel算子则是进行边缘检测，通过对周边像素的加权得到一个像素的变化趋势。想要做阴影的检测可以用（至于人物我觉得还是法线外扩和后处理居多吧）https://zhuanlan.zhihu.com/p/40491339 常见的阴影问题 # shadow swimming # 指的是当物体、阴影或摄像机移动时，阴影会出现不稳定、抖动或“游动”的现象。这种现象主要与实时阴影的计算方式以及深度贴图的精度限制有关。还记得我们说的第二个标题吗？引擎会生成一张从光源视角出发的深度贴图（Shadow Map）来确定场景中哪些像素被遮挡。如果摄像机或者灯光移动，由于shadowmap产生的精度有可能不够高，因此我们看出来计算的阴影就是具有“锯齿”的，并且随着摄像机/灯光的移动而不断抖动。\n此外，如果使用级联阴影贴图（CSM）技术，这种技术类似于mipmap技术，根据距离远近切换成为不同精度的阴影。阴影会根据摄像机距离被分割成多个级别。当摄像机移动时，级联边界可能会导致阴影在不同分级之间切换。\nShadow Acne # 实时渲染中另一种常见的阴影伪影问题。它通常表现为物体表面上出现密集的、锯齿状的暗斑或条纹，看起来像一条一条的横白条纹。这种现象主要是由于深度贴图的精度不足或深度值计算误差引起的。\n产生Shadow acne的根本原因是**深度值的自相交（Self-Shadowing）**问题。在生成深度贴图时，渲染引擎会从光源视角计算场景中每个像素的深度值（即距离光源的距离）， 再从摄像机视角再将场景中的像素投影到深度贴图中，比较像素的深度值与深度贴图中的值来判断是否被遮挡。 但是阴影图并不是矢量，精度也不是无限的；当渲染分辨率和阴影深度分辨率出现了一定程度的精度误差，就会错误的认为像素被遮挡，进而产生错误的阴影。\n还有一个就是光源的角度也会影响Acne的严重与否。如上所示，当光投射角度越接近于物体表面的法线，Acne的出现概率就会越高。所以Unity提供了优化的选项：调整 Unity 中的 Bias 和 Normal Bias 参数，可以有效避免深度值冲突问题）。但是调太高会出现没有阴影的问题。\nAnti-Aliasing（抗锯齿） # 由于屏幕分辨率的限制，有些高对比度的边缘会显得不平滑，呈现出“锯齿状”或者“阶梯状”的效果。 所以需要抗锯齿。抗锯齿的核心思想是通过平滑颜色过渡来减少锯齿现象。\n常见的有SSAA（超采样抗锯齿）、FXAA（快速近似抗锯齿）、SMAA（多重采样抗锯齿）、TAA（时域采样抗锯齿）、DLAA/DLSS（深度学习抗锯齿，需要硬件实现）\n从零开始分解UnityShader阴影，从AutoLight到PCF，从百草园杀到三味书屋 - 哔哩哔哩 实时阴影技术（1）Shadow Mapping - KillerAery - 博客园 【Unity3D】阴影原理及应用 - little_fat_sheep - 博客园 关于ShadowMap中Shadow acne现象的解释-CSDN博客 Shadow outlines tutorial ","date":"7 February 2025","externalUrl":null,"permalink":"/posts/unity%E9%98%B4%E5%BD%B1%E5%AE%9E%E7%8E%B0/","section":"","summary":"问题解决方案","title":"Unity阴影实现","type":"posts"},{"content":"Quixel是来自Epic的一个模型资产库，其中的Bridge可以将资产库下载到本地并且进行个性化导出，如Blender、3dsmax、unity等等。但是遗憾的是blender在2.8版本后就不再支持。\n不过Quixel将他们的导入blender插件进行了开源，有人将这个项目进行了fork并做修改。主要是适配了新版blender部分api和节点名字的变化。但是在导入的时候我们会发现导入后的物体大小放大了100倍，而且此时的旋转出现了绕x轴旋转了-90°。（其实旋转是fbx的通病，因为fbx和blender默认的坐标轴的定义不一致出现的错误，主要是着手修改放大问题）\n所以我们找到blender的插件地址进行修改。地址在\n\u0026quot;C:\\Users\\Danny\\AppData\\Roaming\\Blender Foundation\\Blender\\4.2\\scripts\\addons\\NodePreview\\__init__.py\u0026quot;\n修改一：将Principled BSDF改为”原理化 BSDF“ # 位于代码的340行。这是节点生成器的一部分，由于blender更改语言会导致节点api名称也发生变化，因此我们将\nself.parentName = \u0026quot;Principled BSDF\u0026quot;修改为\nself.parentName = \u0026quot;原理化 BSDF\u0026quot;。\n修改二：增加导入后的缩放锁定\u0026amp;旋转适配 # 位于代码的198行。导入fbx后我们需要对缩放进行调整。\nif meshFormat.lower() == \u0026#34;fbx\u0026#34;: bpy.ops.import_scene.fbx(filepath=meshPath) 此时我们要修改成为\nif meshFormat.lower() == \u0026#34;fbx\u0026#34;: bpy.ops.import_scene.fbx(filepath=meshPath,global_scale=0.1,axis_forward=\u0026#39;Z\u0026#39;,axis_up=\u0026#39;Y\u0026#39;) # 缩放和旋转调整 imported_objects = [obj for obj in bpy.context.selected_objects if obj.type == \u0026#39;MESH\u0026#39;] for obj in imported_objects: obj.scale = (0.01, 0.01, 0.01) # 将模型缩小100倍 obj.rotation_euler[0] += 1.5708 # 绕X轴旋转 90 度 (1.5708弧度 ≈ 90度) # get selected objects obj_objects = [ o for o in bpy.context.scene.objects if o.select_get() ] self.selectedObjects += obj_objects 对bpy.ops.import_scene.fbx(filepath=meshPath,global_scale=0.1,axis_forward=\u0026lsquo;Z\u0026rsquo;,axis_up=\u0026lsquo;Y\u0026rsquo;)之后进行了缩放调整和绕x轴旋转90°的调整。\n最后附上修改后的源码：\n# ##### QUIXEL AB - MEGASCANS PLugin FOR BLENDER ##### # # The Megascans Plugin plugin for Blender is an add-on that lets # you instantly import assets with their shader setup with one click only. # # Because it relies on some of the latest 2.80 features, this plugin is currently # only available for Blender 2.80 and forward. # # You are free to modify, add features or tweak this add-on as you see fit, and # don\u0026#39;t hesitate to send us some feedback if you\u0026#39;ve done something cool with it. # # ##### QUIXEL AB - MEGASCANS PLUGIN FOR BLENDER ##### import bpy, threading, os, time, json, socket from bpy.app.handlers import persistent globals()[\u0026#39;Megascans_DataSet\u0026#39;] = None # This stuff is for the Alembic support globals()[\u0026#39;MG_Material\u0026#39;] = [] globals()[\u0026#39;MG_AlembicPath\u0026#39;] = [] globals()[\u0026#39;MG_ImportComplete\u0026#39;] = False bl_info = { \u0026#34;name\u0026#34;: \u0026#34;Megascans Plugin Fork\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Connects Blender to Quixel Bridge for one-click imports with shader setup and geometry\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Quixel \\ Sören Schmidt-Clausen\u0026#34;, \u0026#34;version\u0026#34;: (3, 7, 0), \u0026#34;blender\u0026#34;: (3, 4, 0), \u0026#34;location\u0026#34;: \u0026#34;File \u0026gt; Import\u0026#34;, \u0026#34;warning\u0026#34;: \u0026#34;\u0026#34;, # used for warning icon and text in addons panel \u0026#34;wiki_url\u0026#34;: \u0026#34;https://docs.quixel.org/bridge/livelinks/blender/info_quickstart.html\u0026#34;, \u0026#34;tracker_url\u0026#34;: \u0026#34;https://docs.quixel.org/bridge/livelinks/blender/info_quickstart#release_notes\u0026#34;, \u0026#34;support\u0026#34;: \u0026#34;COMMUNITY\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Import-Export\u0026#34; } # MS_Init_ImportProcess is the main asset import class. # This class is invoked whenever a new asset is set from Bridge. class MS_Init_ImportProcess(): # This initialization method create the data structure to process our assets # later on in the initImportProcess method. The method loops on all assets # that have been sent by Bridge. def __init__(self): print(\u0026#34;Initialized import class...\u0026#34;) try: # Check if there\u0026#39;s any incoming data if globals()[\u0026#39;Megascans_DataSet\u0026#39;] != None: globals()[\u0026#39;MG_AlembicPath\u0026#39;] = [] globals()[\u0026#39;MG_Material\u0026#39;] = [] globals()[\u0026#39;MG_ImportComplete\u0026#39;] = False self.json_Array = json.loads(globals()[\u0026#39;Megascans_DataSet\u0026#39;]) # Start looping over each asset in the self.json_Array list for js in self.json_Array: self.json_data = js self.selectedObjects = [] self.IOR = 1.45 self.assetType = self.json_data[\u0026#34;type\u0026#34;] self.assetPath = self.json_data[\u0026#34;path\u0026#34;] self.assetID = self.json_data[\u0026#34;id\u0026#34;] self.isMetal = bool(self.json_data[\u0026#34;category\u0026#34;] == \u0026#34;Metal\u0026#34;) # Workflow setup. self.isHighPoly = bool(self.json_data[\u0026#34;activeLOD\u0026#34;] == \u0026#34;high\u0026#34;) self.activeLOD = self.json_data[\u0026#34;activeLOD\u0026#34;] self.minLOD = self.json_data[\u0026#34;minLOD\u0026#34;] self.RenderEngine = bpy.context.scene.render.engine.lower() # Get the current render engine. i.e. blender_eevee or cycles self.Workflow = self.json_data.get(\u0026#39;pbrWorkflow\u0026#39;, \u0026#39;specular\u0026#39;) self.DisplacementSetup = \u0026#39;regular\u0026#39; self.isCycles = bool(self.RenderEngine == \u0026#39;cycles\u0026#39;) self.isScatterAsset = self.CheckScatterAsset() self.textureList = [] self.isBillboard = self.CheckIsBillboard() self.ApplyToSelection = False self.isSpecularWorkflow = True self.isAlembic = False self.NormalSetup = False self.BumpSetup = False if \u0026#34;workflow\u0026#34; in self.json_data.keys(): self.isSpecularWorkflow = bool(self.json_data[\u0026#34;workflow\u0026#34;] == \u0026#34;specular\u0026#34;) if \u0026#34;applyToSelection\u0026#34; in self.json_data.keys(): self.ApplyToSelection = bool(self.json_data[\u0026#34;applyToSelection\u0026#34;]) if (self.isCycles): if(bpy.context.scene.cycles.feature_set == \u0026#39;EXPERIMENTAL\u0026#39;): self.DisplacementSetup = \u0026#39;adaptive\u0026#39; texturesListName = \u0026#34;components\u0026#34; if(self.isBillboard): texturesListName = \u0026#34;components\u0026#34; # Get a list of all available texture maps. item[1] returns the map type (albedo, normal, etc...). self.textureTypes = [obj[\u0026#34;type\u0026#34;] for obj in self.json_data[texturesListName]] self.textureList = [] for obj in self.json_data[texturesListName]: texFormat = obj[\u0026#34;format\u0026#34;] texType = obj[\u0026#34;type\u0026#34;] texPath = obj[\u0026#34;path\u0026#34;] if texType == \u0026#34;displacement\u0026#34; and texFormat != \u0026#34;exr\u0026#34;: texDir = os.path.dirname(texPath) texName = os.path.splitext(os.path.basename(texPath))[0] if os.path.exists(os.path.join(texDir, texName + \u0026#34;.exr\u0026#34;)): texPath = os.path.join(texDir, texName + \u0026#34;.exr\u0026#34;) texFormat = \u0026#34;exr\u0026#34; # Replace diffuse texture type with albedo so we don\u0026#39;t have to add more conditions to handle diffuse map. if texType == \u0026#34;diffuse\u0026#34; and \u0026#34;albedo\u0026#34; not in self.textureTypes: texType = \u0026#34;albedo\u0026#34; self.textureTypes.append(\u0026#34;albedo\u0026#34;) self.textureTypes.remove(\u0026#34;diffuse\u0026#34;) # Normal / Bump setup checks if texType == \u0026#34;normal\u0026#34;: self.NormalSetup = True if texType == \u0026#34;bump\u0026#34;: self.BumpSetup = True self.textureList.append((texFormat, texType, texPath)) # Create a tuple list of all the 3d meshes available. # This tuple is composed of (meshFormat, meshPath) self.geometryList = [(obj[\u0026#34;format\u0026#34;], obj[\u0026#34;path\u0026#34;]) for obj in self.json_data[\u0026#34;meshList\u0026#34;]] # Create name of our asset. Multiple conditions are set here # in order to make sure the asset actually has a name and that the name # is short enough for us to use it. We compose a name with the ID otherwise. if \u0026#34;name\u0026#34; in self.json_data.keys(): self.assetName = self.json_data[\u0026#34;name\u0026#34;].replace(\u0026#34; \u0026#34;, \u0026#34;_\u0026#34;) else: self.assetName = os.path.basename(self.json_data[\u0026#34;path\u0026#34;]).replace(\u0026#34; \u0026#34;, \u0026#34;_\u0026#34;) if len(self.assetName.split(\u0026#34;_\u0026#34;)) \u0026gt; 2: self.assetName = \u0026#34;_\u0026#34;.join(self.assetName.split(\u0026#34;_\u0026#34;)[:-1]) self.materialName = self.assetName + \u0026#39;_\u0026#39; + self.assetID self.colorSpaces = [\u0026#34;sRGB\u0026#34;, \u0026#34;Non-Color\u0026#34;, \u0026#34;Linear\u0026#34;] # Initialize the import method to start building our shader and import our geometry self.initImportProcess() print(\u0026#34;Imported asset from \u0026#34; + self.assetName + \u0026#34; Quixel Bridge\u0026#34;) if len(globals()[\u0026#39;MG_AlembicPath\u0026#39;]) \u0026gt; 0: globals()[\u0026#39;MG_ImportComplete\u0026#39;] = True except Exception as e: print( \u0026#34;Megascans Plugin Error initializing the import process. Error: \u0026#34;, str(e) ) globals()[\u0026#39;Megascans_DataSet\u0026#39;] = None # this method is used to import the geometry and create the material setup. def initImportProcess(self): try: if len(self.textureList) \u0026gt;= 1: if(self.ApplyToSelection and self.assetType not in [\u0026#34;3dplant\u0026#34;, \u0026#34;3d\u0026#34;]): self.CollectSelectedObjects() self.ImportGeometry() self.CreateMaterial() self.ApplyMaterialToGeometry() if(self.isScatterAsset and len(self.selectedObjects) \u0026gt; 1): self.ScatterAssetSetup() elif (self.assetType == \u0026#34;3dplant\u0026#34; and len(self.selectedObjects) \u0026gt; 1): self.PlantAssetSetup() self.SetupMaterial() if self.isAlembic: globals()[\u0026#39;MG_Material\u0026#39;].append(self.mat) except Exception as e: print( \u0026#34;Megascans Plugin Error while importing textures/geometry or setting up material. Error: \u0026#34;, str(e) ) def ImportGeometry(self): try: # Import geometry abcPaths = [] if len(self.geometryList) \u0026gt;= 1: for obj in self.geometryList: meshPath = obj[1] meshFormat = obj[0] if meshFormat.lower() == \u0026#34;fbx\u0026#34;: bpy.ops.import_scene.fbx(filepath=meshPath,global_scale=0.1,axis_forward=\u0026#39;Z\u0026#39;,axis_up=\u0026#39;Y\u0026#39;) # 缩放和旋转调整 imported_objects = [obj for obj in bpy.context.selected_objects if obj.type == \u0026#39;MESH\u0026#39;] for obj in imported_objects: obj.scale = (0.01, 0.01, 0.01) # 将模型缩小100倍 obj.rotation_euler[0] += 1.5708 # 绕X轴旋转 90 度 (1.5708弧度 ≈ 90度) # get selected objects obj_objects = [ o for o in bpy.context.scene.objects if o.select_get() ] self.selectedObjects += obj_objects elif meshFormat.lower() == \u0026#34;obj\u0026#34;: if bpy.app.version \u0026lt; (2, 92, 0): bpy.ops.import_scene.obj(filepath=meshPath, use_split_objects = True, use_split_groups = True, global_clight_size = 1.0) else: bpy.ops.import_scene.obj(filepath=meshPath, use_split_objects = True, use_split_groups = True, global_clamp_size = 1.0) # get selected objects obj_objects = [ o for o in bpy.context.scene.objects if o.select_get() ] self.selectedObjects += obj_objects elif meshFormat.lower() == \u0026#34;abc\u0026#34;: self.isAlembic = True abcPaths.append(meshPath) if self.isAlembic: globals()[\u0026#39;MG_AlembicPath\u0026#39;].append(abcPaths) except Exception as e: print( \u0026#34;Megascans Plugin Error while importing textures/geometry or setting up material. Error: \u0026#34;, str(e) ) def dump(self, obj): for attr in dir(obj): print(\u0026#34;obj.%s = %r\u0026#34; % (attr, getattr(obj, attr))) def CollectSelectedObjects(self): try: sceneSelectedObjects = [ o for o in bpy.context.scene.objects if o.select_get() ] for obj in sceneSelectedObjects: if obj.type == \u0026#34;MESH\u0026#34;: self.selectedObjects.append(obj) except Exception as e: print(\u0026#34;Megascans Plugin Error::CollectSelectedObjects::\u0026#34;, str(e) ) def ApplyMaterialToGeometry(self): for obj in self.selectedObjects: # assign material to obj obj.active_material = self.mat def CheckScatterAsset(self): if(\u0026#39;scatter\u0026#39; in self.json_data[\u0026#39;categories\u0026#39;] or \u0026#39;scatter\u0026#39; in self.json_data[\u0026#39;tags\u0026#39;] or \u0026#39;cmb_asset\u0026#39; in self.json_data[\u0026#39;categories\u0026#39;] or \u0026#39;cmb_asset\u0026#39; in self.json_data[\u0026#39;tags\u0026#39;]): return True return False def CheckIsBillboard(self): # Use billboard textures if importing the Billboard LOD. if(self.assetType == \u0026#34;3dplant\u0026#34;): if (self.activeLOD == self.minLOD): return True return False #Add empty parent for the scatter assets. def ScatterAssetSetup(self): bpy.ops.object.empty_add(type=\u0026#39;ARROWS\u0026#39;) emptyRefList = [ o for o in bpy.context.scene.objects if o.select_get() and o not in self.selectedObjects ] for scatterParentObject in emptyRefList: scatterParentObject.name = self.assetID + \u0026#34;_\u0026#34; + self.assetName for obj in self.selectedObjects: obj.parent = scatterParentObject break #Add empty parent for plants. def PlantAssetSetup(self): bpy.ops.object.empty_add(type=\u0026#39;ARROWS\u0026#39;) emptyRefList = [ o for o in bpy.context.scene.objects if o.select_get() and o not in self.selectedObjects ] for plantParentObject in emptyRefList: plantParentObject.name = self.assetID + \u0026#34;_\u0026#34; + self.assetName for obj in self.selectedObjects: obj.parent = plantParentObject break # def AddModifiersToGeomtry(self, geo_list, mat): # for obj in geo_list: # # assign material to obj # bpy.ops.object.modifier_add(type=\u0026#39;SOLIDIFY\u0026#39;) #Shader setups for all asset types. Some type specific functionality is also handled here. def SetupMaterial (self): if \u0026#34;albedo\u0026#34; in self.textureTypes: if \u0026#34;ao\u0026#34; in self.textureTypes: self.CreateTextureMultiplyNode(\u0026#34;albedo\u0026#34;, \u0026#34;ao\u0026#34;, -250, 320, -640, 460, -640, 200, 0, 1, True, \u0026#34;Base Color\u0026#34;) else: self.CreateTextureNode(\u0026#34;albedo\u0026#34;, -640, 420, 0, True, \u0026#34;Base Color\u0026#34;) if self.isSpecularWorkflow: if \u0026#34;specular\u0026#34; in self.textureTypes: self.CreateTextureNode(\u0026#34;specular\u0026#34;, -1150, 200, 0, True, \u0026#34;Specular\u0026#34;) if \u0026#34;gloss\u0026#34; in self.textureTypes: glossNode = self.CreateTextureNode(\u0026#34;gloss\u0026#34;, -1150, -60) invertNode = self.CreateGenericNode(\u0026#34;ShaderNodeInvert\u0026#34;, -250, 60) # Add glossNode to invertNode connection self.node_group.links.new(invertNode.inputs[\u0026#34;Color\u0026#34;], glossNode.outputs[\u0026#34;Color\u0026#34;]) # Connect roughness node to the material parent node. self.ConnectNodeToMaterial(\u0026#34;Roughness\u0026#34;, invertNode) elif \u0026#34;roughness\u0026#34; in self.textureTypes: self.CreateTextureNode(\u0026#34;roughness\u0026#34;, -1150, -60, 1, True, \u0026#34;Roughness\u0026#34;) else: if \u0026#34;metalness\u0026#34; in self.textureTypes: self.CreateTextureNode(\u0026#34;metalness\u0026#34;, -1150, 200, 1, True, \u0026#34;Metallic\u0026#34;) if \u0026#34;roughness\u0026#34; in self.textureTypes: self.CreateTextureNode(\u0026#34;roughness\u0026#34;, -1150, -60, 1, True, \u0026#34;Roughness\u0026#34;) elif \u0026#34;gloss\u0026#34; in self.textureTypes: glossNode = self.CreateTextureNode(\u0026#34;gloss\u0026#34;, -1150, -60) invertNode = self.CreateGenericNode(\u0026#34;ShaderNodeInvert\u0026#34;, -250, 60) # Add glossNode to invertNode connection self.node_group.links.new(invertNode.inputs[\u0026#34;Color\u0026#34;], glossNode.outputs[\u0026#34;Color\u0026#34;]) # Connect roughness node to the material parent node. self.node_group.links.new(self.nodes.get(self.parentName).inputs[\u0026#34;Roughness\u0026#34;], invertNode.outputs[\u0026#34;Color\u0026#34;]) self.ConnectNodeToMaterial(\u0026#34;Roughness\u0026#34;, invertNode) if \u0026#34;opacity\u0026#34; in self.textureTypes: self.CreateTextureNode(\u0026#34;opacity\u0026#34;, -1550, -160, 1, True, \u0026#34;Alpha\u0026#34;) self.mat.blend_method = \u0026#39;HASHED\u0026#39; if \u0026#34;translucency\u0026#34; in self.textureTypes: self.CreateTextureNode(\u0026#34;translucency\u0026#34;, -1550, -420, 0, True, \u0026#34;Transmission\u0026#34;) elif \u0026#34;transmission\u0026#34; in self.textureTypes: self.CreateTextureNode(\u0026#34;transmission\u0026#34;, -1550, -420, 1, True, \u0026#34;Transmission\u0026#34;) # If HIGH POLY selected \u0026gt; use normal_bump and no displacement # If LODs selected \u0026gt; use corresponding LODs normal + displacement if self.isHighPoly: self.BumpSetup = False self.CreateNormalNodeSetup(True, \u0026#34;Normal\u0026#34;) if \u0026#34;displacement\u0026#34; in self.textureTypes and not self.isHighPoly: self.CreateDisplacementSetup(True) def CreateMaterial(self): self.mat = (bpy.data.materials.get( self.materialName ) or bpy.data.materials.new( self.materialName )) self.mat.use_nodes = True self.nodes = self.mat.node_tree.nodes self.parentName = \u0026#34;原理化 BSDF\u0026#34; self.materialOutputName = \u0026#34;Material Output\u0026#34; self.mat.node_tree.nodes[self.parentName].distribution = \u0026#39;MULTI_GGX\u0026#39; #self.mat.node_tree.nodes[self.parentName].inputs[\u0026#34;Metallic\u0026#34;].default_value = 1 if self.isMetal else 0 # Metallic value #self.mat.node_tree.nodes[self.parentName].inputs[\u0026#34;IOR\u0026#34;].default_value = self.IOR #self.mat.node_tree.nodes[self.parentName].inputs[\u0026#34;Specular\u0026#34;].default_value = 0 Macht kein Sinn! Sieht mit Specular besser aus. #self.mat.node_tree.nodes[self.parentName].inputs[\u0026#34;Clearcoat\u0026#34;].default_value = 0 #Create Node Group self.node_group = bpy.data.node_groups.new(name=self.assetType + \u0026#34;_\u0026#34; + self.materialName, type=\u0026#34;ShaderNodeTree\u0026#34;) #Create Input self.node_group_in = self.node_group.nodes.new(\u0026#34;NodeGroupInput\u0026#34;) self.node_group_in.location = (-1500, 0) self.node_group.interface.new_socket(name=\u0026#34;NodeSocketVector\u0026#34;, socket_type=\u0026#34;NodeSocketVector\u0026#34;) #Create Output self.node_group_out = self.node_group.nodes.new(\u0026#34;NodeGroupOutput\u0026#34;) self.node_group_out.location = (500, 0) #Outputs are assigned later #Instance it in node Tree self.node_group_inst = self.mat.node_tree.nodes.new(\u0026#34;ShaderNodeGroup\u0026#34;) self.node_group_inst.node_tree = self.node_group self.node_group_inst.location = (-600, 297) self.mappingNode = None #Hehe do it anyway #if self.isCycles and self.assetType not in [\u0026#34;3d\u0026#34;, \u0026#34;3dplant\u0026#34;]: # Create mapping node. self.mappingNode = self.mat.node_tree.nodes.new(\u0026#34;ShaderNodeMapping\u0026#34;) self.mappingNode.location = (-1000, 389) self.mappingNode.vector_type = \u0026#39;TEXTURE\u0026#39; # Create texture coordinate node. texCoordNode = self.mat.node_tree.nodes.new(\u0026#34;ShaderNodeTexCoord\u0026#34;) texCoordNode.location = (-1200, 389) # Connect texCoordNode to the mappingNode self.mat.node_tree.links.new(self.mappingNode.inputs[\u0026#34;Vector\u0026#34;], texCoordNode.outputs[\u0026#34;UV\u0026#34;]) #Connect mapping node to Group Input self.mat.node_tree.links.new(self.mappingNode.outputs[\u0026#34;Vector\u0026#34;], self.node_group_inst.inputs[0]) def CreateTextureNode(self, textureType, PosX, PosY, colorspace = 1, connectToMaterial = False, materialInputIndex = \u0026#34;\u0026#34;): texturePath = self.GetTexturePath(textureType) textureNode = self.CreateGenericNode(\u0026#39;ShaderNodeTexImage\u0026#39;, PosX, PosY) textureNode.image = bpy.data.images.load(texturePath) textureNode.show_texture = True textureNode.image.colorspace_settings.name = self.colorSpaces[colorspace] # \u0026#34;sRGB\u0026#34;, \u0026#34;Non-Color\u0026#34;, \u0026#34;Linear\u0026#34; if textureType in [\u0026#34;albedo\u0026#34;, \u0026#34;specular\u0026#34;, \u0026#34;translucency\u0026#34;]: if self.GetTextureFormat(textureType) in \u0026#34;exr\u0026#34;: textureNode.image.colorspace_settings.name = self.colorSpaces[2] # \u0026#34;sRGB\u0026#34;, \u0026#34;Non-Color\u0026#34;, \u0026#34;Linear\u0026#34; if connectToMaterial: self.ConnectNodeToMaterial(materialInputIndex, textureNode) #Connect Uvs to Vector unput self.node_group.links.new(textureNode.inputs[\u0026#34;Vector\u0026#34;], self.node_group_in.outputs[0]) return textureNode def CreateTextureMultiplyNode(self, aTextureType, bTextureType, PosX, PosY, aPosX, aPosY, bPosX, bPosY, aColorspace, bColorspace, connectToMaterial, materialInputIndex): #Add Color\u0026gt;MixRGB node, transform it in the node editor, change it\u0026#39;s operation to Multiply and finally we colapse the node. multiplyNode = self.CreateGenericNode(\u0026#39;ShaderNodeMixRGB\u0026#39;, PosX, PosY) multiplyNode.blend_type = \u0026#39;MULTIPLY\u0026#39; #Setup A and B nodes textureNodeA = self.CreateTextureNode(aTextureType, aPosX, aPosY, aColorspace) textureNodeB = self.CreateTextureNode(bTextureType, bPosX, bPosY, bColorspace) # Conned albedo and ao node to the multiply node. self.node_group.links.new(multiplyNode.inputs[\u0026#34;Color1\u0026#34;], textureNodeA.outputs[\u0026#34;Color\u0026#34;]) self.node_group.links.new(multiplyNode.inputs[\u0026#34;Color2\u0026#34;], textureNodeB.outputs[\u0026#34;Color\u0026#34;]) if connectToMaterial: self.ConnectNodeToMaterial(materialInputIndex, multiplyNode) return multiplyNode def CreateNormalNodeSetup(self, connectToMaterial, materialInputIndex): bumpNode = None normalNode = None bumpMapNode = None normalMapNode = None if self.NormalSetup and self.BumpSetup: bumpMapNode = self.CreateTextureNode(\u0026#34;bump\u0026#34;, -640, -130) normalMapNode = self.CreateTextureNode(\u0026#34;normal\u0026#34;, -1150, -580) bumpNode = self.CreateGenericNode(\u0026#34;ShaderNodeBump\u0026#34;, -250, -170) bumpNode.inputs[\u0026#34;Strength\u0026#34;].default_value = 0.1 normalNode = self.CreateGenericNode(\u0026#34;ShaderNodeNormalMap\u0026#34;, -640, -400) # Add normalMapNode to normalNode connection self.node_group.links.new(normalNode.inputs[\u0026#34;Color\u0026#34;], normalMapNode.outputs[\u0026#34;Color\u0026#34;]) # Add bumpMapNode and normalNode connection to the bumpNode self.node_group.links.new(bumpNode.inputs[\u0026#34;Height\u0026#34;], bumpMapNode.outputs[\u0026#34;Color\u0026#34;]) if (2, 81, 0) \u0026gt; bpy.app.version: self.node_group.links.new(bumpNode.inputs[\u0026#34;Normal\u0026#34;], normalNode.outputs[\u0026#34;Normal\u0026#34;]) else: self.node_group.links.new(bumpNode.inputs[\u0026#34;Normal\u0026#34;], normalNode.outputs[\u0026#34;Normal\u0026#34;]) # Add bumpNode connection to the material parent node if connectToMaterial: self.ConnectNodeToMaterial(materialInputIndex, bumpNode) elif self.NormalSetup: normalMapNode = self.CreateTextureNode(\u0026#34;normal\u0026#34;, -640, -207) normalNode = self.CreateGenericNode(\u0026#34;ShaderNodeNormalMap\u0026#34;, -250, -170) # Add normalMapNode to normalNode connection self.node_group.links.new(normalNode.inputs[\u0026#34;Color\u0026#34;], normalMapNode.outputs[\u0026#34;Color\u0026#34;]) # Add normalNode connection to the material parent node if connectToMaterial: self.ConnectNodeToMaterial(materialInputIndex, normalNode) elif self.BumpSetup: bumpMapNode = self.CreateTextureNode(\u0026#34;bump\u0026#34;, -640, -207) bumpNode = self.CreateGenericNode(\u0026#34;ShaderNodeBump\u0026#34;, -250, -170) bumpNode.inputs[\u0026#34;Strength\u0026#34;].default_value = 0.1 # Add bumpMapNode and normalNode connection to the bumpNode self.node_group.links.new(bumpNode.inputs[\u0026#34;Height\u0026#34;], bumpMapNode.outputs[\u0026#34;Color\u0026#34;]) # Add bumpNode connection to the material parent node if connectToMaterial: self.ConnectNodeToMaterial(materialInputIndex, bumpNode) def CreateDisplacementSetup(self, connectToMaterial): #Achtung könnte was kaputt machen was ich hier gemnacht hab SOEREN displacementMapNode = self.CreateTextureNode(\u0026#34;displacement\u0026#34;, -640, -740) self.JustConnectToGroupOutput(displacementMapNode, \u0026#34;NodeSocketFloat\u0026#34;, \u0026#34;Height\u0026#34;) def JustConnectToGroupOutput(self, textureNode, socket_type, name): #Create the output for the node_group: output = self.node_group.interface.new_socket(name=name, in_out=\u0026#34;OUTPUT\u0026#34;, socket_type=socket_type) #Connect to node group self.node_group.links.new(textureNode.outputs[0], self.node_group_out.inputs[name]) def ConnectNodeToMaterial(self, materialInputIndex, textureNode): #Get the input in the Principled BSDF: #Name for the Material bsdf_in = self.nodes.get(self.parentName).inputs[materialInputIndex] # Remove Factor from ID, I actually have no idea why its even standing there node_socket_id = bsdf_in.bl_idname.replace(\u0026#34;Factor\u0026#34;, \u0026#34;\u0026#34;) self.JustConnectToGroupOutput(textureNode, node_socket_id, bsdf_in.name) #Note this doesnt work for duplicate names, but this shouldn\u0026#39;t happen here self.mat.node_tree.links.new(self.node_group_inst.outputs[bsdf_in.name], bsdf_in) def CreateGenericNode(self, nodeName, PosX, PosY): genericNode = self.node_group.nodes.new(nodeName) genericNode.location = (PosX, PosY) return genericNode def GetTexturePath(self, textureType): for item in self.textureList: if item[1] == textureType: path = item[2] path = path.replace(\u0026#34;\\\\\\\\\u0026#34;, \u0026#34;*\u0026#34;) path = path.replace(\u0026#34;\\\\\u0026#34;, \u0026#34;/\u0026#34;) path = path.replace(\u0026#34;*\u0026#34;, \u0026#34;\\\\\\\\\u0026#34;) print(path) return path def GetTextureFormat(self, textureType): for item in self.textureList: if item[1] == textureType: return item[0].lower() class ms_Init(threading.Thread): #Initialize the thread and assign the method (i.e. importer) to be called when it receives JSON data. def __init__(self, importer): threading.Thread.__init__(self) self.importer = importer #Start the thread to start listing to the port. def run(self): try: run_livelink = True host, port = \u0026#39;localhost\u0026#39;, 28888 #Making a socket object. socket_ = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #Binding the socket to host and port number mentioned at the start. socket_.bind((host, port)) #Run until the thread starts receiving data. while run_livelink: socket_.listen(5) #Accept connection request. client, addr = socket_.accept() data = \u0026#34;\u0026#34; buffer_size = 4096*2 #Receive data from the client. data = client.recv(buffer_size) if data == b\u0026#39;Bye Megascans\u0026#39;: run_livelink = False break #If any data is received over the port. if data != \u0026#34;\u0026#34;: self.TotalData = b\u0026#34;\u0026#34; self.TotalData += data #Append the previously received data to the Total Data. #Keep running until the connection is open and we are receiving data. while run_livelink: #Keep receiving data from client. data = client.recv(4096*2) if data == b\u0026#39;Bye Megascans\u0026#39;: run_livelink = False break #if we are getting data keep appending it to the Total data. if data : self.TotalData += data else: #Once the data transmission is over call the importer method and send the collected TotalData. self.importer(self.TotalData) break except Exception as e: print( \u0026#34;Megascans Plugin Error initializing the thread. Error: \u0026#34;, str(e) ) class thread_checker(threading.Thread): #Initialize the thread and assign the method (i.e. importer) to be called when it receives JSON data. def __init__(self): threading.Thread.__init__(self) #Start the thread to start listing to the port. def run(self): try: run_checker = True while run_checker: time.sleep(3) for i in threading.enumerate(): if(i.getName() == \u0026#34;MainThread\u0026#34; and i.is_alive() == False): host, port = \u0026#39;localhost\u0026#39;, 28888 s = socket.socket() s.connect((host,port)) data = \u0026#34;Bye Megascans\u0026#34; s.send(data.encode()) s.close() run_checker = False break except Exception as e: print( \u0026#34;Megascans Plugin Error initializing thread checker. Error: \u0026#34;, str(e) ) pass class MS_Init_LiveLink(bpy.types.Operator): bl_idname = \u0026#34;bridge.plugin\u0026#34; bl_label = \u0026#34;Megascans Plugin\u0026#34; socketCount = 0 def execute(self, context): try: globals()[\u0026#39;Megascans_DataSet\u0026#39;] = None self.thread_ = threading.Thread(target = self.socketMonitor) self.thread_.start() bpy.app.timers.register(self.newDataMonitor) return {\u0026#39;FINISHED\u0026#39;} except Exception as e: print( \u0026#34;Megascans Plugin Error starting blender plugin. Error: \u0026#34;, str(e) ) return {\u0026#34;FAILED\u0026#34;} def newDataMonitor(self): try: if globals()[\u0026#39;Megascans_DataSet\u0026#39;] != None: MS_Init_ImportProcess() globals()[\u0026#39;Megascans_DataSet\u0026#39;] = None except Exception as e: print( \u0026#34;Megascans Plugin Error starting blender plugin (newDataMonitor). Error: \u0026#34;, str(e) ) return {\u0026#34;FAILED\u0026#34;} return 1.0 def socketMonitor(self): try: #Making a thread object threadedServer = ms_Init(self.importer) #Start the newly created thread. threadedServer.start() #Making a thread object thread_checker_ = thread_checker() #Start the newly created thread. thread_checker_.start() except Exception as e: print( \u0026#34;Megascans Plugin Error starting blender plugin (socketMonitor). Error: \u0026#34;, str(e) ) return {\u0026#34;FAILED\u0026#34;} def importer (self, recv_data): try: globals()[\u0026#39;Megascans_DataSet\u0026#39;] = recv_data except Exception as e: print( \u0026#34;Megascans Plugin Error starting blender plugin (importer). Error: \u0026#34;, str(e) ) return {\u0026#34;FAILED\u0026#34;} class MS_Init_Abc(bpy.types.Operator): bl_idname = \u0026#34;ms_livelink_abc.py\u0026#34; bl_label = \u0026#34;Import ABC\u0026#34; def execute(self, context): try: if globals()[\u0026#39;MG_ImportComplete\u0026#39;]: assetMeshPaths = globals()[\u0026#39;MG_AlembicPath\u0026#39;] assetMaterials = globals()[\u0026#39;MG_Material\u0026#39;] if len(assetMeshPaths) \u0026gt; 0 and len(assetMaterials) \u0026gt; 0: materialIndex = 0 old_materials = [] for meshPaths in assetMeshPaths: for meshPath in meshPaths: bpy.ops.wm.alembic_import(filepath=meshPath, as_background_job=False) for o in bpy.context.scene.objects: if o.select_get(): old_materials.append(o.active_material) o.active_material = assetMaterials[materialIndex] materialIndex += 1 for mat in old_materials: try: if mat is not None: bpy.data.materials.remove(mat) except: pass globals()[\u0026#39;MG_AlembicPath\u0026#39;] = [] globals()[\u0026#39;MG_Material\u0026#39;] = [] globals()[\u0026#39;MG_ImportComplete\u0026#39;] = False return {\u0026#39;FINISHED\u0026#39;} except Exception as e: print( \u0026#34;Megascans Plugin Error starting MS_Init_Abc. Error: \u0026#34;, str(e) ) return {\u0026#34;CANCELLED\u0026#34;} @persistent def load_plugin(scene): try: bpy.ops.bridge.plugin() except Exception as e: print( \u0026#34;Bridge Plugin Error::Could not start the plugin. Description: \u0026#34;, str(e) ) def menu_func_import(self, context): self.layout.operator(MS_Init_Abc.bl_idname, text=\u0026#34;Megascans: Import Alembic Files\u0026#34;) def register(): if len(bpy.app.handlers.load_post) \u0026gt; 0: # Check if trying to register twice. if \u0026#34;load_plugin\u0026#34; in bpy.app.handlers.load_post[0].__name__.lower() or load_plugin in bpy.app.handlers.load_post: return bpy.utils.register_class(MS_Init_LiveLink) bpy.utils.register_class(MS_Init_Abc) bpy.app.handlers.load_post.append(load_plugin) bpy.types.TOPBAR_MT_file_import.append(menu_func_import) def unregister(): bpy.types.TOPBAR_MT_file_import.remove(menu_func_import) if len(bpy.app.handlers.load_post) \u0026gt; 0: # Check if trying to register twice. if \u0026#34;load_plugin\u0026#34; in bpy.app.handlers.load_post[0].__name__.lower() or load_plugin in bpy.app.handlers.load_post: bpy.app.handlers.load_post.remove(load_plugin) ","date":"6 February 2025","externalUrl":null,"permalink":"/posts/quixel%E5%AF%BC%E5%85%A5blender%E7%9A%84bl%E6%8F%92%E4%BB%B6%E4%BB%A5%E5%8F%8A%E4%B8%AD%E6%96%87%E5%8C%96%E9%80%82%E9%85%8Dbug%E4%BF%AE%E6%94%B9/","section":"","summary":"Blender导入Unity指南","title":"quixel导入blender的bl插件以及中文化适配\u0026bug修改","type":"posts"},{"content":"比较有名的Schink近似式。本身的计算式并不难，就是描述光在两个介质中反射光强度和折射光强度的规律。\n菲涅尔方程（Fresnel equations），也称为菲涅尔条件，是描述光在两种不同介质界面处反射和折射行为的一组公式。这些方程由法国物理学家奥古斯丁·让·菲涅耳于19世纪初提出，主要用于计算入射光、反射光和折射光之间的振幅比。根据光的偏振状态，菲涅尔方程分为两种情况：平行偏振光（s偏振或TE模式）和垂直偏振光（p偏振或TM模式）。\n菲涅尔方程的基本形式 # 对于从介质1（折射率为(n_1)）以角度(\\theta_1)入射到介质2（折射率为(n_2)）的平面波，其反射率(R)和透射率(T)可以通过以下方程计算：\n对于s偏振光： # 对于p偏振光： # 其中， 是折射角，遵循斯涅尔定律（Snell\u0026rsquo;s law）: 反射率(R)可以由反射系数的模平方给出，即R=|r|2\n什么是P光、什么是S光？ # P光和S光是针对光的入射反射和折射来说的，此时，将入射光与反射光/折射光所在的平面定义为参考面，S光和P光分别为偏振方向垂直以及平行于该参考面的线偏振光。\n举例而言，如下图所示，一束光照射到入射面上，形成反射和折射，将入射光束与法线所形成的平面定义为入射平面。P光（德文Parallel的首字母，意为平行的）是偏振方向与入射平面平行的光，而S光（德文Senkrecht的首字母，意为竖直的）是偏振方向与入射平面垂直的光。\nSchlink近似 # 但是在图形学中，一般不会使用这种p光和s光的表达式，用于估算光在介质界面发生反射时的反射系数的一种简化公式，特别适用于计算机图形学中的光线追踪算法。\n对于s偏振光和p偏振光，Schlick近似可以将p光和s光的反射率统一表达为：\n用代码可以表示为：\nvec3 fresnelSchlick(float cosTheta, vec3 R0) { return R0 + (1.0 - R0) * pow(1.0 - cosTheta, 5.0); } 其中：\n是在给定入射角θ下的反射率。 是当光垂直入射（即θ=0）时的反射率，可以通过两种介质的折射率n1和n2计算得出： cos⁡θ是入射角的余弦值。 利用这个特点可以制作外发光。 引用：https://zhuanlan.zhihu.com/p/375746359 ","date":"31 January 2025","externalUrl":null,"permalink":"/posts/%E8%8F%B2%E6%B6%85%E5%B0%94%E8%AE%A1%E7%AE%97%E5%BC%8F%E4%BB%A5%E5%8F%8A%E8%BF%91%E4%BC%BC%E5%85%AC%E5%BC%8F/","section":"","summary":"算法原理解析","title":"菲涅尔计算式以及近似公式","type":"posts"},{"content":"我在跟教程的时候，发现principal shader节点再用Solaris的karma渲染的时候会出现概率性的材质丢失；并且有的材质节点的channels不再会被渲染出来（比如粗糙度，法线映射）\nMantra渲染\nKarma渲染\n很明显材质贴图出现了部分参数的丢失，并且整个球都变得圆润起来了（其实一开始的球甚至全是绿色，也就是再obj的group分组材质分配节点不再会被渲染）\n结合Karma可以调用gpu进行渲染 mantra的光线追踪和噪点都十分难绷的情况下，切换为Karma工作流显得迫在眉睫，但是我很多节点都用到了材质分配节点啊！我要怎么改？\n问了一下claude，它是这么回答的：\n这是从Mantra切换到Karma时的常见问题。材质丢失的原因和解决方法：\n主要原因 # 1. 材质节点不兼容 # Mantra使用VOP网络和Principled Shader Karma使用MaterialX和USD材质 某些Mantra特有的节点在Karma中不支持 2. 自动转换局限 # Houdini会尝试自动转换，但不是100%完整 复杂的材质网络可能转换不完全 解决方法 # 方法1：使用Material Builder # 1. 在Material Builder中重新构建材质 2. 使用Karma兼容的节点 3. 主要使用 mtlxstandard_surface 节点 方法2：检查Material Network # 在Solaris中添加Material Library节点 确保材质正确分配给几何体 方法3：手动转换 # 1. 创建新的Material Builder 2. 使用 mtlxstandard_surface 替代 principled shader 3. 重新连接贴图和参数 方法4：使用Karma Material # 直接使用Karma专用的材质节点 避免使用Mantra特有的材质 推荐工作流程 # 为了避免材质丢失，建议从项目开始就使用MaterialX标准，这样在Mantra和Karma之间切换时兼容性更好。\n你具体丢失了哪种类型的材质？我可以提供更针对性的解决方案。\nMaterialX标准，我在Houdini官网上看了一下：\nMaterialX是一种开放源代码的标准，用于描述着色网络，旨在跨应用和渲染器之间实现材料的可移植性。通过UsdMaterialX插件，USD（Universal Scene Description）能够导入MaterialX着色器，而Houdini允许用户在VOP中构建MaterialX材质，并自动将其转换为UsdShade节点。尽管存在将VOP节点转换为.mtx文件的脚本，但Solaris主要支持作为USD prim的MaterialX节点。\n也就是说MaterialX是一个开放源代码的标准，可以描述着色网络，并且被设计成可以在不同的应用程序和渲染器之间共享。此外，Karma XPU不会支持传统基于VEX的着色器，由于优异的跨平台性，MaterialX是构建Karma渲染器的未来。\n那要怎么切换为支持karma的材质？ # Karma的材质节点一般是以mtlx开头；比如说mtlxstandard__surface shader；_在这里“mtlx”是MaterialX的文件扩展名，它是一种用于描述着色网络的开放源代码标准。在Solaris中，用户可以创建纯MaterialX材质或Karma中心化的材质。而Karma只支持体积材质，不支持表面和体积材质同时存在于同一个prim上。此外，Compositing节点也不受Karma支持。\n直接重命名方法不可行，因为节点类型不同，参数结构也不一样。但有几种批量替换的方法：\n推荐方法 # 方法1：再Material Network中使用Type Properties批量替换 # 1. 选中所有principled节点 2. 右键 → Change Type → mtlxstandard_surface 3. Houdini会尝试自动映射相似参数 Mark小知识：parameter和name在材质节点之间的区别： # 1. 在 OBJ 级（比如 /obj/geo1）使用 material 节点分配材质时 # Material 节点分配材质的机制 # Material 节点要求你指定一个材质路径，比如 /mat/test_mat 或 /shop/my_shader。 你可以为不同 primitive 或 group 分配不同的材质路径。 这里的“路径”其实就是材质节点的名字或其路径。 2. “Name”和“Parameter”在这里的含义 # Keep Name # 指的是材质节点本身在网络树中的节点名称，比如 test_mat。 Material 节点分配材质时，是通过路径或名字找到材质节点，并将材质“指派”给几何体。 Keep Parameters # 指的是材质节点上的参数值（比如颜色、roughness、贴图路径等）。 这些参数决定了材质的具体外观。 3. 答案总结 # 你在 OBJ 里用 material 节点分配材质时—— # 起决定作用的是“材质节点的名字/路径”\n（即你在 material 节点里填的 /mat/xxx，这里的 xxx 是材质节点的名字） 当你切换材质类型时，如果材质节点的名字没有变，material 节点仍然能找到并分配它。 如果你只保留了参数，但节点名字变了，material 节点找不到那个名字就不会分配成功。 换句话说： # Material 节点分配材质时，主要靠“名字/路径”起作用。 “参数”决定了材质本身的内容（外观），如果没有参数，材质就用默认值渲染。 图示理解 # 影响对象 现实例子 Name（节点名、路径） /mat/test_mat 这个路径 Parameters（参数值） Base Color = 红色、Roughness=0.2 Material 节点分配时是通过 路径/名字 找到材质节点。 节点的参数决定材质的最终效果。 结论 # 在 OBJ 层 material 节点分配材质时，\n决定绑定是否成功的是“Name”（节点名字/路径）；\n而“Parameters”决定最终材质外观。\n方法2：使用Python脚本批量替换 # # 在Python Shell中运行 import hou # 获取当前材质网络 matnet = hou.pwd() # 确保在material network中 # 找到所有principled节点 principled_nodes = [n for n in matnet.children() if n.type().name() == \u0026#34;principled\u0026#34;] for node in principled_nodes: # 获取节点位置和连接 pos = node.position() # 创建新的mtlx节点 new_node = matnet.createNode(\u0026#34;mtlxstandard_surface\u0026#34;) new_node.setPosition(pos) new_node.setName(node.name()) # 复制相似参数（需要手动映射） # base_color = node.parm(\u0026#34;basecolor\u0026#34;).eval() # new_node.parm(\u0026#34;base_color\u0026#34;).set(base_color) # 复制连接 # ... 需要详细的连接复制代码 # 删除旧节点 node.destroy() 方法3：创建HDA (资产) # 创建一个包含mtlxstandard_surface的HDA，然后批量替换引用。\n最实用的方法 # 方法1是最简单的，Houdini会自动尝试参数映射。但是可能并不会替换完全。比如如下从principal切换为mtlx类型的材质出现报错：\n这里其实是参数引用错误，说明从principled shader转换到mtlxstandard_surface后，参数名称不匹配，换句话说，也可以认为是Channels出现了不匹配的情况。\n方法1：清理无效引用 # 选中mtlxstandard_surface节点 右键 → Channels and Keyframes → Delete Channels 选择删除所有无效的参数引用 方法2：手动修复参数映射 # 主要的参数名称对应关系：\n複製\nprincipled → mtlxstandard_surface metallic → metalness reflectint → specular (需要调整数值) baseBumpAndNormal_* → normal相关参数 dispInput_* → displacement相关参数 方法3：重新创建材质(推荐) # 由于转换后问题较多，建议：\n创建新的mtlxstandard_surface节点 手动重新连接贴图 重新设置参数值 删除转换后有问题的节点 方法4：使用参数表达式修复 # 在有问题的参数中，将引用改为正确的参数名：\nmetallic → metalness reflectint → specular ","date":"28 January 2025","externalUrl":null,"permalink":"/posts/houdini%E4%BB%8Emantra%E5%88%87%E6%8D%A2%E4%B8%BAkarma%E7%9A%84%E6%9D%90%E8%B4%A8%E5%B7%A5%E4%BD%9C%E6%B5%81%E5%88%87%E6%8D%A2/","section":"","summary":"VEX编程技巧","title":"Houdini：从Mantra切换为Karma的材质工作流切换","type":"posts"},{"content":"当然可以！帧缓冲（Framebuffer）是现代图形渲染管线中的核心概念之一。下面我从渲染管线的结构、帧缓冲的作用和所在阶段来系统讲解。\n1. 什么是帧缓冲（Framebuffer）？ # 帧缓冲本质上是一块显存中的内存区域，用于存储最终要显示在屏幕上的像素数据。注意是最终。 它通常包含颜色缓冲区（Color Buffer）、深度缓冲区（Depth Buffer）、有时还包括模板缓冲区（Stencil Buffer）。 2. 帧缓冲在渲染管线中的位置 # A. 图形渲染管线主要阶段 # 应用阶段 场景管理、逻辑、摄像机、渲染命令的下达 几何处理阶段 顶点着色（Vertex Shader）、投影、变换、裁剪 光栅化阶段 将三角形转为像素（Fragment/Pixel） 片元处理阶段 片元着色（Fragment/Pixel Shader），决定每个像素的颜色和深度 输出合并阶段（Output Merger） 将片元着色器输出的颜色、深度等数据，写入帧缓冲 显示（Scan-out） 将帧缓冲内容传给显示器 帧缓冲正好处在：\n片元着色器输出和最终显示之间。 是所有像素“最终落地”的地方。 3. 帧缓冲的结构 # 颜色缓冲区：存储每个像素的最终颜色（通常是RGBA，8bit/10bit/16bit/float等）。 深度缓冲区：存储每个像素的深度值（用于遮挡判断）。 模板缓冲区：用于高级像素操作（如遮罩、分区渲染等）。 多重帧缓冲（MRT）：现代引擎还支持一次渲染输出到多个帧缓冲（G-buffer、延迟渲染等）。\n4. 帧缓冲的作用 # 存储最终像素结果：所有片元着色器输出的颜色/深度/模板值都写入帧缓冲。 后处理/屏幕特效的输入：如Bloom、色调映射、抗锯齿等后期处理都以帧缓冲为输入。 多阶段渲染的中转站：比如先把一帧渲染到离屏帧缓冲（Render Texture），再做特效，最后再输出到主帧缓冲/屏幕。 5. 帧缓冲的“生命周期” # 每一帧渲染开始时，帧缓冲会清空/重置（clear）。 场景中所有可见物体经过渲染管线，最终像素值被写入帧缓冲。 一帧渲染完毕，帧缓冲的内容被扫描输出到显示器，成为你看到的画面。 6. 帧缓冲和色彩空间/色深的关系 # 帧缓冲的格式决定它能存储的色深（8bit/10bit/float等），影响HDR/色彩精度。 后处理通常直接操作帧缓冲的浮点数值，实现色彩调整、色调映射等。 7. 一句话总结 # 帧缓冲是渲染管线中负责存储最终像素颜色、深度等信息的内存区域，处于片元着色器输出和画面显示之间，是所有像素“最终汇合与输出”的关键节点。\n可视化流程（简化） # [顶点数据] ↓ [顶点着色器] ↓ [光栅化/插值] ↓ [片元着色器] ↓ [帧缓冲（颜色、深度、模板）] ← 后处理等特效也在此基础上进行 ↓ [显示器] ","date":"27 January 2025","externalUrl":null,"permalink":"/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E5%B8%A7%E7%BC%93%E5%86%B2frame-buffer/","section":"","summary":"渲染技术解析","title":"什么是帧缓冲（Frame Buffer）？","type":"posts"},{"content":" 游戏里的渲染流水线 （渲染管线）分为几个阶段，不同的文献里可能有不同的分法，但都大同小异，大概可以分为应用阶段、几何阶段、光栅化 阶段。\n应用阶段 # 应用阶段的主要职责是输出渲染所需的几何信息（渲染图元，可以是点、线等）和渲染设置给GPU，并调用DrawCall指令开启渲染：\n几何阶段 # 几何阶段主要负责将三维的模型顶点坐标转换为屏幕空间的二维顶点坐标，输出各顶点的深度值等信息到光栅化阶段：\n![](ca7a1bf5.png)\r顶点着色器 中，顶点着色器输出的顶点数据输出后的数据将会马上被组装成图元（如三角形、线段等），我们也称之为图元组装阶段（当然也有一种观点认为，图元装配应该属于光栅化阶段的三角形设置部分）。把顶点从模型空间转换到裁剪空间的过程，这里使用MVP矩阵完成这个操作，完成后的顶点只是转换到了裁剪空间 (Cliip Space)里，还不是屏幕空间(Screen Space)，这一步只是为后面的裁剪步骤做准备。\n顶点转换到裁剪坐标系(Clip Space)后，在屏幕映射步骤里会由硬件做透视除法（齐次除法 ），最后得到归一化的设备坐标（Normalized Device Coordinates， NDC），这个坐标是归一化的，xy 的取值范围是 -11，OpenGL中 z的取值范围应该是 -11 ，DirectX 中 z 的取值范围是 0~1。\nNDC空间坐标最后会进行映射运算得到二维的电脑屏幕的坐标。要注意：OpenGL的屏幕坐标原点左下角：\n而DirectX的屏幕坐标原点是左上角：\nUnity 选择了OpenGL 的规范，微软的许多窗口都是用左上角为原点的坐标系统，因为这样和我们的阅读方式：从左到右、从上到下的习惯是匹配的。\n光栅化阶段 # 光栅化步骤主要负责：每个渲染图元（点、线、面）中的哪些像素应该被绘制在屏幕上，也需要对上一个阶段得到的逐顶点数据进行插值赋值给每个像素，再逐像素处理光照。三角形在设置后会随后生成片元，通过遍历三角形覆盖了那些像素，随后会生成片元。fragment是光栅化阶段生成的候选像素，包含颜色、深度、纹理坐标等属性。在确定片元序列，后就会开始进行片元着色（fragment shader），并开始通过各种测试。\n![](bb80ff03.png)\r有关透明度测试和提前深度测试(Early Z) # 如果你在片元代码里调用了 clip 函数，那么就是开启了透明度测试，不过 clip 函数传递进参数的可以不是颜色的透明度，只要是一个 float 值就行。\n提前深度测试可以把被不透明物体遮挡的片元提前舍弃掉，提高渲染的效率，但我们都知道一个片元如果通过了深度测试，默认情况下他是会向深度缓冲区写入自己的深度的，试想一下：如果开启了透明度测试的物体 B 在另一个物体 A 之前，假设物体 B 先渲染，并且假设进行了提前深度测试：\n由于物体B第一个渲染，他发现深度缓冲里没有值，所以通过了提前深度测试并写入了自己的深度值，这导致后面的物体 A 在进行深度测试的时候，无法通过测试，抛弃了这部分的片元。如果物体B像一个正常的不透明物体那样渲染这没什么问题，但是问题是现在的物体B会进行透明度测试，如果物体B的一部分没有通过透明度测试，被裁剪，那么将会出现错误的现象：\n因此提前深度测试和透明度测试是互相冲突的，当你开启了透明度测试，也就意味着这个片元不能进行提前深度测试。由于提前深度测试对渲染性能有所提升，所以说透明度测试会导致渲染性能下降。\n屏幕后处理技术是指当这一帧的画面渲染完毕的时候，在输出到电脑屏幕之前，我们可以对这个画面进行一些特殊的处理，比如泛光效果等，值得注意的是虽然此时处理的图元是一个完整的长方形的二维的图画（可以理解为一个png）但是其中每个像素点都是记录了一些信息的，比如深度等。\nhttps://zhuanlan.zhihu.com/p/435326086 有关于渲染管线中涉及到的空间 # 一个物体从模型数据到最终屏幕像素，需要经过一系列空间变换。这些空间变换是3D渲染的基础，也是Shader开发常考的知识点。\n下面详细梳理一下完整的空间变换流程和各空间坐标的计算方式：\n1. 主要的空间坐标体系 # 按渲染流程顺序，一个物体的坐标会依次经过：\n模型空间（Model Space / Object Space） 物体自身的本地坐标系。 顶点坐标就是建模时定义的坐标，原点一般在物体中心或底部。 世界空间（World Space） 场景统一的坐标系。 所有物体都放在这个空间，原点和轴向一般由世界场景定义。 视图空间（View Space / Camera Space / Eye Space） 以摄像机为原点、摄像机方向为Z轴的空间。 所有物体会被“转换到以摄像机为中心的空间”。 裁剪空间（Clip Space） 经过投影变换后的坐标空间，已应用视锥体截取（Near/Far、FOV等）。 用于裁剪视锥外的顶点。 齐次裁剪空间（Homogeneous Clip Space） 裁剪空间坐标还没除以w。 用于进一步的几何处理和裁剪。 NDC空间（Normalized Device Coordinates） 齐次除法后得到的[-1,1]范围坐标，左下(-1,-1)，右上(1,1)。 用于光栅化。 屏幕空间（Screen Space / Pixel Space） 归一化坐标映射到屏幕像素坐标系。 (0,0)通常为左下角，(width-1, height-1)为右上角。 2. 各空间之间的变换矩阵 # 每一步坐标变换，都需要一个变换矩阵：\n空间 变换矩阵 作用 模型空间 → 世界空间 模型矩阵 (M) 物体在世界中的位置、旋转、缩放 世界空间 → 观察空间 视图矩阵 (V) 摄像机的变换（相机逆矩阵） 观察空间 → 裁剪空间 投影矩阵 (P) 透视投影/正交投影成视锥体 裁剪空间 → NDC 齐次除法（/w） x/w, y/w, z/w NDC → 屏幕空间 屏幕映射 视口变换（[-1,1]到[0,width/height]） 总变换公式 # 模型空间坐标 → 世界空间: p_world = M * p_model 世界空间 → 观察空间: p_view = V * p_world 观察空间 → 裁剪空间: p_clip = P * p_view 裁剪空间 → NDC: p_ndc = p_clip.xyz / p_clip.w NDC → 屏幕空间: p_screen = （p_ndc.xy + 1）* 0.5 * 屏幕尺寸 3. Unity中的矩阵名称说明 # unity_ObjectToWorld：模型到世界 unity_WorldToObject：世界到模型 UNITY_MATRIX_V：世界到观察 UNITY_MATRIX_VP：世界到裁剪（V * P） UNITY_MATRIX_P：观察到裁剪 UNITY_MATRIX_MVP：模型到裁剪（M * V * P） 4. Shader中的具体使用举例 # 顶点着色器常见流程 # // 顶点模型空间 float4 pos_model = v.vertex; // 变到世界空间 float4 pos_world = mul(unity_ObjectToWorld, pos_model); // 变到观察空间 float4 pos_view = mul(UNITY_MATRIX_V, pos_world); // 变到裁剪空间 float4 pos_clip = mul(UNITY_MATRIX_P, pos_view); // 或者直接一步到裁剪空间 float4 pos_clip = mul(UNITY_MATRIX_MVP, pos_model); // 输出到光栅化 o.pos = pos_clip; 5. 坐标空间的用途总结 # 模型空间：贴图、法线、动画蒙皮等 世界空间：灯光、阴影、物体交互 观察空间：视锥裁剪、摄像机对齐操作 裁剪/NDC空间：光栅化 屏幕空间：屏幕特效、后处理、UI 6. 一句话总结 # Unity渲染管线中，一个物体从模型空间，经过世界空间、观察空间、裁剪空间、NDC空间，最终被光栅化到屏幕空间。每一步都通过特定的变换矩阵进行转换。\n如果你想看更详细的Shader代码、矩阵推导、或各空间的可视化示例，欢迎继续提问！\n","date":"27 January 2025","externalUrl":null,"permalink":"/posts/%E5%9F%BA%E6%9C%AC%E6%B8%B2%E6%9F%93%E7%AE%A1%E7%BA%BF/","section":"","summary":"渲染技术解析","title":"基本渲染管线","type":"posts"},{"content":"在创建虚拟环境中的草地或者使用混凝土/路面的纹理时，我们常常面临的一个挑战是如何避免纹理重复带来的不真实感。为了解决这个问题可以有两种方法：一个快速的方法是基于沃罗诺伊噪声的旋转；二是利用砖块纹理进行分区旋转。\n首先，添加沃罗诺伊纹理节点来引入沃罗诺伊噪声。什么是沃洛诺伊噪声？沃罗诺伊纹理 - Blender 4.4 Manual 这种类型的噪声以其随机分布的点和区域而闻名，非常适合模拟自然界中的非规律性模式。这个噪波很适合打乱由于缩放造成的重复性，原理是在纹理坐标层面进行打乱，使重新映射过的坐标发生变化，进而造成“没有重复性”的假象。\n但是这样造成的边缘比较明显怎么办？有一个小技巧是在从物体的纹理坐标传递到沃罗洛伊噪声的时候再引入白噪声节点与纹理坐标进行颜色混合，将颜色混合的强度调低一些；白噪声可以让坐标边缘起到类似于模糊的效果。\n最后的节点图如下（只是对于坐标层面的改变所以很容易） 另一种方法是利用砖块纹理进行分区旋转。砖块纹理具有天然的分区特性，恰好blender/maya/C4D中都可以对每一个砖块进行随机化，非常适合用来划分对于不同草地的不同区域。这个步骤同样可以通过映射节点完成，这次的原理是使用了砖块纹理的输出来驱动旋转值的变化，而不是单纯的使用噪声打乱纹理坐标。优点是可以由于只是对坐标进行旋转处理，而不是用噪声打乱坐标，从而实现了更加真实的效果。\n【中字】Blender 材质教程：无缝贴图, PBR随机化无缝衔接 -修复初学者在着色方面的主要错误！_哔哩哔哩_bilibili ","date":"26 January 2025","externalUrl":null,"permalink":"/posts/blender%E5%B0%8F%E6%8A%80%E5%B7%A7%E7%94%A8%E5%99%AA%E5%A3%B0%E6%B6%88%E9%99%A4%E9%87%8D%E5%A4%8D%E7%BA%B9%E7%90%86/","section":"","summary":"问题解决方案","title":"Blender小技巧：用噪声消除重复纹理","type":"posts"},{"content":"","date":"26 January 2025","externalUrl":null,"permalink":"/tags/%E6%8A%80%E5%B7%A7/","section":"Tags","summary":"","title":"技巧","type":"tags"},{"content":" Unity中实现体积光的三种方式 # 体积光就是让光看起来“有体积”，像是穿过雾气的光柱，或者空气中漂浮着的尘埃被光线照亮的效果。它比普通的光照多了一种在空间中传播的感觉，让游戏画面更有氛围感。\n实现体积光的方式有很多，但总的来说可以分为三种：面片方法、屏幕空间方法和光线步进方法。这三种方法从简单到复杂各有优缺点。\n1. 面片方法/粒子特效方法 # 这里先讲面片的实现方法。面片方法很好理解，假设你拿着一根手电筒，手电筒打出了一束光柱。我们并不真的去模拟这些光线在空气中传播，而是直接在手电筒前面摆一个锥形的“透明罩子”，让它看上去像光柱一样。这个“罩子”就是一个模型（比如锥体或者圆柱体），通过材质的透明效果让它变得像一束光。\n实现思路： # 对于直接使用模型模拟的情况，可以创建一个几何体，比如锥体或者一个平面面片。把这个几何体放在光源位置，并让它的方向和光源的方向一致。给几何体加一个材质，材质可以通过渐变纹理让中央部分更亮，边缘更透明。为了更真实，可以再叠加一个噪声纹理，模拟空气中的尘埃。如果光源会动，比如手电筒或者舞台灯光，那么这个几何体也需要随着光源移动和旋转。\n还有一个方法比较Trick、就是利用Unity自带的粒子系统进行动态体积光的模拟,这个方法比上一个方法更加富有动态上的美感（但是需要配合Bloom效果，意味整个画面需要有所改变），适合作为一个附加项。\n优点和缺点 # 优点：\n简单，一个几何体加一个材质/使用粒子就能搞定。 性能开销很小，因为是基于物体的材质或者是引擎自带的粒子系统，非常适合性能敏感的场景，比如移动端游戏。 艺术表现力强，通过调整材质的渐变、纹理、透明度等参数，可以轻松制作出各种风格的光柱效果。 缺点：\n真实感有限，因为它只是一个几何体，并没有模拟光线与空气的真实交互。 遮挡问题比较明显。如果光柱穿过了场景中的墙壁或者其他物体，它不会被正确遮挡，看起来就很假。 靠近光柱时可能看起来很平面，缺乏立体感。 适用场景 # 面片方法特别适合那些对真实感要求不高的场景，比如舞台灯光、探照灯、或者卡通渲染风格的游戏。如果你的项目对性能要求比较高，这种方法并不是最优选择。\n参考 # https://www.youtube.com/watch?v=kbsd6askiCY\u0026amp;t=195s Using Particle System # 2. 屏幕空间方法：光柱和场景的互动 # 面片方法的问题在于，它不会考虑光柱和场景中的物体之间的遮挡关系。而屏幕空间方法解决了这个问题。它的核心思想是：我们不在三维空间中画光，而是在屏幕上“画光”。可以理解为，我们直接在屏幕上计算光柱的效果，而不是在场景中放一个几何体。\n实现思路 # 首先，把光源的位置投影到屏幕上，确定光柱在屏幕上的范围。从光源的屏幕位置出发，向外扩散光线。沿着这些光线的方向，我们对屏幕上的像素进行采样。通过深度缓冲（Depth Buffer），判断光线是否被遮挡。比如，如果光柱前面有一堵墙，通过深度值可以知道光柱应该被遮挡。计算完光柱的强度和遮挡效果后，把它作为一个后处理效果叠加到最终画面上。\n优点和缺点 # 优点：\n动态遮挡效果逼真。比如光柱被墙挡住后，遮挡关系是自动处理的，真实感更好。 可以处理多个光源的体积光效果，比如汽车大灯或者手电筒光柱。 性能开销可以调节，比如可以减少采样点来提升性能。 缺点：\n它只能处理屏幕上看得见的部分，屏幕外的光柱是无法计算。也就是说如果不出现在屏幕空间则没有体积光。 效果和分辨率密切相关，分辨率越高，计算量越大。 遮挡的精度受限于深度缓冲，如果场景比较复杂，可能会有一些不准确的地方。 适用场景 # 屏幕空间方法特别适合那些需要动态光柱效果的场景，比如手电筒、汽车大灯，或者光线穿过窗户洒进房间的画面。它对硬件的要求不算太高，适用于中高端设备。\n参考 # https://www.youtube.com/watch?v=vnBfbZeV928 3. 光线步进方法 # 上面两种方法都是“假的”体积光，而光线步进方法则是“真的”体积光。它直接在三维空间中模拟了光线在空气、雾气、烟尘中的传播过程。简单来说，它会从摄像机出发，沿着每一条视线（即屏幕上的每个像素）不断采样，计算光线在这些点上的散射和吸收。\n实现思路 # 从摄像机出发，沿视线方向，对每条光线进行步进采样（类似于光线追踪）。\n每个采样点都会计算光线的强度变化，比如步进一步之后光线在传播中因为散射而变弱，使用数组对当前光线进行记录。\n如果采样点被光源照亮，还要计算光线的直接照明效果。\n把所有采样点的结果累加起来，得到当前像素的体积光效果。\n最后，把体积光效果和场景颜色合成，生成最终画面。\n优点和缺点 # 优点：\n真实感爆表！它可以精确模拟光线在体积介质中的散射、吸收和遮挡。 支持动态雾气、烟尘等复杂效果，比如爆炸后的烟雾中透出的一束光。 灵活性极高，可以表现各种复杂的光照效果。 缺点：\n计算量巨大，每条光线都需要多次采样（比如说你的步长），对性能要求非常高。 实现难度较大，需要对体积渲染和光照模型有深入理解。 对硬件要求高，通常只适合高端设备和次世代游戏。 适用场景 # 光线步进方法是高质量渲染的终极选择，适合那些需要表现复杂雾气、烟尘或者动态光线传播的场景，比如次世代游戏、电影级别的渲染效果。如果你追求极致的真实感，并且硬件性能允许，那这就是你的最佳选择。\n方法 优点 缺点 适用场景 面片方法 简单高效，性能占用低 真实感不足，无法处理遮挡 卡通风格、移动端游戏 屏幕空间方法 动态效果好，可处理遮挡 受屏幕限制，分辨率依赖 手电筒、大灯等动态光柱 光线步进方法 真实感最高，支持复杂动态光线传播 性能开销大，硬件要求高 次世代游戏、电影级别渲染 总的来说，如果性能要求高，可以选择面片方法。如果需要动态效果并且硬件性能允许，屏幕空间方法会是一个很好的平衡。而如果你追求极致的真实感，那就选择光线步进方法。\n参考 # https://www.youtube.com/watch?v=0G8CVQZhMXw\u0026amp;t=199s 这采用了houdini生成3D纹理并使用Shadergraph实现Raymarching的效果\nhttps://www.youtube.com/watch?v=hXYOlXVRRL8\u0026amp;t=570s 上面的省流版\n","date":"24 January 2025","externalUrl":null,"permalink":"/posts/unity%E5%AE%9E%E7%8E%B0%E4%BD%93%E7%A7%AF%E5%85%89godray%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%A1%88/","section":"","summary":"体积光实现方案","title":"Unity实现体积光（GodRay）的三种方案","type":"posts"},{"content":" 引言 # 这是我偶然在Houdini的时候在导出的时候遇到的问题，Houdini为了更加可定制化的工作流直接采用了exr作为输出；但是exr文件在ps里面打开后的效果和达芬奇的效果完全不一致，恰好当时色彩空间也是我的心头之痛，而且几乎没有从图形学上解释色彩的文章，比较深的也只是讲到了色度图；因此打算写这篇文章。\n色彩的表示 # 我们都知道，在选择颜色的时候，不同的软件有不同的办法来调整颜色。如果我们把三原色R,G,B来表示为坐标系的三个数据，就可以得到科研中常用于色彩分析的三维空间：\n当然，这是计算机领域的表示法，艺术家们需要一个更加直观的表示方法，于是除了简单的RGB直接相加，另一种表示方法HSL应运而生，HSL模型可以更直观地反映了人类对颜色的感知方式，直接利用Hue(色相),Saturation(饱和度),Lightness(明度）表示。我们Ps中的选色盘和选色图都是基于这个标准。\n但其实HSL并不是最终发送给计算机的，计算机最后还是用RGB来进行色彩的表示。因为实际上RGB的数据结构只有(x,y,z)，即colorRGB = vec3(x,y,z)。也就是说，这个数值实际上只能直接表示Hue和Saturation，而Lightness则是用RGB进行一套公式转换出来的；在一些shader中，比如要进行亮度阈值计算，就会直接提取RGB来算出Lightness，再套亮度判断。\n另外HSL和RGB之间是可以无损转换的，虽然两者的表示方法不一致(HSL:0360°,0100%,0100%;RGB 8bit:0-255，如果都取int则360x101x101=3672360（360万色）和256x256x256=16777216 （1670万色），**亦可以用16进制表示:#000000#FFFFFF，一个FF表示为2的8次幂255**)，HSL 色轮的某些组合（特别是高亮度或低饱和度）会收敛到白色、灰色、黑色，导致一些 HSL 组合映射到相同的 RGB 值；但是理论上，在算法下只要表达的精度够高，两者之间仍然是可以进行色彩转换的。\n有一个理论是说HSL能够表达的色彩要少于RGB，这个说法属于是理所当然的认为HSL认为为整数/一位小数了，**我在色轮中给你看到一位小数 != 这个颜色的实际HSL值。**除此以外，HSL有一个特点就是因为引入了Lightness，那么Lightness约接近于0/1，那么颜色重合度越高（因为越接近于白色和黑色）\n因此HSL的色彩坐标图更加接近于双锥体，和上图的RGB三维坐标图作为对比：\nH（Hue，色相）：绕圆周旋转，360°，代表颜色种类。 S（Saturation，饱和度）：从中心（灰色）到边缘（最纯色）。 L（Lightness，亮度）：从底部（黑）到顶部（白），中间是最大纯度。 并且现代计算机为了进行图层叠加或者混合（也就是alpha test、alpha clipping等等和透明度有关的计算），会引入一个新的量Alpha，也会使用colorRGBA = vec4(x,y,z,a)，不过至于引入alpha之后色彩要如何叠加表示我就先不讲了，这里///又是一个大坑\u0026hellip;\n扩展：CIE色度图 # 实际上，rgb(255, 0, 0) 或者 #FF0000 这样的色值参数，仅仅只是告诉计算机显示设备：请你展示纯红色，不掺杂一点蓝也不掺杂一点绿的纯红色。此时，显示设备会尽自己的全力去显示一个 100% 饱和度的纯红色，但不同的显示设备，他们的展示效果是不同的，其中某些显示设备的红色看起来可能会比另外一些更鲜艳（比如手机屏幕看起来比电脑屏色彩艳丽）。这就带来了一个问题：RGB 色值的表现是基于显示设备的，不同的显示设备对 RGB 的表达能力有所不同，虽然他们接受到的色值参数是一致的，但呈现出来的颜色的视觉感受却并不相同，换言之就是他们并没有展示出相同的颜色，那到底哪个红色才是准确的呢？\n实际上这个问题早在 1931 年就得到了解决，为了定义一种与设备无关的颜色模型，让色彩能够脱离设备而准确定义，1931 年国际照明委员会（CIE）的色彩科学家们在 RGB 模型 基础上，通过数学的方法推导出了理论的 RGB 三基色，并以此创建了一个标准的色彩系统。而这一色彩系统的常见展示方式，就是 CIE 色度图（CIE chromaticity diagram）（如下图），我们之后所讨论的色彩空间往往都是基于这个色度图去展示的。\n为什么 CIE 色度图会长这个样子，一个看起来没什么规则的奇怪三角形，之前的圆形色盘不是更美观吗？事实上这个形状是通过数学计算得来的，之所以计算结果长这样，其中一个重要规则是 “感知均匀性 （perceptual uniformity）”，说人话就是这个形状更容易表现色彩之间的关系。还是没明白？没关系，我们举个例子:\n比如图一中，如果 A 点（绿色） 和 B 点（红色）两个点的颜色等比例混合，那么得到的新的颜色黄色一定在 A 点和 B 点连线的中点 E 点上，反观（图二）在圆形色盘（非感知均匀性图形）上去连接纯绿色 A 和纯红色 B 两个点，他们的中点 E 并不是纯黄色，纯黄色在圆周 F 点上；换言之，如果 A 点和 B 点两个颜色通过任意比例混合，那么得到的新的颜色也必然在 A B 两点的连线上。也就是说，（图三）R G B 三个点的颜色通过任意比例混合，他们所表现出来的新颜色也必然在这个三角范围内。\n这里要声明一下，除了几何上的追求，还有一个就是生物学上对于感知的特点：\nCIE 色度图（如 CIE 1931 xy 色度图，最常见的显示器色域可视化，除了1931还有很多版本，比如说1960UCS系列、1976系列、CIECAM系列）的边界是根据人眼对颜色的实际感知和混合结果、而不是简单的几何规则推算出来的。 它追求的是“感知均匀性”——即色彩空间的距离对应人眼感知的色差，空间中距离相等的两点，人的感知色差也尽量相等。 红色和绿色在这两个色彩空间中差异最大，也就是说如果采用蓝色调作为界面主色的话，在不同的色彩空间设备上差异会小一些，相反，采用红色和绿色，表现差异会比较大，越是饱和度高的红、绿色，差异越明显。 为什么都是红色，相同的数值（如P3，sRGB都是0~255的RGB表示），在CIE的坐标映射却不同？ # sRGB、P3等色彩空间的红绿蓝基色的CIE xy坐标，其实是由其标准定义直接给出的，来源于它们的物理“基色光”的CIE 1931 xy色度坐标。（直接去CIE委员会查坐标就行了） 还有P3和sRGB其实用的数据共通只是显示器协议\n换而言之，每个RGB色彩空间（比如sRGB、P3、AdobeRGB）都规定了其R、G、B三基色的CIE xyY坐标（通常Y=1，因为y代表亮度，这个已经不是我们色域考虑的范围），再加上白点（D65等），这样就能唯一确定该空间的全部色域。\n色彩的数据结构 # 色深（bit depth）在数据结构上如何体现，其实就是图像文件、帧缓冲、内存等如何“用多少比特来存每个像素/通道的颜色值”。\n1. 单个像素的数据结构 # A. 8bit色深的RGB图片（最常见） # 每个像素有3个通道（R、G、B），每个通道用8位（1字节）。 每个通道能表达0~255（256种）数值。 一个像素一共占3字节（24位）。 数据排列方式通常是交错（R, G, B, R, G, B\u0026hellip;）。 示例（像素值）：\nR G B 78 129 255 二进制数据：\n01001110 10000001 11111111 B. 10bit、12bit色深的RGB图片（高端显示/渲染/视频） # 每通道用10位或12位，能表示1024或4096级。 数据排列： 常见存储方式是16位对齐（即每通道用16位，但高位有效，低位补0）。 也有“打包存储”：比如30位图像（R10G10B10A2），4字节打包3个10bit加2bit alpha。 示例（10bit每通道）：\nR G B 278 800 1023 二进制数据（打包方式，举例）：\n000100010110 1100100000 1111111111 C. 16bit、32bit色深（高精度浮点） # 每通道用16位无符号整数（0~65535），或者32位float（如OpenEXR格式）。 用于HDR渲染、科学图像、电影特效等。 2. 整个图像的数据结构 # 以2x2像素RGB 8bit图片为例：\n(R0,G0,B0) (R1,G1,B1) (R2,G2,B2) (R3,G3,B3) 内存中数据排列（交错存储）：\nR0 G0 B0 R1 G1 B1 R2 G2 B2 R3 G3 B3 总共需要2×2×3=12字节。\n3. 色深在文件格式中的表现 # PNG：支持8bit和16bit色深（每通道）。 JPEG：大多数只支持8bit（有部分高端JPEG支持12bit）。 TIFF：支持8/16/32bit，甚至浮点数。 OpenEXR：支持16bit半精度浮点、32bit浮点、高端影视通用。 BMP：8/16/24/32bit，各种排列。 Raw（相机底片）：常见12bit、14bit、16bit，通常是单通道bayer阵列。 4. 显卡/显示器/操作系统的色深支持 # 帧缓冲（framebuffer）可以是24bit（8bit×3）、30bit（10bit×3）、36bit等。 HDMI、DisplayPort等接口支持10bit、12bit输出（取决于显示器和显卡）。 显示器也分8bit、10bit、12bit面板。 色深在数据结构上，就是每个颜色通道用多少位来存储。8bit每通道就是1字节，10bit/12bit通常用16位对齐，16bit/32bit直接用高精度数值。色深越高，单像素占用空间越大，能表达的颜色级数越多。\n色彩的传输 # 1. 基本流程概述 # 应用输出 RGB 数值（通常是 sRGB 空间，gamma 编码） 操作系统或图形驱动进行色彩空间转换（如 sRGB → P3/Rec.2020/显示器色彩空间） 进行 gamma/degamma 处理（线性化和显示端编码） 发送到显示器，显示器解读并显示实际的光学颜色 2. 详细流程分解 # A. 应用程序输出（一般为 sRGB） # 图片、网页、视频等，通常以 sRGB 编码的 RGB 数值输出。 这些数值是经过 gamma 编码的（非线性，适合存储和人的感知）。 B. 操作系统/色彩管理系统处理 # 1）degamma：sRGB → 线性空间（Linear RGB） # 首先，把 sRGB 的 gamma 编码 RGB 数值“解码”为线性光强度。 这一步叫degamma或gamma解码。 公式如下（sRGB为例）： 如果 C_srgb \u0026lt;= 0.04045: C_linear = C_srgb / 12.92 否则: C_linear = ((C_srgb + 0.055) / 1.055) ^ 2.4 其中 C_srgb ∈ [0,1]。\ngamma和degamma是描述 亮度信号与显示亮度之间非线性关系 的关键参数。它通过调整输入信号（如数字图像中的RGB值）与输出亮度之间的映射关系。\n但其实gamma设计的初衷是为了适配CRT显示器，CRT显示器的特性就是亮度不随电压的变化而线性变化。因此在传输到显示器之前还要做一遍gamma，也就是gamma2.2（sRGB标准）.反过来，还有gamma0.45（用于degamma恢复到线性响应）。因为现在的显示器已经可以基本做到线性的亮度显示，而现代操作系统仍然保持gamma工作流（即从主机端输出到显示器端仍然是被gamma后的色彩数据），因此显示器端要做一遍degamma。\n2）色彩空间变换（色域映射） # 把线性 sRGB 转换为显示器的色彩空间（比如显示器是 P3、Rec.709、Rec.2020、AdobeRGB等）。 这一步通过矩阵变换完成：\n线性sRGB → 线性显示器RGB 变换用的是不同空间的基色矩阵（色度坐标）和白点适配。 3）gamma 编码 # 线性显示器 RGB 需要“重新编码”（gamma），以适配显示器的电子特性和人眼感知。 比如普通 SDR 显示器用 sRGB gamma，HDR 显示器用 PQ（ST2084）、HLG等。 这一步叫gamma编码或EOTF（Electro-Optical Transfer Function）。 常见 gamma 编码（以 sRGB 为例）：\n如果 C_linear \u0026lt;= 0.0031308: C_display = C_linear * 12.92 否则: C_display = 1.055 * (C_linear ^ (1/2.4)) - 0.055 4）Dithering/Bit-depth 降位 # 有时候还会做抖动处理，以便更好地显示低位深色彩。 C. 信号传输到显示器 # 最终的 RGB 数值（已编码，适配显示器色域和 gamma）通过 HDMI/DP 等传给显示器。 显示器解读信号，发光显示。 3. 流程图简化 # [应用输出(sRGB, gamma编码)] | degamma，先统一转换为线性空间计算 （Windows因为软件支持不同，由系统收集应用显示后统一在系统层WCS/DWM转换） （iOS系列则在架构设计之初就会启用全局色彩管理，即：在构建应用时苹果就已经嵌入色彩转换引擎ColorSync） ↓ [framebuffer上所有出现的应用都将遍历以上两步，将所有色彩空间转为线性空间] | 色彩空间变换（如果有不同色域出现在屏幕上则在这一步进行统一的色彩重映射） ↓ 统一gamma压缩，将数据传输到显示器 | 显示器接收到gamma后的色彩 ↓ [显示器RGB, degamma编码] | 显示器显示 4. 实际例子 # 用 Photoshop 打开一张 sRGB 图片，在 P3 屏幕上查看： Photoshop 输出 sRGB (gamma编码) 操作系统 degamma → 线性sRGB 线性sRGB → 线性P3（色彩空间变换） gamma编码 → P3 gamma（或保持线性，视显示器而定） 输出到显示器，正确显示原色 5. 补充说明 # 色彩空间管理（ICC Profile）：操作系统和应用程序要正确读取、嵌入、转换色彩空间信息（profile），否则会色偏。 未管理流程：如果没有色彩管理（很多PC默认如此），sRGB数值直接发到P3或其他色域显示器，会导致“溢色”或“错误显示”。 6. 关键词解释 # gamma：一种将线性光强度非线性编码为接近人眼感知的数值的数学函数。 degamma：将非线性数值还原为线性光强度。 色彩空间变换：不同色域之间的RGB值变换。 EOTF/OETF：光电/电光转移函数，描述信号和实际亮度的关系。 操作系统的色彩管理流程，会先把应用输出的 gamma 编码 RGB 数值“degamma”成线性空间，再做色彩空间变换，最后重新 gamma 编码后输出到显示器，确保在不同色域和显示特性下，色彩能准确还原。\nTips：从主机到显示器会传输什么数据？ # 主机通过显卡（GPU）把帧缓冲区里的像素数据输出到显示器，主要包括：\n像素 RGB 数值流 每个像素的 RGB（或 YCbCr，也称为YUV我们的采集卡经常使用。Y（亮度）、Cb（蓝色色度）、Cr（红色色度））数值，通常是 8bit/10bit/12bit，色彩空间、gamma 编码方式由上下文决定。 信号格式 比如 HDMI、DisplayPort、DVI、USB-C Alt Mode 等。 协议规定了分辨率、刷新率、色深、色彩格式（RGB/YCbCr）、压缩方式等。 同步信号 时序信号（VSync、HSync、DE），确保图像正确显示。 部分元数据 比如 HDR 标记、色度子采样信息、颜色量化范围（全/限幅）等。 1. EDID（显示器能力描述） # EDID（Extended Display Identification Data）是显示器通过 I²C 通道（DDC/CI）向主机报告“我能支持什么”的标准协议。 包含： 支持的分辨率、刷新率、色深、色彩格式 支持的色域（如 sRGB、AdobeRGB、P3、BT.2020）、色度坐标 支持的 HDR 标准（如 HDR10、Dolby Vision）、EOTF（Gamma/PQ/HLG 等） 主机读取EDID流程 # 操作系统启动/显示设备热插拔时，显卡读取显示器 EDID 信息 操作系统/驱动据此列出分辨率、刷新率、色深、色域等选项 部分操作系统会自动根据 EDID 选择合适的色彩输出（如 macOS 优先输出 P3） 2. 色域握手的历史和现状 # 传统 SDR 环境（sRGB 时代）： 大多数显示器只支持 sRGB，EDID 并不强调色域 主机默认以 sRGB 方式输出，广色域显示器通常手动切换模式 现代广色域/高端显示器： EDID 里会声明 P3、AdobeRGB、Rec.2020 等支持情况 新版 HDMI/DP 协议和 VESA DisplayID 标准允许更细致的色度坐标、色域声明 主机可以据此自动优化输出 3. HDR 和高级色域的元数据 # 当启用 HDR 或 BT.2020 色域时，主机不仅通过 EDID 得知显示器能力，还会在视频流里插入 HDR 元数据（如 EOTF、色域主色点、白点等），显示器据此调整显示模式。 比如 Windows 11、macOS、部分 Linux 桌面环境都已支持 HDR 色域和 EOTF 自动协商。 4. 实际操作流程 # 主机读取显示器 EDID/DisplayID，了解其支持的分辨率、色深、色域等能力 主机/操作系统选择合适的输出配置： 比如输出 P3 10bit，或 sRGB 8bit，或 HDR BT.2020 10bit （可选）操作系统/专业软件做色彩空间变换，把内容适配到目标色域 主机编码好信号，通过 HDMI/DP 发送到显示器 显示器根据输入信号和自身设置，选择合适的显示模式（有的能自动切换色域/模式） 为什么Gamma如今还存在？ # 其实就是船大难调头了你不可能在已经成熟的市场重新定义一个完全线性的显示流\n以前Gamma的存在是因为CRT显示器的亮度并不随电压的变化而均匀变化；因此在传输视频信号给显示器的时候，主机端就要做一遍Gamma2.2（Linear to sRGB），使主机的输出Gamma视频能和CRT的显示劣势相抵消。\n虽然现代显示屏已经实现了将视频信号几乎以线性亮度显示出来，但是因此还要说到Gamma的另一个特性——暗部的低压缩。\n人眼的非线性感知 # 其实这是在使用Gamma抵消CRT显示器显示缺陷的副产物——Gamma矫正会使暗部的灰阶被拉伸，而亮部的灰阶会被压缩。\n但是人眼对亮度的感知是非线性的：在暗部区域，人眼对亮度变化的敏感度更高，而在亮部区域，亮度变化对人眼的影响会减弱。\nRGB的每个通道（红、绿、蓝）的取值范围为0到255，这表示每个颜色通道有256个灰阶（0到255共256个值）。如果用人眼的范围感知打比方则有：\n暗部敏感：在暗部区域，亮度的微小变化（如从1到2）会显著影响视觉感知。 亮部迟钝：在亮部区域，亮度的大幅变化（如从200到255）对视觉感知的影响较小。 Gamma校正的补偿作用：\n通过非线性映射，Gamma校正将更多灰阶分配到人眼敏感的暗部区域。例如，将256个灰阶中的一部分（如前100个灰阶）用于暗部，而仅用少量灰阶（如后50个灰阶）覆盖亮部。这样，在有限的256个灰阶中，暗部能呈现更丰富的细节。 用数学方式来表达Gamma的灰阶拉伸作用的话，有：\n假设Gamma值为2.2（常见标准），输入线性亮度值为 ，输出Gamma编码值为 ：\n暗部：\n假设线性亮度 （10%亮度），则Gamma编码值为：\n在Gamma空间中，0.1的亮度被映射到像素值91，而非线性空间中的0.1亮度对应像素值25.5（25.5 × 0.1 = 2.55）。显然，Gamma空间中暗部的灰阶被拉伸了。\n亮部：\n线性亮度 （90%亮度），Gamma编码值为：\n在Gamma空间中，0.9的亮度仅映射到像素值242，而非线性空间中的0.9亮度对应像素值229.5。亮部的灰阶被压缩了。\n带宽的压缩 # 我们知道通过非线性映射（如Gamma值2.2），将暗部灰阶拉伸，亮部灰阶压缩(我们用sRGB 8bit来做比方，即灰阶由RGB加权后。但这个压缩的gamma具体的色阶分配是多少？又压缩了多少？\n假设图像为8位通道（256级灰阶），亮度范围为0-1（0=黑，1=白）：\n亮度区间 未使用Gamma校正（线性） 使用Gamma校正（Gamma=2.2） 暗部（0-0.2） 占256级中的51级（0-51） 占256级中的约150级（0-150） 中灰（0.2-0.5） 占256级中的76级（52-127） 占256级中的约80级（151-230） 亮部（0.5-1.0） 占256级中的129级（128-255） 占256级中的约26级（231-256） 而实际上我们日常的显示几乎是高灰阶为主的，比如我们平时的浏览器；人们对高灰阶的感知并不明显，经过gamma压缩后高灰阶的数据量也被减少了，这也就变相减少了主机到显示器的数据传输量。\n其实这个做法也和摄影圈“向右曝光”的思路一致：因为人眼对于暗部细节更加敏感，所以向右曝光至少不会产生欠曝（因为曝光过低导致暗部拉不回来的叫法），保留暗部细节\u0026gt;保留亮部细节。这句话用在Gamma的作用也同样适用。\nICC配置文件——文件的自身色彩声明 # 如果我们在打开一个图像文件的时候，在查看元数据的时候可以看到这一行：\nphotoshop:ICCProfile sRGB IEC61966-2.1 (线性 RGB 配置文件)\u0026lt;/photoshop:ICCProfile\u0026gt;\n这就是嵌入到文件中的ICC Profile，用于告诉系统要如何处理这个色域/degamma的数值。\n其实一般情况下ICC配置文件并没有什么用，因为大家都是约定俗成的使用sRGB，包括所有网站的图像压缩都会自动转换成为sRGB格式，有用的情况是在高色域显示器进行创作编辑时可以直接读取并且直接渲染到屏幕上。\nICC数据结构 # 当一个图像文件嵌入了 ICC Profile（ICC配置文件） 时，该文件中会包含以下关键数据，用于描述图像的色彩特性并确保跨设备的颜色一致性：\n1. 设备描述文本（Device Description） # 内容： 设备类型：说明该ICC配置文件是为哪种设备设计的（如显示器、打印机、扫描仪等）。 制造商信息：记录配置文件的创建者或设备制造商。 创建时间：配置文件生成的时间戳。 校准条件：例如显示器的白点色温（如D65）、亮度、环境光条件等。 作用：帮助用户和软件识别配置文件的来源及适用性。 2. 颜色空间与色域信息 # 内容： 色彩空间：描述图像数据所基于的色彩空间（如 sRGB、Adobe RGB、ProPhoto RGB 等）。 色域（Color Gamut）：通过 CIE XYZ 或 Lab 坐标表示设备或图像能再现的颜色范围。 白点（White Point）：定义设备的白色参考值（如 D65 表示色温 6500K）。 Gamma 曲线：描述设备对亮度的响应关系（如 sRGB 的 Gamma=2.2）。 作用：确保图像在不同设备上以相同的色彩空间和色域呈现。 3. 颜色转换规则 # 内容： 设备到 PCS 的转换：将设备色彩空间（如 RGB）转换为标准色彩空间（如 CIE XYZ 或 Lab）。 PCS 到设备的转换：将标准色彩空间转换回目标设备色彩空间（如 CMYK）。 颜色转换表（LUT）：通过表格形式存储输入值到输出值的映射关系（如 3D LUT 或 1D LUT）。 作用：实现图像颜色在不同设备间的精确转换。 4. 渲染意图（Rendering Intent） # 内容： Perceptual（感知意图）：保持颜色之间的视觉关系，适合图像和照片。 Relative Colorimetric（相对色度意图）：保留可再现的颜色，超出目标色域的颜色裁剪或替换。 Absolute Colorimetric（绝对色度意图）：严格匹配白点，用于特定行业（如医疗影像）。 Saturation（饱和度意图）：优先保持颜色的鲜艳度，适合图表和图形。 作用：定义颜色转换时的策略，处理色域不匹配的问题。 5. 校准数据 # 内容： 校准测量值：通过专业校色工具（如 Datacolor Spyder）测量设备的色彩特性后生成。 3D LUT（查找表）：高级配置文件可能包含 3D LUT，直接定义输入 RGB 值到输出 RGB 值的映射关系。 作用：确保图像颜色在设备上准确再现（如显示器校准后的颜色校正）。 6. 特定设备信息 # 内容： 输入设备：如扫描仪或数码相机的色彩捕捉能力（如色深、动态范围）。 输出设备：如打印机的墨水特性（如光泽纸与哑光纸的色差）。 显示设备：如显示器的亮度、对比度、色域覆盖范围。 作用：描述设备的物理特性，确保颜色在设备间传递的一致性。 7. 标准化格式与兼容性 # 内容： ICC 标准格式：遵循 ICC 于 1995 年制定的规范（ICC.1:2001-12）。 文件扩展名：通常为 .icc 或 .icm（Windows）。 作用：确保跨平台（Windows、macOS）和跨设备（RGB、CMYK、扫描仪等）的通用性。 8. 示例：嵌入的 ICC Profile 数据 # 假设一个图像文件嵌入了 sRGB IEC61966-2.1 配置文件：\n色域：sRGB 标准色域（覆盖约 35% 的 NTSC 色域）。 白点：D65（6500K 色温）。 Gamma 曲线：Gamma=2.2。 用途：广泛用于网页、电子设备，默认支持所有浏览器和操作系统。 9. 嵌入 ICC Profile 的实际作用 # 确保颜色一致性：嵌入的 ICC Profile 会告诉软件（如 Photoshop、浏览器）图像数据基于哪种色彩空间，从而在不同设备上正确解释颜色。 避免色差：例如，若图像基于 Adobe RGB 色域，而显示器仅支持 sRGB，嵌入的 ICC Profile 会提示软件进行色域转换，避免颜色失真。 支持专业工作流：摄影师、设计师通过嵌入 ICC Profile，确保图像从拍摄到打印的全链路颜色一致。 10. 常见嵌入场景 # 文件类型 是否支持嵌入 ICC Profile 典型用途 PSD ✅ 专业设计、摄影后期 TIFF ✅ 高精度图像存储 JPEG ✅（部分压缩可能丢失数据） 网络图片、社交媒体 PNG ✅ 无损压缩图像 总结 # 嵌入的 ICC Profile 包含了 设备色彩特性描述、颜色转换规则、渲染意图、校准数据 等核心信息。它通过标准化格式（如 .icc）确保图像在不同设备（显示器、打印机、扫描仪）之间传递时，颜色表现一致，是专业设计、摄影和印刷领域不可或缺的工具。\nWindows屏幕的多软件色彩管理 # 在Windows，由于没有苹果的强制标准，~~所有人都可以插上一脚 ~~不同软件对于色彩理解有不同的显示方法。这就要聊到我们高贵的渲染管线了\n一般来说，Framebuffer（储存于显存）desktop的一般是24位RGB+8位Alpha通道，合计32位深度，也就是说我们传统的8bit。当然可以自适应改善成为40位（4x10bit）、48位（4x12bit）。\n但是在Windows中，CPU在提交渲染信息给GPU之前，WDDM会查询所有在屏幕中的软件渲染API是否有声明这个软件的色彩空间。具体也就是在DirectX中写出：\nDXGI_COLOR_SPACE_TYPE colorSpace = DXGI_COLOR_SPACE_RGB_FULL_G22_NONE_P709; // sRGB swapChain-\u0026gt;SetColorSpace1(colorSpace); 支持的色彩空间类型： DXGI_COLOR_SPACE_RGB_FULL_G22_NONE_P709（sRGB） DXGI_COLOR_SPACE_RGB_STUDIO_G22_NONE_P709（Adobe RGB） DXGI_COLOR_SPACE_RGB_FULL_G10_NONE_P709（DCI-P3） DXGI_COLOR_SPACE_YCBCR709（BT.709） DXGI_COLOR_SPACE_YCBCR601（BT.601） DXGI_COLOR_SPACE_RGB_FULL_G2084_NONE_P2020（HDR10 / BT.2020） DWM 的处理流程： 应用声明色彩空间：通过 SetColorSpace1 告知 DWM 输出的色彩空间（如 HDR10）。 DWM 合成阶段： 将应用输出的色彩空间转换为 CCCS（规范合成色彩空间）（scRGB + FP16 线性伽马）。 再将 CCCS 转换为显示器的物理色彩空间（如 BT.2020 或 ST.2084）。 最终输出：通过 GPU 显式控制显示器的色彩输出，确保颜色准确性。 (2) 实际效果 # HDR 应用：如果应用声明为 HDR10（DXGI_COLOR_SPACE_RGB_FULL_G2084_NONE_P2020），DWM 会启用 HDR 模式，使用 ST.2084 EOTF 进行转换。 SDR 应用：如果应用声明为 sRGB，DWM 会使用 sRGB 伽马（Gamma=2.2）进行合成。 在应用内实现色彩空间转换（Bypass DWM） # 如果希望完全绕过 DWM 的色彩管理，应用可以在自身内部实现完整的色彩空间转换逻辑，并直接输出到显示器。\n全屏独占模式（Full-Screen Exclusive Mode） # 原理： 应用通过 DirectX 的 全屏独占模式 直接控制显示器，绕过 DWM 的合成阶段。 此模式下，应用可以完全控制帧缓冲区的色彩空间和输出格式。 这对吗？ # 实则不然。这个说法在历史上部分正确，但现在已经基本过时了。\n传统全屏模式（Exclusive Fullscreen）的确实情况：\n在Windows 7/8时代，独占全屏模式确实可以绕过很多系统层面的处理：\n// 传统的独占全屏 DXGI_SWAP_CHAIN_DESC desc = {}; desc.Windowed = FALSE; // 独占全屏 desc.SwapEffect = DXGI_SWAP_EFFECT_DISCARD; 此时应用直接控制显示器，DWM被绕过，色彩管理也相对简单。\n现代情况的变化：\n从Windows 10开始，为了能支持未来的HDR显示器，并且适配新的Dither功能等新技术（比如我们常说的显示器8抖10，8bit用dithering升格到10bit），情况发生了重大变化：\nFullscreen Optimizations: 即使是\u0026quot;全屏\u0026quot;应用，系统也可能将其作为全屏窗口运行，仍然通过DWM处理 DWM色彩管理增强: 现代DWM有更完善的色彩管理，即使在全屏模式下也会参与色彩处理 HDR和Wide Color支持: 系统需要在全屏模式下也能正确处理HDR/WCG内容 专业应用的实际做法：\n现代专业软件更依赖于：\n正确的DirectX DXGI色彩空间声明 与系统色彩管理的协作而非对抗 硬件级的校色bypass（通过显卡驱动扩展） 所以简单的\u0026quot;开全屏就能bypass色彩管理\u0026quot;这种说法现在并不可靠，特别是对于需要精确色彩的专业应用。\n扩展：HDR # 特性 SDR HDR 峰值亮度 100-400 nits 400-10,000+ nits 位深度 8位 (偶尔10位) 10位/12位 色彩空间 Rec.709 (sRGB) Rec.2020 (DCI-P3) 传递函数 Gamma 2.4 PQ (SMPTE ST 2084) / HLG 动态范围 ~6-7档光圈 10-14档光圈 对比度 1,000:1 - 5,000:1 100,000:1 - 1,000,000:1 编码方式 相对编码 PQ:绝对编码 / HLG:相对编码 显示器握手 无需握手，手动调节亮度 需要EDID信息交换 色调映射 不需要 需要Tone mapping 向后兼容 N/A HDR显示器可显示SDR内容(HLG编码) 文件大小 相对较小 更大 (更高位深度) 制作成本 较低 较高 (需要HDR监视器等) 主要标准 Rec.709，约等于sRGBAdobeRGB, Display-P3(iOS) Rec.2020,DCI-P3(色域标准)HDR10, HDR10+, Dolby Vision 广播应用 传统电视广播 新一代广播 (HLG主导) 流媒体 通用支持 Netflix, YouTube等支持 游戏支持 全平台支持 PS5, Xbox Series, PC支持 显示设备要求 普通显示器 支持HDR的显示器/电视 关键差异总结：\nHDR最大优势是亮度范围和色彩表现力 SDR更成熟，兼容性更好 HDR需要完整的端到端支持链条 两者可以共存，HDR设备通常向下兼容SDR 除了HDR和SDR在数据结构上还有几个关键区别：\n位深度差异：SDR通常使用8位或10位编码，而HDR一般需要10位或12位来表示更大的动态范围。更高的位深度允许HDR存储更多的亮度级别，避免在高动态范围下出现色带问题。\n色彩空间：SDR主要使用Rec.709色彩空间，而HDR使用更广的色彩空间如Rec.2020，能够表示更丰富的颜色。\n传递函数：这是最核心的区别。SDR使用Gamma 2.4编码，而HDR使用PQ（SMPTE ST 2084）或HLG（Hybrid Log-Gamma）编码。\n关于编码实现：\nGamma编码：SDR的Gamma 2.4是一个简单的幂函数，将线性光信号压缩到0-1范围。公式大致是 V = L^(1/2.4)，其中L是线性亮度值。\nPQ编码：PQ是感知量化器，专门为人眼视觉系统设计。它使用复杂的数学函数将0到10000 nits的亮度范围映射到0-1的编码值。PQ曲线基于人眼对不同亮度的敏感度，在暗部分配更多编码值，亮部相对较少。\nHLG编码：HLG结合了传统Gamma和对数编码的优势。在低亮度区域使用类似Gamma的平方根函数，在高亮度区域切换到对数编码。这种混合方式既保持了与SDR的向后兼容性，又能处理高动态范围内容。\nPQ是绝对编码，每个编码值对应固定的物理亮度；而HLG是相对编码，需要根据显示设备的峰值亮度来解释编码值。这使得HLG在广播应用中更加实用。\n扩展：LutsLUT（Look-Up Table）完全解析：从原理到GPU实现 # 什么是LUT？ # LUT（Look-Up Table，查找表）是色彩处理中的核心工具，本质上是一个三维色彩映射表。它将输入的RGB值直接映射到输出的RGB值，实现快速而精确的色彩变换。\n基本流程：输入RGB → LUT查表 → 输出RGB LUT的数据结构 # 以常见的32×32×32 LUT为例：\n逻辑结构 # 维度：R、G、B三个维度，每维32个采样点 总容量：32 × 32 × 32 = 32,768个预设映射点 数据内容：每个格子存储 输入(R,G,B) → 输出(R\u0026rsquo;,G\u0026rsquo;,B\u0026rsquo;) 的映射关系 文件大小：通常几百KB 工作原理 # 输入处理：8位输入(0-255) → 缩放到LUT坐标(0-31) 例如：RGB(128,64,192) → LUT坐标(16,8,24) 查表映射：LUT[16][8][24] = (新R值, 新G值, 新B值) GPU中的实现 # 虽然逻辑上是三维结构，但在GPU中有更巧妙的实现方式：\n存储方式：2D纹理数组 # 32×32×32的LUT实际存储为32张32×32的2D纹理：\nBlue=0 → 第1张32×32纹理 Blue=1 → 第2张32×32纹理 ... Blue=31 → 第32张32×32纹理 查找过程 # 第一步：B值确定纹理层\n输入RGB(R=128, G=64, B=192) B值192 → 缩放到0-31 → B_index = 24 选择第24层纹理 第二步：RG值作为UV坐标\nR值128 → 缩放到0-1 → U = 0.502 G值64 → 缩放到0-1 → V = 0.251 在第24层纹理的UV(0.502, 0.251)位置采样 第三步：三线性插值\n为确保平滑过渡，需要在相邻B层间插值：\n// GLSL着色器实现 vec3 applyLUT(vec3 inputColor) { float b_index = inputColor.b * 31.0; int b_floor = int(b_index); int b_ceil = b_floor + 1; float b_weight = fract(b_index); vec2 uv = inputColor.rg; vec3 color1 = texture(lutTexture, vec3(uv, b_floor)).rgb; vec3 color2 = texture(lutTexture, vec3(uv, b_ceil)).rgb; return mix(color1, color2, b_weight); } 当然，三线性插值同样可以在R和G（即UV采样中）实现。\nLUT的实际应用 # 电影调色 # 原始素材：平淡的原始RGB值 LUT处理：增强对比度、调整色温、营造氛围 结果：具有电影级视觉效果的画面 HDR到SDR的色调映射 # 输入：HDR (R=1024, G=512, B=256) [超出SDR范围] LUT处理：智能压缩到SDR范围，保持视觉层次 输出：(R=240, G=180, B=120) [适合SDR显示] 色彩空间转换 # 将不同色彩标准间的复杂转换预计算并存储在LUT中，实现实时转换。\nLUT的优势与限制 # 优势 # 处理速度快 - 直接查表，无需实时复杂计算 精确度高 - 可以实现任意复杂的非线性调整 结果一致 - 同样输入永远得到相同输出 硬件友好 - GPU纹理采样天然适合LUT查找 易于分发 - 调色师可以将复杂的调色方案打包成LUT文件 限制 # 精度限制 - 32×32×32只有32,768个采样点，可能产生色彩断层。但是可以进行改进，或者使用双线性/三线性插值算法 文件大小 - 更高精度的LUT（如64×64×64）占用更多存储空间 单向映射 - LUT变换通常不可逆 内存占用 - 需要将整个LUT作为纹理加载到显存中 扩展：色度取样/色度子采样（Chroma Subsampling）技术 # YUV色彩空间基础 # 首先需要理解视频通常使用YUV色彩空间而非RGB：\nY：亮度信息（Luma），人眼最敏感 U/V：色度信息（Chroma），人眼相对不敏感 为什么不用RGB？这和视频格式的bitrate有关。详见VBR/CBR技术。听说现在Av1格式用到了更强的 子采样格式解析 # 4:4:4（无子采样） # Y Y Y Y U U U U V V V V 完整保留所有像素的色度信息 文件大小：最大，质量最好 4:2:2（水平子采样） # Y Y Y Y U U V V 水平方向每2个像素共享1个色度样本 文件大小：比4:4:4小33% 4:2:0（水平+垂直子采样） # Y Y Y Y U V Y Y Y Y 每4个像素（2×2块）共享1个色度样本 文件大小：比4:4:4小50% 实际应用场景 # 格式 应用场景 文件大小 质量 4:4:4 专业后期制作、无损存储 最大 最高 4:2:2 专业摄像、直播推流 中等 高 4:2:0 消费级录制、网络视频 最小 良好 为什么这样做？ # 人眼视觉特性 # 人眼对亮度变化非常敏感 对色彩变化相对不敏感 利用这个特性可以在几乎不影响观感的情况下大幅压缩 实际效果对比 # 4K视频1小时存储空间： 4:4:4 → ~200GB 4:2:2 → ~130GB 4:2:0 → ~100GB 各厂商的支持情况 # 手机厂商 # iPhone 14 Pro：支持4:2:2 ProRes录制 三星S23 Ultra：支持4:2:0录制（消费级） 索尼Xperia：专业模式支持4:2:2 专业设备 # RED相机：支持4:4:4 RAW BlackMagic：4:2:2 ProRes为主 佳能/索尼专业摄像机：4:2:2标准 选择建议 # 选择4:2:0：\n日常录制、社交分享 存储空间有限 网络传输需求 选择4:2:2：\n专业视频制作 需要后期调色 绿幕抠像等专业需求 选择4:4:4：\n无损存储要求 极致后期处理 存储空间充足 总结：这些数字直接关系到视频质量和文件大小的平衡。厂商提供不同选项让用户根据需求选择最适合的录制格式。\nRef： # 在高/标准动态范围显示器上将 DirectX 与高级颜色配合使用 - Win32 apps 什么是 Windows 中的 HDR？ - Microsoft 支持 在 Windows 中更改屏幕亮度和颜色 - Microsoft 支持 D-Gamut白皮书\nhttps://dl.djicdn.com/downloads/zenmuse+x7/20171010/D-Log_D-Gamut_Whitepaper.pdf Nikon N-Log技术规格解读\nhttps://zhuanlan.zhihu.com/p/492611718 Windows的色彩管理问题\nhttps://www.zhihu.com/question/416078246 Gamma和Linear空间\nhttps://zhuanlan.zhihu.com/p/492870542 What is NaN?\nhttps://zhuanlan.zhihu.com/p/655239364 如何理解色度取样？\nhttps://zhuanlan.zhihu.com/p/41530371 - ","date":"23 January 2025","externalUrl":null,"permalink":"/posts/%E8%89%B2%E5%BD%A9%E7%A9%BA%E9%97%B4/","section":"","summary":"算法原理解析","title":"色彩空间","type":"posts"},{"content":"屏幕空间反射 - Unity 手册 平面反射探针 (Planar Reflection Probe) | High Definition RP | 10.4.0 平面反射探针仅限于HDRP？这么离谱\nJust a moment\u0026hellip; ","date":"9 January 2025","externalUrl":null,"permalink":"/posts/%E5%8F%8D%E5%B0%84%E7%AE%97%E6%B3%95/","section":"","summary":"Blender导入Unity指南","title":"反射算法","type":"posts"},{"content":"最近在水qq群的时候遇到了一个问题：无厚度的披风zfighting问题。\n好的，我们来分析一下这个关于Unity中双面材质披风Z-fighting（Z冲突，也常被称作深度冲突）的问题。\n对话中的核心问题：\n为什么一个两面颜色不同的披风，Cull Back (背面剔除) 可以缓解Z-fighting？ Z-fighting不是精度问题吗？怎么能用剔除来解决？ 为什么有些角色两边颜色不同的披风就不会出现这种情况？ 让我们一步步来解释。\n理解Z-fighting # 首先，我们要明白什么是Z-fighting。当两个或多个多边形（模型面片）在屏幕上的同一像素位置，并且它们的深度值（Z值，表示离摄像机的远近）非常接近甚至相同时，在光栅化阶段的ztest时，显卡就难以通过深度图来判断哪个面应该显示在前面。这会导致这些面片在该像素上交替闪烁，或者出现“缝合”一样的破碎效果，尤其在摄像机或物体移动时更为明显。这就是Z-fighting。 具体参见基本渲染管线：ztest发生在光栅化的片元着色后。\n基本渲染管线 主要原因有两个：\n深度缓冲精度有限：深度缓冲区（Z-buffer）用来存储每个像素的深度值，它的精度是有限的（比如16位、24位、32位浮点数）。当物体离摄像机非常远，或者摄像机的近裁剪面（Near Clip Plane）和远裁剪面（Far Clip Plane）设置的范围过大时，深度值的精度会下降，导致原本不同深度的表面被量化到相同的深度值。 共面或几乎共面：如果两个面片在几何上就是重叠的，或者非常非常接近，那么它们的深度值自然也会非常接近，容易引发Z-fighting。 为什么Cull Back可以缓解特定情况下的Z-fighting？ # 现在来解释这个情况：一个两面颜色不同的披风。\n双面渲染与Z-fighting： 如果这个披风是用单个面片（thin geometry/single-sided mesh）建模的，为了让它两面都显示不同颜色（或者都能被看见），你可能会在Shader中设置 Cull Off（关闭剔除）。 [4] [5] 当设置为 Cull Off 时，同一个面片的“正面”和“背面”都会被尝试渲染。关键在于，对于一个无限薄的单个面片来说，它的“正面”和“背面”在空间中的位置是完全相同的。这意味着它们的深度值也是完全相同的。 此时，GPU在渲染这个面片时，正面和背面都在争抢同一个像素的写入权，因为它们的深度值一样。这就直接导致了Z-fighting。 Cull Back的作用： Cull Back 的意思是“剔除背面”。在Unity（以及大多数3D引擎）中，默认情况下，多边形的顶点是按特定顺序（通常是顺时针或逆时针，称为Winding Order）定义的，这个顺序决定了哪个是正面，哪个是背面。 当你将剔除模式设置为 Cull Back 时，只有面片的正面会朝向摄像机时才会被渲染，背面则会被丢弃不渲染。 [4] [5] 对于你那个单面建模的披风，如果开启了 Cull Back，那么当摄像机看到披风的“正面”时，只有正面被渲染；当摄像机转到能看到披风“背面”的角度时（此时原来的“正面”就成了背向摄像机的面），这个面片因为是“背面”（相对于其法线方向和剔除设置）而不会被渲染。 所以，Cull Back** 实际上是阻止了同一个面片的两个“虚拟”的、在同一深度的面（正面和背面）同时参与渲染竞争。它并没有改变深度缓冲区的精度，而是通过只渲染一面，直接消除了冲突的根源。** 精度问题怎么能用剔除解决的？ 严格来说，剔除并没有“解决”深度缓冲区的精度问题本身。深度缓冲区的精度依然是那么多。 但是，对于由 Cull Off 导致的、同一个几何体的正反两面在完全相同深度上竞争所引发的Z-fighting，剔除（比如Cull Back或Cull Front）通过只允许其中一个面参与渲染，从而避免了这场“战斗”。如果只有一个参与者，自然就没有战斗了。 所以，这里的“解决”更应该理解为“规避”了导致Z-fighting的特定条件。 为什么有些角色两边颜色不同的披风就不会出现这种情况？ # 这种情况通常有以下几种原因：\n不同的建模方式： 使用两个独立的、略微分开的面片（Shell/Double Mesh）：最常见的做法是，披风不是一个无限薄的单面片，而是由两层略有间距的面片组成，或者是一个有实际厚度的“壳”状模型。一层代表披风的外面，另一层代表披风的内面。这两层有各自的法线方向，并且在空间上有微小的物理分离。这样，它们的深度值本身就有差异，不容易发生Z-fighting。每一层都可以使用标准的 Cull Back。 这种情况下，你可以给外面和内面的材质分别指定不同的颜色或纹理。 更高级的Shader技巧（即使是单面片）： **使用 VFACE 或 **gl_FrontFacing：现代Shader语言（如HLSL, GLSL）提供了一个语义（如 VFACE 在ShaderLab/HLSL中，或 gl_FrontFacing 在GLSL中），它可以在片元着色器（Fragment Shader/Pixel Shader）中判断当前正在处理的片元是属于正面还是背面。 [7] 通过这个判断，即使设置了 Cull Off，也可以在同一个Shader Pass里，根据是正面还是背面，来采样不同的颜色或纹理，或者执行不同的光照计算。这样只渲染一次，但根据朝向选择不同的外观，避免了两个pass在相同深度上的竞争。 例如，在Shader中可以这样写（伪代码）： Cull Off // 关闭剔除，让两面都进入渲染管线 float4 fragment_shader(VertexOutput input, fixed facing : VFACE) : SV_Target { float4 color; if (facing \u0026gt; 0) { // 或者 facing \u0026gt; 0 对于某些平台表示正面 color = SampleTextureForFrontFace(input.uv); } else { color = SampleTextureForBackFace(input.uv); // 对于背面，法线可能需要反转才能正确光照 input.normal = -input.normal; } // ... 进行光照计算等 return color; } 不同的摄像机或场景设置： 如果其他披风距离摄像机非常近，或者摄像机的近裁剪面和远裁剪面之间的范围（far/near ratio）设置得比较合理（比例较小），那么深度缓冲的精度在那个距离上可能足够高，即使是微小的深度差异也能被区分开。 总结 # 对于你的问题：\n披风很可能是用单层非常薄的几何体制作的。 当你使用一个希望两面都显示（可能用了 Cull Off 或者一个强制双面渲染的Shader）的材质时，这个几何体的“正面”和“背面”在完全相同的深度上，导致Z-fighting。 设置 Cull Back 后，只渲染了其中一面（通常是法线朝外的那一面），另一面被剔除，冲突消失，所以Z-fighting“缓解”了。但代价是，如果你的披风需要从反面看也有不同的颜色，那么 Cull Back 会导致反面不可见或者显示的是正面的颜色（取决于你如何处理法线和光照）。 解决方案建议：\n最佳实践：模型层面处理 如果希望披风内外两侧有不同材质且稳定显示，最好的方法是在3D建模软件中就将披风做成两层独立的网格，或者给予它一定的厚度。然后给内外两层分别赋予不同的材质，并都使用默认的 Cull Back。 Shader层面处理 (如果必须用单层网格) 使用一个支持 VFACE (或等效功能) 的自定义Shader。在Shader中设置 Cull Off，然后在片元着色器中根据 VFACE 的值来判断是正面还是背面，并据此应用不同的颜色/纹理和光照逻辑。这样可以只用一个渲染遍（Pass）就实现双面不同材质的效果，且能有效避免Z-fighting。 调整深度偏移 (Polygon Offset / Shader Offset) 在某些情况下，可以通过在Shader中对其中一个“面”（比如背面）的深度值进行微小的偏移（Polygon Offset，在Unity ShaderLab中可以用 Offset 命令）来尝试解决Z-fighting。但这需要小心调整，且可能在某些角度或距离下仍然出现问题或产生其他视觉瑕疵。 对话中提到的 \u0026ldquo;shadowmap\u0026rdquo; 和 \u0026ldquo;shadow acne\u0026rdquo; 是与阴影渲染相关的深度问题，虽然也和深度精度有关，但和你描述的披风自身两面Z-fighting是略有不同的问题。不过，解决Z-fighting的很多原则是相通的，即确保深度值能够被正确区分。\nReference:\nZ-Fighting - 博客 Z-fighting 緩和措施- Azure Remote Rendering - Learn Microsoft Z-Fighting 权威指南 - BimAnt Unity中Shader的面剔除Cull_cull front cull back-CSDN博客 CullMode - Unity 脚本API Unity Basics: Triangle Winding, Culling Modes \u0026amp; Double Sided Materials ✔️ 2020.3 | Game Dev Tutorial - YouTube How do I get double sided normals without adding vertices? - Blender Stack Exchange Unity解决同材质物体重叠产生Z-Fighting的问题 - 稀土掘金 ","date":"6 January 2025","externalUrl":null,"permalink":"/posts/%E6%97%A0%E5%8E%9A%E5%BA%A6_%E4%BD%8E%E5%8E%9A%E5%BA%A6%E7%9A%84zfighting%E9%97%AE%E9%A2%98/","section":"","summary":"Blender导入Unity指南","title":"无厚度_低厚度的zfighting问题","type":"posts"},{"content":" 关于注册域名 # 注册域名这一方面我用的是Github Education Pack。里面的name.student可以允许创建一个免费一年的域名。必须从github education pack详情页点入name网站来激活student pack权限。\n缺点是：\n后缀比较抽象，一般后缀是.game;.software;.video等。但是对于实现nas ddns的域名解析也是相当够用的了 需要google pay/visa绑定 续费贼贵（大概十几美刀一年），等一年过去我可能就要换其他域名了。但是无所谓，我统一用cloudflare进行托管 Name.com配合实现CloudFlare域名托管 # 对于阿里云亦或是腾讯云，肯定是可以实现自托管的；因为这两家服务提供商都有很广泛的业务；但我是白嫖的，肯定就得请出另一个大善人——cloudflare了！其中的Free套餐也是相当可用的。\n这个指南会分为三个主要部分：\nCloudflare 的初始设置：将您的域名添加到 Cloudflare 并获取 Cloudflare 的域名服务器（NS）地址。 Name.com 的配置：将 Name.com 的域名服务器（NS）更改为 Cloudflare 的。 极空间 NAS 的 DDNS 设置：在极空间里配置 DDNS，使其能够自动更新您域名的 IPv6 地址。 核心概念 # 在开始操作前，我们先简单理解一下各个角色的作用：\nName.com：您域名的“房产证”颁发机构。我们在这里只需要做一件事：告诉它以后域名的地址解析（DNS）工作不要自己管了，全权交给 Cloudflare 处理。 Cloudflare：一个强大的网络服务平台，我们将使用它免费的 DNS 托管和 DDNS 功能。它会接收来自您家 NAS 的指令，将您的域名指向您家宽带动态分配的 IPv6 地址。 极空间 NAS：您家里的数据中心。它会监测自己获取到的公网 IPv6 地址，一旦地址发生变化，就会自动通知 Cloudflare 更新。 整个流程是：极空间 NAS → 通知 Cloudflare → Cloudflare 更新 DNS 记录 → 您通过域名访问 NAS。\n第一部分：Cloudflare 初始设置 # 在这一步，我们将在 Cloudflare 添加您的域名，为后续步骤做准备。\n注册 Cloudflare 账户： 访问 Cloudflare 官网 (cloudflare.com)。 点击 \u0026ldquo;Sign Up\u0026rdquo;（注册），使用您的邮箱和密码创建一个免费账户。 添加您的域名： 登录后，点击仪表板上的 \u0026ldquo;+ Add a domain\u0026rdquo;（添加域）。 [1] 输入您在 Name.com 购买的域名（例如 yourdomain.com），然后点击 \u0026ldquo;Add domain\u0026rdquo;。 [2] 选择套餐： Cloudflare 会展示不同的套餐计划。对于我们的需求，最下面的 Free（免费） 套餐完全足够。 [3] 选中 Free 套餐，然后点击 \u0026ldquo;Continue\u0026rdquo;（继续）。 检查 DNS 记录： Cloudflare 会尝试扫描您域名现有的 DNS 记录。因为是新域名，这里很可能是空的，或者只有几条 Name.com 的默认记录。 暂时不用管这些记录，直接点击 \u0026ldquo;Continue\u0026rdquo;。 获取 Cloudflare 的域名服务器（NS）地址： 这是关键一步。Cloudflare 会提示您，需要将您域名的 NS 记录更改为 Cloudflare 提供的地址。 [4] 您会看到两条 NS 地址，通常是类似 xxx.ns.cloudflare.com 和 yyy.ns.cloudflare.com 的格式。 请将这两个地址完整地复制下来，下一步马上会用到。先不要关闭这个页面。 第二部分：Name.com 配置 # 现在，我们需要回到 Name.com，将域名的管理权“委托”给 Cloudflare。\n登录 Name.com 账户： 打开 Name.com 官网并登录。 进入您的 \u0026ldquo;My Domains\u0026rdquo;（我的域名）列表。 找到域名服务器设置： 点击您需要设置的那个域名，进入管理页面。 寻找名为 \u0026ldquo;Nameservers\u0026rdquo;（域名服务器）的选项卡或链接。通常它位于 DNS 设置区域。 修改域名服务器： 系统默认会使用 Name.com 自己的 Nameservers。您需要选择“自定义”或“使用自己的域名服务器”之类的选项。 删除掉原有的 Name.com 的 NS 地址。 将上一步从 Cloudflare 复制的两条 NS 地址，分别粘贴到输入框中。 保存更改。Name.com 可能会有安全提示，确认即可。 等待 DNS 生效： 域名服务器的更改在全球范围内生效需要一些时间，这个过程称为“DNS 传播”。通常几分钟到几小时即可完成，但官方说法是最长可能需要 24-48 小时。 您可以回到刚才的 Cloudflare 页面，点击 \u0026ldquo;Done, check nameservers\u0026rdquo;（完成，检查域名服务器）按钮。Cloudflare 会开始定期检查 NS 是否已指向它。当它检测到更改成功后，会给您发送一封确认邮件，并且您的域名在 Cloudflare 仪表板上会显示为“Active”（有效）状态。 第三部分：极空间 NAS 的 DDNS 设置 # 当您的域名在 Cloudflare 上激活后，我们就可以配置极空间了。极空间需要一个“密码”（API Token）才能和 Cloudflare 通信。\n步骤 A：在 Cloudflare 创建 API Token # 为了安全，我们不使用全局 API 密钥，而是创建一个权限受限的专用 Token。 [5] 进入 API Token 页面： 在 Cloudflare 仪表板右上角，点击您的头像，然后选择 \u0026ldquo;My Profile\u0026rdquo;（我的个人资料）。 [6] 在左侧菜单中，选择 \u0026ldquo;API Tokens\u0026rdquo;（API 令牌）。 [7] 创建 Token： 点击 \u0026ldquo;Create Token\u0026rdquo;（创建令牌）。 [7] 在模板中找到 \u0026ldquo;Edit Zone DNS\u0026rdquo;（编辑区域 DNS）这一项，点击右侧的 \u0026ldquo;Use template\u0026rdquo;（使用模板）。 [5] 配置 Token 权限： Token name：给这个 Token 起一个容易识别的名字，比如 Zspace-DDNS。 Permissions（权限）：模板已经帮我们选好了 Zone - DNS - Edit，这正是我们需要的权限，无需改动。 [8] Zone Resources（区域资源）：这是最重要的一步，用于限制此 Token 只能管理您指定的域名。 [5] 选择 Include（包括） -\u0026gt; Specific zone（特定区域） -\u0026gt; 然后在下拉菜单中选择您刚刚添加的域名（例如 yourdomain.com）。 Client IP Address Filtering / TTL：这两项保持默认即可。 点击 \u0026ldquo;Continue to summary\u0026rdquo;（继续以查看摘要）。 获取并保存 Token： 在摘要页面确认信息无误后，点击 \u0026ldquo;Create Token\u0026rdquo;。 Cloudflare 会生成一长串字符，这就是您的 API Token。请立即点击旁边的“复制”按钮，并将其保存在一个安全的地方（例如记事本）。这个 Token 只会显示这一次，关闭页面后将无法再次查看。 步骤 B：在极空间中设置 DDNS # 打开极空间 DDNS 应用： 登录您的极空间网页端或客户端。 在桌面或应用中心找到并打开 \u0026ldquo;DDNS\u0026rdquo; 应用。 [9] 添加 DDNS 服务： 点击“添加”或“创建新的 DDNS”。 服务商：选择 Cloudflare。 IP地址获取地：请一定要选择“本地网口获取”，而不是“从极空间服务器获取”。极空间保存的IPv6地址和本地网口的不一致！ 主机名：填写您想使用的域名前缀，例如 nas、home 等。如果您想直接使用主域名，可以尝试填写 @。这里我们以 nas 为例，那么未来您将通过 nas.yourdomain.com 访问。 域名：选择或填写您的主域名，例如 yourdomain.com。 API Token / 密钥：将刚刚从 Cloudflare 复制并保存的 API Token 粘贴到这里。 IP 地址类型：务必选择 IPv6。我选择的是关闭了IPv4，因为我想做的就是P2P。必须确保你的NAS已经被路由器和光猫放行（即入站出站的端口/IP被允许放行，需要用超管权限。可以叫宽带师傅或者自行破解。我家的光猫配置在这里：https://blog.csdn.net/qq_40709699/article/details/118657782 **\nhttps://www.right.com.cn/forum/forum.php?mod=viewthread\u0026amp;tid=8236227\u0026amp;highlight=H2. ）**。如果选择IPv4大概率会无法访问（因为没有配置穿透） 代理（Proxy）：这里会有一个开关，对应 Cloudflare 的“小云朵”。 建议首次设置时关闭代理（灰色云朵）：这会直接将域名解析到您 NAS 的公网 IPv6 地址。这种模式兼容性最好，适合各种服务（如 FTP、SMB 等）。 开启代理（橙色云朵）：这会隐藏您的真实 IP，并由 Cloudflare 提供 CDN 加速和安全防护。这对于网页访问（HTTP/HTTPS）非常有用，但可能会影响非网页服务的直接连接。可以先在关闭状态下测试成功，再按需开启。 保存并测试： 保存设置。极空间的 DDNS 服务会立即尝试连接 Cloudflare 并更新一次 IP。 查看 DDNS 列表中的状态，如果显示“成功”或“IP 地址已是最新”，则表示配置成功。 最后验证 # 在 Cloudflare 检查：回到 Cloudflare 网站，进入您的域名管理页面，点击左侧的 \u0026ldquo;DNS\u0026rdquo;。您应该能看到一条新的 AAAA 记录，名称是您设置的主机名（如 nas），内容是您 NAS 当前的公网 IPv6 地址。 尝试访问 [7] ：在连接了 IPv6 网络的设备上（例如您的手机使用蜂窝网络），尝试通过 http://[您的NAS的IPv6地址]:5055 （NAS的实际端口）看是否能访问。如果可以，再尝试通过 http://nas.yourdomain.com:5055 访问。比如说我的极空间使用的端口是5055 http;如果两者都能成功，那么恭喜，全部设置完成了！ 至此，您的极空间 NAS 已经成功配置了基于 Cloudflare 的 IPv6 DDNS。无论您家的公网 IPv6 地址如何变化，NAS 都会自动通知 Cloudflare，确保您的域名始终指向正确的位置。\nReference:\nAdd a site · Cloudflare Fundamentals docs Add a site · Cloudflare Learning Paths How to Add a Website to Cloudflare (Step-by-Step for Beginners) - YouTube How do I set up my website on Cloudflare? - Rocketspark How to set up Cloudflare Dynamic DNS (DDNS) in OPNsense - Reddit Dynamic DNS with Cloudflare - Cloud Jake - Medium Create API token · Cloudflare Fundamentals docs Dynamic DNS (custom service) with Cloudflare API, how does it work? - Ubiquiti Community 【教程】极空间DDNS使用方法- NAS交流社区 https://zhuanlan.zhihu.com/p/650354462 https://blog.csdn.net/qq_40709699/article/details/118657782 https://www.right.com.cn/forum/forum.php?mod=viewthread\u0026amp;tid=8236227\u0026amp;highlight=H2. ","date":"3 January 2025","externalUrl":null,"permalink":"/posts/name.com%E9%85%8D%E5%90%88cloudflare%E5%AE%9E%E7%8E%B0%E6%9E%81%E7%A9%BA%E9%97%B4%E7%9A%84ddns%E6%89%98%E7%AE%A1%E5%9F%9F%E5%90%8D/","section":"","summary":"Git配置指南","title":"Name.com配合CloudFlare实现极空间的ddns托管域名","type":"posts"},{"content":"","date":"3 January 2025","externalUrl":null,"permalink":"/tags/%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/","section":"Tags","summary":"","title":"网络配置","type":"tags"},{"content":"首先先搬上链接：\n【教程】关于blender模型导入unity的问题解答_哔哩哔哩_bilibili Blender中导出带贴图的FBX_哔哩哔哩_bilibili 因为发现太少有关于这个资料了所以我试错试了很久。。。（下面的图是Blender的大致步骤，Unity还有）\nBlender处理 # 解包对应的材质 # Blender有“打包功能”和“解包功能”。顾名思义，由于blender的保存文件是.blend文件，所以材质贴图可以一起打包进去。Blender的材质管理有三种模式：\n索引模式：Blender建模默认执行的材质管理格式。在这种情况下，就算你点击“导入图像”或者是将图像/纹理拖入到Blender窗口，它的本质是读取这张图像的文件索引（类似于Latex未提供图床的图像添加），不生成额外文件。 打包模式：左上角的“文件”——外部数据——“打包资源”/“自动打包资源”。就是将所有在项目里索引过的文件（图像、SDF图、糙度图）全部打包进入.blend文件中。所有文件打包进入一个文件中，最适合进行文件的传输和保存。 解包模式：**将所有索引过的文件（图像、SDF图、糙度图）在指定文件夹后自动分类命名。**方便其他未适配.blend文件而使用.fbx文件的导入导出（比如Unity） 点击左上角的文件——外部数据——解包资源——“将文件写至当前目录（覆盖目前现有文件）（不是将文件写至源目录！）”,这会在之后的导出生成对应的纹理文件夹。\n导出设置 # 如左图所示，一定要设置坐标轴**“-X向前”“Z向上”。这是由于.fbx文件和blender默认的坐标轴定义不一致导致的，要手动更改，还要点击“应用变换”（曾经的Transform全部归零以免坐标鬼畜）**；路径模式选择“复制”，激活右边的“内嵌纹理”图标。\n导出完成后会有一个textures文件夹。fbx文件夹是我自己创建的不用管。\nUnity处理 # 直接拉入.fbx # 没啥好说的直接拉进去就行了。这时候显示的还都是白模状态。\n拉入先前准备的.texture文件夹 # 这时候Unity会自动处理并匹配对应名字的所有texture图像（因为blender已经为我们重新命名好了）。但是目前隶属于\u0026quot;Checkin\u0026quot;状态，材质不可更改。\n进入Project的Inspector选项进行更改，选中“Materials”，\u0026ldquo;extract textures \u0026ldquo;和“extract materials”都进行导出，选择你想要导出的文件夹就可以了。\n在这之后在Hierarchy内的inspector就可以更改材质/shader了\n问： # 为什么在做完一些还有一些属于白模状态？ # 这是由于Blender的着色器节点导致的。除了Blender默认的原理化BSDF可以被解析，其他非原理化着色器比如玻璃BSDF、透明BSDF不可被解析。需要你自己去匹配Unity对应的Shader（也就是说你要重写了，悲）。\n我的灯好亮啊 # 那就调低点；这是由于Blender对于灯光定义的亮度和Unity自带的灯光定义亮度不一致。\n","date":"26 December 2024","externalUrl":null,"permalink":"/posts/blender%E5%AF%BC%E5%85%A5%E5%9C%BA%E6%99%AF%E8%BF%9B%E5%85%A5unity/","section":"","summary":"Blender导入Unity指南","title":"Blender导入场景进入Unity","type":"posts"},{"content":"因为在有些时候一般的Shader做不到一些东西，比如说提取深度图或者法线图计算，当然你可以写C#脚本来在每次的Update中获取，但是每次要进行修改的时候就只能在C#和Shader之间来回切换，但是邮电不美观（因为每次都要进行声明-传递-接收），更恼火的是用**C#脚本不能指定在某个渲染顺序进行渲染，也就是说你要每次在最后的工序进行渲染，而某些效果是不能在最后的时候才渲染 （如体积光和光线步进）。 **\n主播主播，你的C#脚本还是太吃操作了，有没有更简单（大嘘）的方法来实现这些高级功能呢？\n有的兄弟有的，RenderFeature可以获取渲染中的深度图、法线图和ShadowMap，甚至可以指定插入到某个渲染顺序进行处理。\n在Unity使用Scriptable Render Pipeline时，渲染顺序通常是从开始帧的资源准备和初始化工作，接着进行环境预计算如阴影贴图和光照探针的更新，随后渲染不透明物体并根据深度排序优化性能，之后渲染天空盒，再接着按距离从后往前渲染透明物体以确保正确的透明效果，然后应用包括色调映射、景深等后期处理效果增强画面质量，最后渲染用户界面确保其位于最上层并在完成最终帧输出前进行结束帧处理以清理资源准备下一帧。\n可以插入到上面的任意一个顺序并还能进行进一步的细分，其实就是将SRP进行抽象化并集成为了一个api\nRenderFeature的主要结构 # 一个完整的RenderFeature可以由下面的部分组成：\nRenderFeature RenderPass ShaderPass（可选，这里就是正常的Shader） 补充：Unity自带的Add RenderFeature会给你一些可用的框架，比如说全局屏幕效果。这是你可以将一般的Shader拖进去都行，采用的数据块是_Blit。\nRenderFeature主要框架 # RenderFeature : ScriptableRendererFeature { RenderPass renderpass; //声明RenderPass Create(); //实例化renderpass,设置渲染顺序等操作 AddRenderPasses(); //将上一步实例化的renderpass添加进渲染管道使用 } 继承 ScriptableRendererFeature 并实现 Create() 和 AddRenderPasses() 方法。 在 Create() 中初始化需要的资源和渲染逻辑。 在 AddRenderPasses() 中将自定义的 ScriptableRenderPass 注册到渲染器队列中。 RenderPass主要框架 # RenderPass后面有一个Pass，可以联想到Shader里面也有Pass。RenderPass就是连接Shader的一个东西。可以理解为具体操作。\nRenderPass : ScriptableRenderPass { RenderPassEvent(...) //指定渲染的顺序 Config(...) //可选，指定配置渲染目标（Render Target）和深度缓冲区 Execute(...) //使用上一个函数准备的参数进行具体的操作 FrameCleanup(...) //释放临时渲染目标 //下面还可以写一些自定义函数，用于对应的调用情况 } ** ****RenderPassEvent**：指定这个 Render Pass 应该插入到渲染流程的哪个阶段。 config如果你需要渲染到一张自定义纹理而不是直接到屏幕，就可以在这里设置渲染目标。 Execute：核心的操作，对于计算部分都在这里开始 Frame CleanUp：清理内存以防内存泄漏 这里来个形象点的比喻，就拿蟹堡王的运作方式来举例，可以把我们当做是顾客，前台就是章鱼哥，后台（RenderPass，Shader）就是RenderPass \u0026mdash;\u0026mdash; 我们在前台点餐（设置参数），章鱼哥（RenderFeature）就告诉海绵宝宝（RenderPass，Shader）做几个汉堡，该放什么酱等。可以这样理解。\n示例代码（这里用的是RenderFeature和Shader的结合） # 这里以我自己做的模仿openAI发布会上的像素化LED广告牌Shader来示例。目的是抓取camera的渲染并输出到纹理，转交给Shader后再将Shader处理过的纹理返回到相机渲染中。\nRenderFeature部分\nusing UnityEngine; using UnityEngine.Rendering; using UnityEngine.Rendering.Universal; public class PixelizeRenderFeature : ScriptableRendererFeature { [System.Serializable] public class Settings { [Header(\u0026#34;General Settings\u0026#34;)] [Range(0, 1)] public float mixAmount = 0.5f; [Header(\u0026#34;Image Settings\u0026#34;)] [Range(2, 512)] public float pixelResolutionX = 16f; [Range(2, 512)] public float pixelResolutionY = 16f; public bool customresolution = false; [Header(\u0026#34;Circle Settings\u0026#34;)] [Range(0.01f, 0.5f)] public float circleRadius = 0.4f; [Range(0.001f, 0.1f)] public float edgeSharpness = 0.01f; // Circle UV缩放 [Range(2,512)] public float uvScaleX = 2.0f; [Range(2,512)] public float uvScaleY = 2.0f; public bool linkUVScales = false; public Vector2 circleOffset = new Vector2(0.5f, 0.5f); public Color circleColor = Color.white; public Color backgroundColor = Color.black; [Header(\u0026#34;GrayScale Settings\u0026#34;)] public bool enableGrayscale = false; public Color shadowColor = new Color(0.2f, 0.3f, 0.5f, 1f); public Color highlightColor = new Color(0.9f, 0.7f, 0.4f, 1f); [Range(0.1f, 5f)] public float contrast = 1f; [Header(\u0026#34;Sync Settings\u0026#34;)] public bool adaptToScreenRatio = false; } // 在Inspector中公开设置 public Settings settings = new Settings(); class SDFCircleRenderPass : ScriptableRenderPass { private Material _material; private RenderTargetHandle tempTexture; private Settings settings; public SDFCircleRenderPass(Settings settings) { this.settings = settings; tempTexture.Init(\u0026#34;_TempSDFCircleRT\u0026#34;); } // 只接受材质 public void Setup(Material material) { this._material = material; } public override void Execute(ScriptableRenderContext context, ref RenderingData renderingData) { if (_material == null) return; // 从renderingData获取相机颜色目标 var cameraColorTarget = renderingData.cameraData.renderer.cameraColorTarget; CommandBuffer cmd = CommandBufferPool.Get(\u0026#34;Custom/TiledSDFCirclesAdvanced\u0026#34;); _material.SetFloat(\u0026#34;_MixAmount\u0026#34;, settings.mixAmount); _material.SetFloat(\u0026#34;_PixelResolutionX\u0026#34;, settings.pixelResolutionX); _material.SetFloat(\u0026#34;_PixelResolutionY\u0026#34;, settings.pixelResolutionY); _material.SetFloat(\u0026#34;_LinkResolutions\u0026#34;, settings.customresolution ? 1.0f : 0.0f); _material.SetFloat(\u0026#34;_CircleRadius\u0026#34;, settings.circleRadius); _material.SetFloat(\u0026#34;_EdgeSharpness\u0026#34;, settings.edgeSharpness); _material.SetFloat(\u0026#34;_UVScaleX\u0026#34;, settings.uvScaleX); _material.SetFloat(\u0026#34;_UVScaleY\u0026#34;, settings.uvScaleY); _material.SetFloat(\u0026#34;_LinkUVScales\u0026#34;, settings.linkUVScales ? 1.0f : 0.0f); _material.SetVector(\u0026#34;_CircleOffset\u0026#34;, settings.circleOffset); _material.SetColor(\u0026#34;_CircleColor\u0026#34;, settings.circleColor); _material.SetColor(\u0026#34;_BackgroundColor\u0026#34;, settings.backgroundColor); _material.SetInt(\u0026#34;_EnableGrayscale\u0026#34;, settings.enableGrayscale ? 1 : 0); _material.SetColor(\u0026#34;_ShadowColor\u0026#34;, settings.shadowColor); _material.SetColor(\u0026#34;_HighlightColor\u0026#34;, settings.highlightColor); _material.SetFloat(\u0026#34;_Contrast\u0026#34;, settings.contrast); // byd字符串查找怎么你了（生气） RenderTextureDescriptor descriptor = renderingData.cameraData.cameraTargetDescriptor; cmd.GetTemporaryRT(tempTexture.id, descriptor); // 使用cameraColorTarget替代_source cmd.Blit(cameraColorTarget, tempTexture.Identifier(), _material); cmd.Blit(tempTexture.Identifier(), cameraColorTarget); context.ExecuteCommandBuffer(cmd); CommandBufferPool.Release(cmd); //Here is Debug Debug.Log($\u0026#34;Setting parameters: (Tip in l108)circleRadius={settings.circleRadius}\u0026#34;); } public override void FrameCleanup(CommandBuffer cmd) { cmd.ReleaseTemporaryRT(tempTexture.id); } } private SDFCircleRenderPass _renderPass; private Material _material; public override void Create() { // 加载着色器并创建材质 Shader shader = Shader.Find(\u0026#34;Custom/TiledSDFCirclesAdvanced\u0026#34;); if (shader == null) { Debug.LogError(\u0026#34;无法找到SDF圆形后处理着色器!\u0026#34;); return; } _material = new Material(shader); // 创建渲染通道 _renderPass = new SDFCircleRenderPass(settings); // 设置渲染事件时机 - 在后处理之前 _renderPass.renderPassEvent = RenderPassEvent.BeforeRenderingPostProcessing; } public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData renderingData) { if (_material == null) return; // 只传递材质，不传递相机颜色目标 _renderPass.Setup(_material); renderer.EnqueuePass(_renderPass); } protected override void Dispose(bool disposing) { if (disposing \u0026amp;\u0026amp; _material != null) { CoreUtils.Destroy(_material); } } } Shader部分：\nShader \u0026#34;Custom/TiledSDFCirclesAdvanced\u0026#34; { Properties { [Header(GeneralSettings)] _MixAmount (\u0026#34;MixAmount(Image\u0026amp;Circle)\u0026#34;, Range(0, 1)) = 0.5//混合量 [Header(ImageSettings)] _MainTex (\u0026#34;Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} [Toggle] _LinkResolutions (\u0026#34;Open Custom X/Y Resolutions\u0026#34;, Float) = 0 // 链接XY分辨率的开关 _PixelResolutionX (\u0026#34;Pixel Resolution (X)\u0026#34;, Range(2,512)) = 16 _PixelResolutionY (\u0026#34;Pixel Resolution Y\u0026#34;, Range(2,512)) = 16 [Header((I recommend Pixel Resolution is same with UV Scale))] [Header(CircleSettings)] _CircleRadius (\u0026#34;Circle Radius\u0026#34;, Range(0.01, 0.5)) = 0.4 _EdgeSharpness (\u0026#34;Edge Sharpness\u0026#34;, Range(0.001, 0.1)) = 0.01 _UVScaleX (\u0026#34;UV Scale X\u0026#34;, Range(1, 512)) = 2.0 _UVScaleY (\u0026#34;UV Scale Y\u0026#34;, Range(1, 512)) = 2.0 [Toggle] _LinkUVScales (\u0026#34;Link UV X/Y\u0026#34;, Float) = 1 _CircleOffset (\u0026#34;Circle Offset\u0026#34;, Vector) = (0.5, 0.5, 0, 0) _CircleColor (\u0026#34;Circle Color\u0026#34;, Color) = (1,1,1,1) _BackgroundColor (\u0026#34;Background Color\u0026#34;, Color) = (0,0,0,1) [Header(GrayScaleSettings)] [Toggle] _EnableGrayscale (\u0026#34;Enable Grayscale\u0026#34;, Float) = 0//灰度开关控制 _ShadowColor (\u0026#34;Shadow Color\u0026#34;, Color) = (0.2, 0.3, 0.5, 1.0)//阴影色 _HighlightColor (\u0026#34;Highlight Color\u0026#34;, Color) = (0.9, 0.7, 0.4, 1.0)//高光色 _Contrast (\u0026#34;Contrast\u0026#34;, Range(0.1, 5)) = 1.0//对比度 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;Cull\u0026#34;=\u0026#34;Off\u0026#34; \u0026#34;ZWrite\u0026#34;=\u0026#34;Off\u0026#34; \u0026#34;ZTest\u0026#34;=\u0026#34;Always\u0026#34; } LOD 100 Pass { HLSLPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;GrayScale.hlsl\u0026#34; struct appdata { float4 vertex : POSITION; float2 uv2 : TEXCOORD1; float2 uv : TEXCOORD0; }; struct v2f { float2 uv : TEXCOORD0; float2 uv2 : TEXCOORD1; float4 vertex : SV_POSITION; }; sampler2D _MainTex; float _PixelResolution; float _CircleRadius; float _EdgeSharpness; float _UVScaleX; float _UVScaleY; float2 _CircleOffset; float4 _CircleColor; float4 _BackgroundColor; float _MixAmount; float _EnableGrayscale; float4 _ShadowColor; float4 _HighlightColor; float _Contrast; float _LinkResolutions; float _PixelResolutionX; float _PixelResolutionY; float _LinkUVScales; v2f vert (appdata v) { v2f o; o.vertex = UnityObjectToClipPos(v.vertex); o.uv = v.uv; o.uv2 = v.uv2; return o; } fixed4 frag (v2f i) : SV_Target { //像素化部分 // 原始 UV 坐标 float2 uv = i.uv; // 计算像素化UV - 使用独立的X和Y分辨率 float2 pixelResolution = float2(_PixelResolutionX, _PixelResolutionY); // 如果链接分辨率开关打开，则使用X分辨率 if (_LinkResolutions \u0026lt; 0.5) { pixelResolution.y = pixelResolution.x; } float2 pixelatedUV = floor(uv * pixelResolution) / pixelResolution; // 使用修改后的 UV 坐标从纹理中采样 fixed4 col0 = tex2D(_MainTex, pixelatedUV); if(_EnableGrayscale \u0026gt; 0.5) { float grayscale = GrayscaleStandard(col0.rgb); float4 tintedGrayscale = GrayscaleDuotone( grayscale, _ShadowColor, // 阴影色(冷色调) _HighlightColor, // 高光色(暖色调) 0.5 // 中点 ); tintedGrayscale.rgb = pow(tintedGrayscale.rgb,_Contrast); col0 = tintedGrayscale; } //Circles部分 // 计算Circle的UV缩放 float2 uvScale = float2(_UVScaleX, _UVScaleY); if (_LinkUVScales \u0026lt; 0.5) { uvScale.y = uvScale.x; } // 应用UV缩放和偏移 float2 scaledUV = i.uv * uvScale; float2 tiledUV = frac(scaledUV); // 以指定偏移为中心 float2 centeredUV = tiledUV - _CircleOffset; // 计算到圆心的距离 float distance = length(centeredUV); // 平滑边缘的SDF圆 float circle = smoothstep(_CircleRadius + _EdgeSharpness, _CircleRadius - _EdgeSharpness, distance); // 混合颜色 fixed4 col1 = lerp(_BackgroundColor, _CircleColor, circle); fixed4 finalColor = lerp(col0, col1, _MixAmount); return finalColor; } ENDHLSL } } } Shader在RenderFeature的联动引用原理是从缓存区Blit中获取这个Shader的名字，我还以为有多高大上呢原来就是找索引啊（难绷）\n注：Rider在使用在缓存池进行字符串索引的时候会提示“字符串搜索的方式低效”，如果有更高效的办法就是从缓存区里获取ID，替换掉字符串。\n","date":"14 December 2024","externalUrl":null,"permalink":"/posts/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%88%E6%9E%9C%E7%9A%84%E5%A4%A7%E6%9D%80%E5%99%A8renderfeature/","section":"","summary":"问题解决方案","title":"自定义效果的大杀器——RenderFeature","type":"posts"},{"content":" Unity通用Gitignore # # This .gitignore file should be placed at the root of your Unity project directory\r#\r# Get latest from https://github.com/github/gitignore/blob/main/Unity.gitignore\r#\r.utmp/\r/[Ll]ibrary/\r/[Tt]emp/\r/[Oo]bj/\r/[Bb]uild/\r/[Bb]uilds/\r/[Ll]ogs/\r/[Uu]ser[Ss]ettings/\r# MemoryCaptures can get excessive in size.\r# They also could contain extremely sensitive data\r/[Mm]emoryCaptures/\r# Recordings can get excessive in size\r/[Rr]ecordings/\r# Uncomment this line if you wish to ignore the asset store tools plugin\r# /[Aa]ssets/AssetStoreTools*\r# Autogenerated Jetbrains Rider plugin\r/[Aa]ssets/Plugins/Editor/JetBrains*\r# Visual Studio cache directory\r.vs/\r# Gradle cache directory\r.gradle/\r# Autogenerated VS/MD/Consulo solution and project files\rExportedObj/\r.consulo/\r*.csproj\r*.unityproj\r*.sln\r*.suo\r*.tmp\r*.user\r*.userprefs\r*.pidb\r*.booproj\r*.svd\r*.pdb\r*.mdb\r*.opendb\r*.VC.db\r# Unity3D generated meta files\r*.pidb.meta\r*.pdb.meta\r*.mdb.meta\r# Unity3D generated file on crash reports\rsysinfo.txt\r# Builds\r*.apk\r*.aab\r*.unitypackage\r*.unitypackage.meta\r*.app\r# Crashlytics generated file\rcrashlytics-build.properties\r# Packed Addressables\r/[Aa]ssets/[Aa]ddressable[Aa]ssets[Dd]ata/*/*.bin*\r# Temporary auto-generated Android Assets\r/[Aa]ssets/[Ss]treamingAssets/aa.meta\r/[Aa]ssets/[Ss]treamingAssets/aa/* ","date":"2 December 2024","externalUrl":null,"permalink":"/posts/gitignore%E9%85%8D%E7%BD%AE/","section":"","summary":"Git配置指南","title":"Gitignore配置","type":"posts"},{"content":" 关于我 # 欢迎来访，这里是Axon的博客，也叫Axon_Singularity。\n这个博客平时用来记录一些我的技术文章，也会展示一点我已经完成的项目；归档文章一般会放在语雀上（当然我平时也是语雀写的，并且有些文章会一直保持更新，有时候这个博客就不能及时看到。毕竟可视化和Allinone真的太香了）\n什么？不够geek？这就要问语雀为什么不开放token给我了（超级会员）\n什么时候语雀开放他的latebook我再做更新自动化。\n我的语雀主页在这里：Axon_Singularity 自身相关\n一个正在学习的Technical Artist（可能是？未来是？不知道），主要使用Unity；多种DCC软件也会涉及。 目前的工作是做Unity Shader和Houdini相关的内容，主要是Shader和VFX方面的工作。 喜欢探索各种渲染相关技术，图形学、Shader编程和视觉效果。偶尔会写一些渲染解析，从事一些包括OBS插件、Blender Python的插件开发，以及一点的python前端(PyQt-Fluent,其实是写插件的时候觉得自带的qt太丑了所以强迫症去写前端去了)。\n目前在学习的内容/技术栈：\nUnity Shader HLSL/C#(调库侠)/Animator System/VFX graph Houdini/VEX/Procedual Modeling Generation + Karma Render 使用Blender完成大部分的模型工作,少部分必须使用Maya和Marvelous Designer完成(如果没要求我死也不用3dsmax) After Effects/Primiere Pro/DaVinci Resolve 视频三板斧 Photoshop/Procreate/Illustrator/Substance 图像处理，偶尔作画 Stable Diffusion/Comfy UI/ControlNet 无聊玩玩的AI绘画 有学习UE的计划 爱好相关\n电脑爱好：做点3d模型相关的研究。目前在研究Houdini的程序化建模和VFX，也感慨Houdini的强大，属于是TA必修课。 喜欢拍照，拍摄风景和建筑，有拍人像的经历（玩过灯阵）。用的是尼康Z5+35/85/40/24-200. 音乐爱好：音乐大杂烩，尤其是电子音乐的多曲风、J-Pop和游戏原声，番剧原声 游戏经历：主推 合作pve游戏/模拟经营类游戏/高水平美术游戏（大概这么划分吧）/可能是泛二（玩游不混圈）\n目前在玩： Helldivers2、City Skyline1/2、VR chat、Beat Saber、Starfield、Cyberpunk2077、Overwatch2、Inzoi、Minecraft+Forge/Fabric、偶尔上线尘白禁区/碧蓝航线。\n游戏历史：瑞典神人工作室（肥鲨，Paradox，Mojang，Arrowhead）和Yuzusoft的游戏基本都玩过一遍。 地平线5、地平线4、NFS Heat、NFS Payback、NFS Unbound、海豹社 甜蜜女友2、ATRI、Illusion工作室全系列、人类一败涂地、深岩银河、战地5、战地2042、星露谷物语、DJmax、Arcaea、Cytus全系列、Deemo、SDVX6、IIDX29-31、无人深空、怪物猎人世界、泰拉瑞亚、欧卡、胡闹厨房、深海迷航（又叫做难绷的美丽水世界，谁想出来的译名）、GRIS、NEVA、Gemini、ICEY、Celeste。（这些都是正经的，当然还有很多不正经的游戏）\n主线剧情\n成为能画涩图的程序员 能改模并且做高质二创 学日语 (别急！) 有朝一日可以打自己的手办，但还不会拆件打印 你好，这里是关于我的测试页面。这个链接没有使用md自带的内联链接和引用链接，而是使用hugo的语法进行其他文章的引用。 我最近写了第二篇文章 ，欢迎阅读！\n","externalUrl":null,"permalink":"/aboutme/","section":"","summary":"","title":"","type":"aboutme"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]